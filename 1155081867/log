loading raw file: ../data/train_32x32.mat
filling input queue
loading raw file: ../data/test_32x32.mat
random validation set [22910:23910]
filling input queue
loading batch of samples: 128
loading batch of samples: 1000
TEST after_create_session
2017-05-18 07:11:28.552031: step 0, loss = 6.38 (55.7 examples/sec; 2.297 sec/batch), precision = 16.10%
2017-05-18 07:11:31.251403: step 10, loss = 6.29 (396.0 examples/sec; 0.323 sec/batch)
2017-05-18 07:11:33.405454: step 20, loss = 6.24 (594.2 examples/sec; 0.215 sec/batch)
2017-05-18 07:11:35.274223: step 30, loss = 6.19 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:11:37.143056: step 40, loss = 6.20 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:11:39.016555: step 50, loss = 6.16 (683.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:11:40.908675: step 60, loss = 6.13 (676.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:11:42.775437: step 70, loss = 6.12 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:11:44.637282: step 80, loss = 6.04 (687.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:11:46.515244: step 90, loss = 6.03 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:11:48.606358: step 100, loss = 6.01 (657.5 examples/sec; 0.195 sec/batch), precision = 19.90%
2017-05-18 07:11:51.607641: step 110, loss = 5.94 (406.9 examples/sec; 0.315 sec/batch)
2017-05-18 07:11:53.529270: step 120, loss = 5.97 (666.1 examples/sec; 0.192 sec/batch)
2017-05-18 07:11:55.412344: step 130, loss = 5.91 (679.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:11:57.284444: step 140, loss = 5.93 (683.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:11:59.174939: step 150, loss = 5.90 (677.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:12:01.065749: step 160, loss = 5.85 (677.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:12:02.948815: step 170, loss = 5.77 (679.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:04.852645: step 180, loss = 5.76 (672.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:12:06.771561: step 190, loss = 5.73 (667.0 examples/sec; 0.192 sec/batch)
2017-05-18 07:12:08.941517: step 200, loss = 5.77 (634.5 examples/sec; 0.202 sec/batch), precision = 19.60%
2017-05-18 07:12:11.987116: step 210, loss = 5.74 (400.2 examples/sec; 0.320 sec/batch)
2017-05-18 07:12:13.876861: step 220, loss = 5.64 (677.3 examples/sec; 0.189 sec/batch)
2017-05-18 07:12:15.742226: step 230, loss = 5.63 (686.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:12:17.599652: step 240, loss = 5.59 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:12:19.470462: step 250, loss = 5.52 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:12:21.335354: step 260, loss = 5.57 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:12:23.203767: step 270, loss = 5.47 (685.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:12:25.083698: step 280, loss = 5.55 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:26.937263: step 290, loss = 5.53 (690.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:12:29.028084: step 300, loss = 5.40 (657.6 examples/sec; 0.195 sec/batch), precision = 20.40%
2017-05-18 07:12:32.021604: step 310, loss = 5.41 (407.9 examples/sec; 0.314 sec/batch)
2017-05-18 07:12:33.903874: step 320, loss = 5.40 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:35.774986: step 330, loss = 5.34 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:12:37.649700: step 340, loss = 5.33 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:12:39.532017: step 350, loss = 5.30 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:41.390953: step 360, loss = 5.29 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:12:43.248414: step 370, loss = 5.33 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:12:45.130311: step 380, loss = 5.25 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:47.012743: step 390, loss = 5.21 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:49.114886: step 400, loss = 5.22 (653.3 examples/sec; 0.196 sec/batch), precision = 18.30%
2017-05-18 07:12:52.120095: step 410, loss = 5.16 (406.6 examples/sec; 0.315 sec/batch)
2017-05-18 07:12:54.030146: step 420, loss = 5.18 (670.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:12:55.936986: step 430, loss = 5.15 (671.3 examples/sec; 0.191 sec/batch)
2017-05-18 07:12:57.840906: step 440, loss = 5.08 (672.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:12:59.728476: step 450, loss = 5.09 (678.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:13:01.634075: step 460, loss = 5.11 (671.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:13:03.569984: step 470, loss = 5.02 (661.2 examples/sec; 0.194 sec/batch)
2017-05-18 07:13:05.504948: step 480, loss = 4.96 (661.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:13:07.427059: step 490, loss = 5.04 (665.9 examples/sec; 0.192 sec/batch)
2017-05-18 07:13:09.594298: step 500, loss = 5.02 (636.4 examples/sec; 0.201 sec/batch), precision = 18.70%
2017-05-18 07:13:12.559404: step 510, loss = 4.96 (410.1 examples/sec; 0.312 sec/batch)
2017-05-18 07:13:14.419207: step 520, loss = 4.95 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:16.285817: step 530, loss = 4.88 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:13:18.137549: step 540, loss = 4.90 (691.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:13:19.994694: step 550, loss = 4.85 (689.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:21.859604: step 560, loss = 4.84 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:23.717306: step 570, loss = 4.83 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:25.572858: step 580, loss = 4.82 (689.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:27.421457: step 590, loss = 4.84 (692.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:13:29.504079: step 600, loss = 4.80 (659.9 examples/sec; 0.194 sec/batch), precision = 19.50%
2017-05-18 07:13:32.578720: step 610, loss = 4.75 (397.8 examples/sec; 0.322 sec/batch)
2017-05-18 07:13:34.452707: step 620, loss = 4.74 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:13:36.310136: step 630, loss = 4.68 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:38.187119: step 640, loss = 4.67 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:13:40.050072: step 650, loss = 4.68 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:41.906433: step 660, loss = 4.67 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:43.761144: step 670, loss = 4.59 (690.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:13:45.625407: step 680, loss = 4.58 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:47.497786: step 690, loss = 4.59 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:13:49.627372: step 700, loss = 4.59 (646.1 examples/sec; 0.198 sec/batch), precision = 18.00%
2017-05-18 07:13:52.596716: step 710, loss = 4.57 (410.5 examples/sec; 0.312 sec/batch)
2017-05-18 07:13:54.480464: step 720, loss = 4.56 (679.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:13:56.326045: step 730, loss = 4.50 (693.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:13:58.181127: step 740, loss = 4.48 (690.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:00.039327: step 750, loss = 4.50 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:01.893737: step 760, loss = 4.47 (690.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:14:03.740612: step 770, loss = 4.38 (693.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:14:05.600026: step 780, loss = 4.40 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:07.451626: step 790, loss = 4.46 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:14:09.523236: step 800, loss = 4.36 (662.9 examples/sec; 0.193 sec/batch), precision = 18.10%
2017-05-18 07:14:12.501350: step 810, loss = 4.35 (410.4 examples/sec; 0.312 sec/batch)
2017-05-18 07:14:14.388946: step 820, loss = 4.37 (678.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:14:16.250480: step 830, loss = 4.34 (687.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:18.114816: step 840, loss = 4.39 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:19.980010: step 850, loss = 4.32 (686.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:14:21.843989: step 860, loss = 4.29 (686.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:23.703987: step 870, loss = 4.27 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:25.568387: step 880, loss = 4.30 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:27.429292: step 890, loss = 4.23 (687.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:29.517667: step 900, loss = 4.22 (657.3 examples/sec; 0.195 sec/batch), precision = 19.30%
2017-05-18 07:14:32.546055: step 910, loss = 4.16 (403.9 examples/sec; 0.317 sec/batch)
2017-05-18 07:14:34.452283: step 920, loss = 4.27 (671.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:14:36.356225: step 930, loss = 4.17 (672.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:14:38.268712: step 940, loss = 4.15 (669.3 examples/sec; 0.191 sec/batch)
2017-05-18 07:14:40.189328: step 950, loss = 4.16 (666.5 examples/sec; 0.192 sec/batch)
2017-05-18 07:14:42.111178: step 960, loss = 4.07 (666.0 examples/sec; 0.192 sec/batch)
2017-05-18 07:14:44.043053: step 970, loss = 4.13 (662.6 examples/sec; 0.193 sec/batch)
2017-05-18 07:14:45.970431: step 980, loss = 4.15 (664.1 examples/sec; 0.193 sec/batch)
2017-05-18 07:14:47.910696: step 990, loss = 4.06 (659.7 examples/sec; 0.194 sec/batch)
2017-05-18 07:14:50.098071: step 1000, loss = 4.09 (629.1 examples/sec; 0.203 sec/batch), precision = 16.10%
2017-05-18 07:14:53.114262: step 1010, loss = 3.96 (403.9 examples/sec; 0.317 sec/batch)
2017-05-18 07:14:54.990376: step 1020, loss = 4.03 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:14:56.869468: step 1030, loss = 4.05 (681.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:14:58.742667: step 1040, loss = 4.02 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:00.638714: step 1050, loss = 4.02 (675.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:15:02.518558: step 1060, loss = 3.96 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:04.389799: step 1070, loss = 3.93 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:06.265851: step 1080, loss = 3.97 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:08.130240: step 1090, loss = 3.88 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:15:10.230284: step 1100, loss = 3.94 (654.1 examples/sec; 0.196 sec/batch), precision = 20.00%
2017-05-18 07:15:13.187833: step 1110, loss = 3.85 (412.8 examples/sec; 0.310 sec/batch)
2017-05-18 07:15:15.086441: step 1120, loss = 3.92 (674.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:15:16.955786: step 1130, loss = 3.91 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:18.829895: step 1140, loss = 3.89 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:20.710271: step 1150, loss = 3.87 (680.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:22.595344: step 1160, loss = 3.86 (679.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:15:24.462054: step 1170, loss = 3.88 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:26.355029: step 1180, loss = 3.82 (676.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:15:28.257345: step 1190, loss = 3.80 (672.9 examples/sec; 0.190 sec/batch)
2017-05-18 07:15:30.424039: step 1200, loss = 3.81 (635.5 examples/sec; 0.201 sec/batch), precision = 17.10%
2017-05-18 07:15:33.532170: step 1210, loss = 3.82 (392.5 examples/sec; 0.326 sec/batch)
2017-05-18 07:15:35.432610: step 1220, loss = 3.79 (673.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:15:37.299164: step 1230, loss = 3.77 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:39.182722: step 1240, loss = 3.68 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:41.063604: step 1250, loss = 3.75 (680.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:42.950177: step 1260, loss = 3.76 (678.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:15:44.812367: step 1270, loss = 3.70 (687.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:15:46.692350: step 1280, loss = 3.71 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:48.560102: step 1290, loss = 3.69 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:50.651460: step 1300, loss = 3.69 (657.7 examples/sec; 0.195 sec/batch), precision = 20.40%
2017-05-18 07:15:53.635880: step 1310, loss = 3.70 (409.0 examples/sec; 0.313 sec/batch)
2017-05-18 07:15:55.544293: step 1320, loss = 3.69 (670.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:15:57.422542: step 1330, loss = 3.65 (681.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:59.296959: step 1340, loss = 3.59 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:16:01.182999: step 1350, loss = 3.60 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:16:03.056221: step 1360, loss = 3.65 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:16:04.937669: step 1370, loss = 3.63 (680.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:16:06.799164: step 1380, loss = 3.59 (687.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:16:08.666622: step 1390, loss = 3.59 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:16:10.781953: step 1400, loss = 3.52 (650.4 examples/sec; 0.197 sec/batch), precision = 17.10%
2017-05-18 07:16:13.779417: step 1410, loss = 3.54 (407.0 examples/sec; 0.314 sec/batch)
2017-05-18 07:16:15.685821: step 1420, loss = 3.54 (671.4 examples/sec; 0.191 sec/batch)
2017-05-18 07:16:17.588161: step 1430, loss = 3.56 (672.9 examples/sec; 0.190 sec/batch)
2017-05-18 07:16:19.519839: step 1440, loss = 3.54 (662.6 examples/sec; 0.193 sec/batch)
2017-05-18 07:16:21.474238: step 1450, loss = 3.48 (654.9 examples/sec; 0.195 sec/batch)
2017-05-18 07:16:23.399794: step 1460, loss = 3.50 (664.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:16:25.336162: step 1470, loss = 3.49 (661.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:16:27.281347: step 1480, loss = 3.52 (658.0 examples/sec; 0.195 sec/batch)
2017-05-18 07:16:29.249534: step 1490, loss = 3.49 (650.3 examples/sec; 0.197 sec/batch)
2017-05-18 07:16:31.437949: step 1500, loss = 3.45 (628.9 examples/sec; 0.204 sec/batch), precision = 19.80%
2017-05-18 07:16:34.485059: step 1510, loss = 3.44 (400.0 examples/sec; 0.320 sec/batch)
2017-05-18 07:16:36.388561: step 1520, loss = 3.47 (672.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:16:38.276924: step 1530, loss = 3.39 (677.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:16:40.167092: step 1540, loss = 3.44 (677.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:16:42.057999: step 1550, loss = 3.36 (676.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:16:43.934704: step 1560, loss = 3.45 (682.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:16:45.810498: step 1570, loss = 3.41 (682.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:16:47.692958: step 1580, loss = 3.40 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:16:49.560472: step 1590, loss = 3.36 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:16:51.654834: step 1600, loss = 3.37 (656.0 examples/sec; 0.195 sec/batch), precision = 18.60%
2017-05-18 07:16:54.681289: step 1610, loss = 3.36 (403.8 examples/sec; 0.317 sec/batch)
2017-05-18 07:16:56.551446: step 1620, loss = 3.37 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:16:58.418158: step 1630, loss = 3.36 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:00.292513: step 1640, loss = 3.31 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:02.167964: step 1650, loss = 3.35 (682.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:17:04.036608: step 1660, loss = 3.35 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:05.945120: step 1670, loss = 3.27 (670.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:17:07.873189: step 1680, loss = 3.28 (663.9 examples/sec; 0.193 sec/batch)
2017-05-18 07:17:09.799865: step 1690, loss = 3.30 (664.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:17:11.971729: step 1700, loss = 3.21 (632.9 examples/sec; 0.202 sec/batch), precision = 19.80%
2017-05-18 07:17:14.987658: step 1710, loss = 3.31 (404.4 examples/sec; 0.317 sec/batch)
2017-05-18 07:17:16.870610: step 1720, loss = 3.23 (679.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:17:18.740228: step 1730, loss = 3.30 (684.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:20.609967: step 1740, loss = 3.24 (684.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:22.475239: step 1750, loss = 3.20 (686.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:24.346946: step 1760, loss = 3.22 (683.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:26.214251: step 1770, loss = 3.24 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:28.088136: step 1780, loss = 3.25 (683.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:29.994310: step 1790, loss = 3.30 (671.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:17:32.092068: step 1800, loss = 3.22 (654.7 examples/sec; 0.195 sec/batch), precision = 19.60%
2017-05-18 07:17:35.171800: step 1810, loss = 3.19 (397.2 examples/sec; 0.322 sec/batch)
2017-05-18 07:17:37.078062: step 1820, loss = 3.15 (671.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:17:38.936532: step 1830, loss = 3.22 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:17:40.818960: step 1840, loss = 3.17 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:17:42.702131: step 1850, loss = 3.18 (679.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:17:44.572312: step 1860, loss = 3.14 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:46.455101: step 1870, loss = 3.15 (679.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:17:48.318213: step 1880, loss = 3.14 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:17:50.207305: step 1890, loss = 3.14 (677.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:17:52.302464: step 1900, loss = 3.16 (655.6 examples/sec; 0.195 sec/batch), precision = 17.00%
2017-05-18 07:17:55.330979: step 1910, loss = 3.12 (403.6 examples/sec; 0.317 sec/batch)
2017-05-18 07:17:57.261104: step 1920, loss = 3.12 (663.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:17:59.170581: step 1930, loss = 3.14 (670.3 examples/sec; 0.191 sec/batch)
2017-05-18 07:18:01.112474: step 1940, loss = 3.13 (659.2 examples/sec; 0.194 sec/batch)
2017-05-18 07:18:03.049230: step 1950, loss = 3.14 (660.9 examples/sec; 0.194 sec/batch)
2017-05-18 07:18:04.994603: step 1960, loss = 3.07 (658.0 examples/sec; 0.195 sec/batch)
2017-05-18 07:18:06.929627: step 1970, loss = 3.06 (661.5 examples/sec; 0.194 sec/batch)
2017-05-18 07:18:08.874501: step 1980, loss = 3.04 (658.1 examples/sec; 0.194 sec/batch)
2017-05-18 07:18:10.810645: step 1990, loss = 3.10 (661.1 examples/sec; 0.194 sec/batch)
2017-05-18 07:18:13.001123: step 2000, loss = 3.03 (629.2 examples/sec; 0.203 sec/batch), precision = 18.60%
2017-05-18 07:18:16.047761: step 2010, loss = 3.02 (399.7 examples/sec; 0.320 sec/batch)
2017-05-18 07:18:17.980009: step 2020, loss = 3.08 (662.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:18:19.866214: step 2030, loss = 3.03 (678.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:18:21.747868: step 2040, loss = 3.09 (680.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:18:23.622711: step 2050, loss = 3.05 (682.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:18:25.522973: step 2060, loss = 3.07 (673.6 examples/sec; 0.190 sec/batch)
2017-05-18 07:18:27.408823: step 2070, loss = 3.01 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:18:29.289646: step 2080, loss = 3.03 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:18:31.171147: step 2090, loss = 2.98 (680.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:18:33.263025: step 2100, loss = 3.03 (657.1 examples/sec; 0.195 sec/batch), precision = 17.90%
2017-05-18 07:18:36.245807: step 2110, loss = 2.99 (409.4 examples/sec; 0.313 sec/batch)
2017-05-18 07:18:38.123584: step 2120, loss = 2.94 (681.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:18:39.975400: step 2130, loss = 2.92 (691.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:18:41.834156: step 2140, loss = 2.98 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:18:43.693639: step 2150, loss = 2.95 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:18:45.559343: step 2160, loss = 2.93 (686.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:18:47.452933: step 2170, loss = 2.96 (676.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:18:49.369937: step 2180, loss = 2.92 (667.7 examples/sec; 0.192 sec/batch)
2017-05-18 07:18:51.295251: step 2190, loss = 2.93 (664.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:18:53.456835: step 2200, loss = 2.90 (636.5 examples/sec; 0.201 sec/batch), precision = 19.20%
2017-05-18 07:18:56.465412: step 2210, loss = 2.91 (405.2 examples/sec; 0.316 sec/batch)
2017-05-18 07:18:58.350533: step 2220, loss = 2.87 (679.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:19:00.218685: step 2230, loss = 2.98 (685.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:19:02.083627: step 2240, loss = 2.92 (686.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:19:03.956997: step 2250, loss = 2.91 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:19:05.816323: step 2260, loss = 2.93 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:19:07.678940: step 2270, loss = 2.92 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:19:09.539655: step 2280, loss = 2.92 (687.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:19:11.409147: step 2290, loss = 2.86 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:19:13.497205: step 2300, loss = 2.84 (657.9 examples/sec; 0.195 sec/batch), precision = 18.40%
2017-05-18 07:19:16.520411: step 2310, loss = 2.90 (404.3 examples/sec; 0.317 sec/batch)
2017-05-18 07:19:18.406844: step 2320, loss = 2.85 (678.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:19:20.285854: step 2330, loss = 2.82 (681.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:19:22.168153: step 2340, loss = 2.94 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:19:24.039367: step 2350, loss = 2.87 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:19:25.915252: step 2360, loss = 2.84 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:19:27.778313: step 2370, loss = 2.82 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:19:29.646645: step 2380, loss = 2.82 (685.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:19:31.525003: step 2390, loss = 2.84 (681.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:19:33.629279: step 2400, loss = 2.86 (653.8 examples/sec; 0.196 sec/batch), precision = 19.50%
2017-05-18 07:19:36.735654: step 2410, loss = 2.82 (393.5 examples/sec; 0.325 sec/batch)
2017-05-18 07:19:38.645890: step 2420, loss = 2.83 (670.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:19:40.553119: step 2430, loss = 2.84 (671.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:19:42.485388: step 2440, loss = 2.81 (662.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:19:44.426645: step 2450, loss = 2.82 (659.4 examples/sec; 0.194 sec/batch)
2017-05-18 07:19:46.362362: step 2460, loss = 2.78 (661.3 examples/sec; 0.194 sec/batch)
2017-05-18 07:19:48.290985: step 2470, loss = 2.81 (663.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:19:50.227270: step 2480, loss = 2.82 (661.1 examples/sec; 0.194 sec/batch)
2017-05-18 07:19:52.169530: step 2490, loss = 2.79 (659.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:19:54.382455: step 2500, loss = 2.79 (621.2 examples/sec; 0.206 sec/batch), precision = 19.70%
2017-05-18 07:19:57.396909: step 2510, loss = 2.83 (404.2 examples/sec; 0.317 sec/batch)
2017-05-18 07:19:59.290826: step 2520, loss = 2.74 (675.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:20:01.199822: step 2530, loss = 2.77 (670.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:20:03.083194: step 2540, loss = 2.77 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:04.960303: step 2550, loss = 2.78 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:06.858295: step 2560, loss = 2.76 (674.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:20:08.746483: step 2570, loss = 2.74 (677.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:20:10.632177: step 2580, loss = 2.77 (678.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:20:12.518310: step 2590, loss = 2.76 (678.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:20:14.624982: step 2600, loss = 2.72 (652.1 examples/sec; 0.196 sec/batch), precision = 19.70%
2017-05-18 07:20:17.631416: step 2610, loss = 2.75 (406.3 examples/sec; 0.315 sec/batch)
2017-05-18 07:20:19.529682: step 2620, loss = 2.79 (674.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:20:21.400474: step 2630, loss = 2.72 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:20:23.266883: step 2640, loss = 2.80 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:20:25.141921: step 2650, loss = 2.73 (682.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:27.014306: step 2660, loss = 2.74 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:20:28.898655: step 2670, loss = 2.68 (679.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:30.822609: step 2680, loss = 2.71 (665.3 examples/sec; 0.192 sec/batch)
2017-05-18 07:20:32.756638: step 2690, loss = 2.75 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:20:34.912287: step 2700, loss = 2.69 (640.0 examples/sec; 0.200 sec/batch), precision = 18.60%
2017-05-18 07:20:37.908145: step 2710, loss = 2.73 (406.1 examples/sec; 0.315 sec/batch)
2017-05-18 07:20:39.784711: step 2720, loss = 2.72 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:41.642345: step 2730, loss = 2.68 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:20:43.512040: step 2740, loss = 2.70 (684.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:20:45.388467: step 2750, loss = 2.76 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:47.261260: step 2760, loss = 2.64 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:20:49.113736: step 2770, loss = 2.67 (691.0 examples/sec; 0.185 sec/batch)
2017-05-18 07:20:50.973152: step 2780, loss = 2.66 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:20:52.835847: step 2790, loss = 2.67 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:20:54.929986: step 2800, loss = 2.68 (655.8 examples/sec; 0.195 sec/batch), precision = 19.00%
2017-05-18 07:20:57.939937: step 2810, loss = 2.68 (406.1 examples/sec; 0.315 sec/batch)
2017-05-18 07:20:59.851099: step 2820, loss = 2.62 (669.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:21:01.724782: step 2830, loss = 2.65 (683.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:03.608278: step 2840, loss = 2.72 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:05.487122: step 2850, loss = 2.66 (681.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:07.375078: step 2860, loss = 2.66 (678.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:21:09.245701: step 2870, loss = 2.60 (684.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:11.122041: step 2880, loss = 2.61 (682.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:13.011041: step 2890, loss = 2.68 (677.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:21:15.115709: step 2900, loss = 2.59 (651.9 examples/sec; 0.196 sec/batch), precision = 17.10%
2017-05-18 07:21:18.084675: step 2910, loss = 2.66 (411.5 examples/sec; 0.311 sec/batch)
2017-05-18 07:21:20.008710: step 2920, loss = 2.62 (665.3 examples/sec; 0.192 sec/batch)
2017-05-18 07:21:21.913313: step 2930, loss = 2.59 (672.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:21:23.847332: step 2940, loss = 2.55 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:21:25.770461: step 2950, loss = 2.64 (665.6 examples/sec; 0.192 sec/batch)
2017-05-18 07:21:28.322176: step 2960, loss = 2.63 (501.6 examples/sec; 0.255 sec/batch)
2017-05-18 07:21:29.994231: step 2970, loss = 2.65 (765.5 examples/sec; 0.167 sec/batch)
2017-05-18 07:21:31.870878: step 2980, loss = 2.62 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:33.739807: step 2990, loss = 2.64 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:35.833214: step 3000, loss = 2.61 (656.2 examples/sec; 0.195 sec/batch), precision = 19.10%
2017-05-18 07:21:38.860578: step 3010, loss = 2.59 (403.7 examples/sec; 0.317 sec/batch)
2017-05-18 07:21:40.768552: step 3020, loss = 2.60 (670.9 examples/sec; 0.191 sec/batch)
2017-05-18 07:21:42.650508: step 3030, loss = 2.62 (680.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:44.522362: step 3040, loss = 2.59 (683.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:46.400402: step 3050, loss = 2.61 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:48.269296: step 3060, loss = 2.55 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:50.148198: step 3070, loss = 2.59 (681.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:52.022756: step 3080, loss = 2.60 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:53.895742: step 3090, loss = 2.58 (683.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:56.009247: step 3100, loss = 2.60 (651.0 examples/sec; 0.197 sec/batch), precision = 19.70%
2017-05-18 07:21:59.018444: step 3110, loss = 2.63 (405.5 examples/sec; 0.316 sec/batch)
2017-05-18 07:22:00.959101: step 3120, loss = 2.54 (659.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:22:02.855584: step 3130, loss = 2.55 (674.9 examples/sec; 0.190 sec/batch)
2017-05-18 07:22:04.741993: step 3140, loss = 2.56 (678.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:22:06.634196: step 3150, loss = 2.58 (676.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:22:08.522896: step 3160, loss = 2.60 (677.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:22:10.457328: step 3170, loss = 2.58 (661.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:22:12.376059: step 3180, loss = 2.52 (667.1 examples/sec; 0.192 sec/batch)
2017-05-18 07:22:14.313144: step 3190, loss = 2.58 (660.8 examples/sec; 0.194 sec/batch)
2017-05-18 07:22:16.492971: step 3200, loss = 2.59 (631.7 examples/sec; 0.203 sec/batch), precision = 19.30%
2017-05-18 07:22:19.509996: step 3210, loss = 2.54 (403.7 examples/sec; 0.317 sec/batch)
2017-05-18 07:22:21.410585: step 3220, loss = 2.58 (673.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:22:23.290641: step 3230, loss = 2.54 (680.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:25.163165: step 3240, loss = 2.54 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:22:27.056585: step 3250, loss = 2.50 (676.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:22:28.953564: step 3260, loss = 2.50 (674.8 examples/sec; 0.190 sec/batch)
2017-05-18 07:22:30.829634: step 3270, loss = 2.56 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:32.698993: step 3280, loss = 2.57 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:22:34.580259: step 3290, loss = 2.47 (680.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:36.696278: step 3300, loss = 2.51 (650.9 examples/sec; 0.197 sec/batch), precision = 18.70%
2017-05-18 07:22:39.663888: step 3310, loss = 2.52 (410.7 examples/sec; 0.312 sec/batch)
2017-05-18 07:22:41.574875: step 3320, loss = 2.52 (669.8 examples/sec; 0.191 sec/batch)
2017-05-18 07:22:43.456045: step 3330, loss = 2.49 (680.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:45.330430: step 3340, loss = 2.51 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:22:47.210085: step 3350, loss = 2.51 (681.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:49.072868: step 3360, loss = 2.50 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:22:50.940602: step 3370, loss = 2.49 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:22:52.809159: step 3380, loss = 2.48 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:22:54.686963: step 3390, loss = 2.55 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:56.788713: step 3400, loss = 2.52 (654.1 examples/sec; 0.196 sec/batch), precision = 17.30%
2017-05-18 07:22:59.801930: step 3410, loss = 2.52 (405.3 examples/sec; 0.316 sec/batch)
2017-05-18 07:23:01.770304: step 3420, loss = 2.53 (650.3 examples/sec; 0.197 sec/batch)
2017-05-18 07:23:03.706831: step 3430, loss = 2.50 (661.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:23:05.648451: step 3440, loss = 2.52 (659.2 examples/sec; 0.194 sec/batch)
2017-05-18 07:23:07.588729: step 3450, loss = 2.52 (659.7 examples/sec; 0.194 sec/batch)
2017-05-18 07:23:09.532582: step 3460, loss = 2.57 (658.5 examples/sec; 0.194 sec/batch)
2017-05-18 07:23:11.478935: step 3470, loss = 2.50 (657.6 examples/sec; 0.195 sec/batch)
2017-05-18 07:23:13.422348: step 3480, loss = 2.42 (658.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:23:15.378574: step 3490, loss = 2.51 (654.3 examples/sec; 0.196 sec/batch)
2017-05-18 07:23:17.557815: step 3500, loss = 2.55 (632.1 examples/sec; 0.202 sec/batch), precision = 19.00%
2017-05-18 07:23:20.700568: step 3510, loss = 2.44 (388.2 examples/sec; 0.330 sec/batch)
2017-05-18 07:23:22.632576: step 3520, loss = 2.50 (662.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:23:24.512457: step 3530, loss = 2.43 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:23:26.402840: step 3540, loss = 2.49 (677.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:23:28.283563: step 3550, loss = 2.54 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:23:30.173324: step 3560, loss = 2.46 (677.3 examples/sec; 0.189 sec/batch)
2017-05-18 07:23:32.060191: step 3570, loss = 2.45 (678.4 examples/sec; 0.189 sec/batch)
2017-05-18 07:23:33.942336: step 3580, loss = 2.44 (680.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:23:35.837579: step 3590, loss = 2.45 (675.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:23:37.951469: step 3600, loss = 2.44 (650.3 examples/sec; 0.197 sec/batch), precision = 18.10%
2017-05-18 07:23:40.999829: step 3610, loss = 2.49 (400.7 examples/sec; 0.319 sec/batch)
2017-05-18 07:23:42.900855: step 3620, loss = 2.50 (673.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:23:44.781821: step 3630, loss = 2.43 (680.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:23:46.677966: step 3640, loss = 2.45 (675.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:23:48.559734: step 3650, loss = 2.45 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:23:50.458374: step 3660, loss = 2.41 (674.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:23:52.376760: step 3670, loss = 2.49 (667.2 examples/sec; 0.192 sec/batch)
2017-05-18 07:23:54.303254: step 3680, loss = 2.48 (664.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:23:56.253296: step 3690, loss = 2.42 (656.4 examples/sec; 0.195 sec/batch)
2017-05-18 07:23:58.439052: step 3700, loss = 2.47 (629.4 examples/sec; 0.203 sec/batch), precision = 16.50%
2017-05-18 07:24:01.464559: step 3710, loss = 2.43 (402.8 examples/sec; 0.318 sec/batch)
2017-05-18 07:24:03.360338: step 3720, loss = 2.50 (675.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:24:05.249037: step 3730, loss = 2.46 (677.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:24:07.129763: step 3740, loss = 2.42 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:09.013919: step 3750, loss = 2.45 (679.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:10.909114: step 3760, loss = 2.41 (675.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:24:12.781026: step 3770, loss = 2.41 (683.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:24:14.657629: step 3780, loss = 2.47 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:16.533774: step 3790, loss = 2.45 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:18.643205: step 3800, loss = 2.41 (650.7 examples/sec; 0.197 sec/batch), precision = 18.10%
2017-05-18 07:24:21.629209: step 3810, loss = 2.39 (409.2 examples/sec; 0.313 sec/batch)
2017-05-18 07:24:23.525114: step 3820, loss = 2.40 (675.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:24:25.398283: step 3830, loss = 2.40 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:24:27.277979: step 3840, loss = 2.40 (681.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:29.142859: step 3850, loss = 2.43 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:24:31.033236: step 3860, loss = 2.40 (677.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:24:32.931524: step 3870, loss = 2.43 (674.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:24:34.809858: step 3880, loss = 2.42 (681.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:36.693695: step 3890, loss = 2.39 (679.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:38.801458: step 3900, loss = 2.41 (651.6 examples/sec; 0.196 sec/batch), precision = 19.10%
2017-05-18 07:24:41.828657: step 3910, loss = 2.50 (403.7 examples/sec; 0.317 sec/batch)
2017-05-18 07:24:43.736020: step 3920, loss = 2.42 (671.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:24:45.648057: step 3930, loss = 2.47 (669.4 examples/sec; 0.191 sec/batch)
2017-05-18 07:24:47.590389: step 3940, loss = 2.41 (659.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:24:49.533774: step 3950, loss = 2.42 (658.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:24:51.526877: step 3960, loss = 2.42 (642.2 examples/sec; 0.199 sec/batch)
2017-05-18 07:24:53.458715: step 3970, loss = 2.36 (662.6 examples/sec; 0.193 sec/batch)
2017-05-18 07:24:55.406246: step 3980, loss = 2.38 (657.2 examples/sec; 0.195 sec/batch)
2017-05-18 07:24:57.354697: step 3990, loss = 2.37 (656.9 examples/sec; 0.195 sec/batch)
2017-05-18 07:24:59.533857: step 4000, loss = 2.44 (631.8 examples/sec; 0.203 sec/batch), precision = 19.70%
2017-05-18 07:25:02.567103: step 4010, loss = 2.37 (401.7 examples/sec; 0.319 sec/batch)
2017-05-18 07:25:04.444129: step 4020, loss = 2.43 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:25:06.315530: step 4030, loss = 2.39 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:08.256198: step 4040, loss = 2.32 (659.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:25:10.190728: step 4050, loss = 2.38 (661.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:25:12.085111: step 4060, loss = 2.39 (675.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:25:14.070426: step 4070, loss = 2.32 (644.7 examples/sec; 0.199 sec/batch)
2017-05-18 07:25:15.947266: step 4080, loss = 2.45 (682.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:25:17.815447: step 4090, loss = 2.36 (685.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:19.922036: step 4100, loss = 2.39 (651.8 examples/sec; 0.196 sec/batch), precision = 19.60%
2017-05-18 07:25:23.003919: step 4110, loss = 2.37 (396.9 examples/sec; 0.322 sec/batch)
2017-05-18 07:25:24.911813: step 4120, loss = 2.44 (670.9 examples/sec; 0.191 sec/batch)
2017-05-18 07:25:26.776233: step 4130, loss = 2.42 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:25:28.637546: step 4140, loss = 2.33 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:25:30.507973: step 4150, loss = 2.35 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:32.382288: step 4160, loss = 2.41 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:34.378177: step 4170, loss = 2.40 (641.3 examples/sec; 0.200 sec/batch)
2017-05-18 07:25:36.323673: step 4180, loss = 2.39 (657.9 examples/sec; 0.195 sec/batch)
2017-05-18 07:25:38.244370: step 4190, loss = 2.36 (666.4 examples/sec; 0.192 sec/batch)
2017-05-18 07:25:40.413777: step 4200, loss = 2.37 (634.5 examples/sec; 0.202 sec/batch), precision = 17.30%
2017-05-18 07:25:43.428748: step 4210, loss = 2.42 (404.2 examples/sec; 0.317 sec/batch)
2017-05-18 07:25:45.322266: step 4220, loss = 2.38 (676.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:25:47.302881: step 4230, loss = 2.43 (646.3 examples/sec; 0.198 sec/batch)
2017-05-18 07:25:49.166055: step 4240, loss = 2.38 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:25:51.035180: step 4250, loss = 2.38 (684.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:52.915476: step 4260, loss = 2.32 (680.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:25:54.776799: step 4270, loss = 2.39 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:25:56.648159: step 4280, loss = 2.37 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:58.522280: step 4290, loss = 2.37 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:26:00.613885: step 4300, loss = 2.38 (657.2 examples/sec; 0.195 sec/batch), precision = 18.10%
2017-05-18 07:26:03.571604: step 4310, loss = 2.33 (412.7 examples/sec; 0.310 sec/batch)
2017-05-18 07:26:05.463767: step 4320, loss = 2.35 (676.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:26:07.327264: step 4330, loss = 2.39 (686.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:09.196299: step 4340, loss = 2.34 (684.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:26:11.062902: step 4350, loss = 2.38 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:26:12.947445: step 4360, loss = 2.35 (679.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:26:14.816428: step 4370, loss = 2.34 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:26:16.669717: step 4380, loss = 2.36 (690.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:26:18.535633: step 4390, loss = 2.35 (686.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:26:20.631258: step 4400, loss = 2.29 (655.9 examples/sec; 0.195 sec/batch), precision = 20.50%
2017-05-18 07:26:23.642090: step 4410, loss = 2.36 (405.7 examples/sec; 0.315 sec/batch)
2017-05-18 07:26:25.568375: step 4420, loss = 2.37 (664.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:26:27.471855: step 4430, loss = 2.39 (672.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:26:29.384319: step 4440, loss = 2.36 (669.3 examples/sec; 0.191 sec/batch)
2017-05-18 07:26:31.311810: step 4450, loss = 2.35 (664.1 examples/sec; 0.193 sec/batch)
2017-05-18 07:26:33.246415: step 4460, loss = 2.31 (661.6 examples/sec; 0.193 sec/batch)
2017-05-18 07:26:35.180748: step 4470, loss = 2.34 (661.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:26:37.118851: step 4480, loss = 2.35 (660.4 examples/sec; 0.194 sec/batch)
2017-05-18 07:26:39.051671: step 4490, loss = 2.30 (662.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:26:41.222365: step 4500, loss = 2.35 (633.8 examples/sec; 0.202 sec/batch), precision = 20.20%
2017-05-18 07:26:44.211628: step 4510, loss = 2.33 (407.6 examples/sec; 0.314 sec/batch)
2017-05-18 07:26:46.093576: step 4520, loss = 2.32 (680.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:26:47.950243: step 4530, loss = 2.35 (689.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:49.806715: step 4540, loss = 2.34 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:51.666376: step 4550, loss = 2.30 (688.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:53.525186: step 4560, loss = 2.32 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:55.369709: step 4570, loss = 2.38 (693.9 examples/sec; 0.184 sec/batch)
2017-05-18 07:26:57.231634: step 4580, loss = 2.33 (687.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:59.092002: step 4590, loss = 2.33 (688.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:01.193797: step 4600, loss = 2.35 (653.4 examples/sec; 0.196 sec/batch), precision = 16.30%
2017-05-18 07:27:04.152708: step 4610, loss = 2.35 (412.7 examples/sec; 0.310 sec/batch)
2017-05-18 07:27:06.058274: step 4620, loss = 2.35 (671.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:27:07.934495: step 4630, loss = 2.38 (682.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:27:09.805742: step 4640, loss = 2.32 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:11.670431: step 4650, loss = 2.30 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:13.537583: step 4660, loss = 2.34 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:15.437368: step 4670, loss = 2.31 (673.8 examples/sec; 0.190 sec/batch)
2017-05-18 07:27:17.345872: step 4680, loss = 2.32 (670.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:27:19.279887: step 4690, loss = 2.34 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:27:21.462487: step 4700, loss = 2.30 (629.7 examples/sec; 0.203 sec/batch), precision = 19.50%
2017-05-18 07:27:24.563299: step 4710, loss = 2.31 (393.8 examples/sec; 0.325 sec/batch)
2017-05-18 07:27:26.441676: step 4720, loss = 2.29 (681.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:27:28.301168: step 4730, loss = 2.34 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:30.159456: step 4740, loss = 2.32 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:32.018846: step 4750, loss = 2.30 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:33.893710: step 4760, loss = 2.36 (682.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:35.757645: step 4770, loss = 2.37 (686.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:37.622551: step 4780, loss = 2.33 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:39.494573: step 4790, loss = 2.31 (683.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:41.586937: step 4800, loss = 2.33 (657.3 examples/sec; 0.195 sec/batch), precision = 20.70%
2017-05-18 07:27:44.590849: step 4810, loss = 2.30 (406.5 examples/sec; 0.315 sec/batch)
2017-05-18 07:27:46.471458: step 4820, loss = 2.32 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:27:48.335109: step 4830, loss = 2.33 (686.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:50.197942: step 4840, loss = 2.30 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:52.062143: step 4850, loss = 2.40 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:53.934924: step 4860, loss = 2.29 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:55.807578: step 4870, loss = 2.35 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:57.681852: step 4880, loss = 2.31 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:59.546298: step 4890, loss = 2.29 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:28:01.656421: step 4900, loss = 2.30 (651.7 examples/sec; 0.196 sec/batch), precision = 21.60%
2017-05-18 07:28:04.660388: step 4910, loss = 2.34 (406.4 examples/sec; 0.315 sec/batch)
2017-05-18 07:28:06.555953: step 4920, loss = 2.30 (675.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:28:08.452554: step 4930, loss = 2.31 (674.9 examples/sec; 0.190 sec/batch)
2017-05-18 07:28:10.364014: step 4940, loss = 2.31 (669.6 examples/sec; 0.191 sec/batch)
2017-05-18 07:28:12.292533: step 4950, loss = 2.31 (663.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:28:14.228195: step 4960, loss = 2.31 (661.3 examples/sec; 0.194 sec/batch)
2017-05-18 07:28:16.168298: step 4970, loss = 2.34 (659.8 examples/sec; 0.194 sec/batch)
2017-05-18 07:28:18.093585: step 4980, loss = 2.32 (664.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:28:20.027673: step 4990, loss = 2.29 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:28:22.206828: step 5000, loss = 2.33 (632.4 examples/sec; 0.202 sec/batch), precision = 18.00%
2017-05-18 07:28:25.215094: step 5010, loss = 2.36 (404.6 examples/sec; 0.316 sec/batch)
2017-05-18 07:28:27.122553: step 5020, loss = 2.29 (671.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:28:29.009987: step 5030, loss = 2.31 (678.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:28:30.878509: step 5040, loss = 2.36 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:28:32.756927: step 5050, loss = 2.31 (681.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:34.635180: step 5060, loss = 2.28 (681.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:36.504752: step 5070, loss = 2.30 (684.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:28:38.388338: step 5080, loss = 2.33 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:40.271060: step 5090, loss = 2.33 (679.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:42.380297: step 5100, loss = 2.33 (651.4 examples/sec; 0.197 sec/batch), precision = 18.80%
2017-05-18 07:28:45.369614: step 5110, loss = 2.30 (408.5 examples/sec; 0.313 sec/batch)
2017-05-18 07:28:47.271107: step 5120, loss = 2.32 (673.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:28:49.139219: step 5130, loss = 2.29 (685.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:28:51.015909: step 5140, loss = 2.36 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:52.882112: step 5150, loss = 2.34 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:28:54.749918: step 5160, loss = 2.23 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:28:56.633888: step 5170, loss = 2.33 (679.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:58.537450: step 5180, loss = 2.33 (672.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:29:00.474206: step 5190, loss = 2.28 (660.9 examples/sec; 0.194 sec/batch)
2017-05-18 07:29:02.655023: step 5200, loss = 2.33 (631.7 examples/sec; 0.203 sec/batch), precision = 17.80%
2017-05-18 07:29:05.673290: step 5210, loss = 2.27 (403.4 examples/sec; 0.317 sec/batch)
2017-05-18 07:29:07.571788: step 5220, loss = 2.23 (674.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:29:09.442508: step 5230, loss = 2.33 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:29:11.317064: step 5240, loss = 2.31 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:29:13.197620: step 5250, loss = 2.30 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:15.075651: step 5260, loss = 2.30 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:16.953863: step 5270, loss = 2.32 (681.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:18.820438: step 5280, loss = 2.29 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:29:20.696345: step 5290, loss = 2.28 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:22.790011: step 5300, loss = 2.29 (656.2 examples/sec; 0.195 sec/batch), precision = 18.40%
2017-05-18 07:29:25.909631: step 5310, loss = 2.24 (392.3 examples/sec; 0.326 sec/batch)
2017-05-18 07:29:27.797248: step 5320, loss = 2.32 (678.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:29:29.673870: step 5330, loss = 2.31 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:31.533285: step 5340, loss = 2.23 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:29:33.405544: step 5350, loss = 2.27 (683.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:29:35.287265: step 5360, loss = 2.28 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:37.163257: step 5370, loss = 2.28 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:39.037876: step 5380, loss = 2.31 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:29:40.923515: step 5390, loss = 2.34 (678.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:29:43.018722: step 5400, loss = 2.31 (655.6 examples/sec; 0.195 sec/batch), precision = 18.20%
2017-05-18 07:29:46.011439: step 5410, loss = 2.27 (408.2 examples/sec; 0.314 sec/batch)
2017-05-18 07:29:47.927041: step 5420, loss = 2.29 (668.2 examples/sec; 0.192 sec/batch)
2017-05-18 07:29:49.836447: step 5430, loss = 2.27 (670.4 examples/sec; 0.191 sec/batch)
2017-05-18 07:29:51.754137: step 5440, loss = 2.32 (667.5 examples/sec; 0.192 sec/batch)
2017-05-18 07:29:53.690525: step 5450, loss = 2.26 (661.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:29:55.629587: step 5460, loss = 2.24 (660.1 examples/sec; 0.194 sec/batch)
2017-05-18 07:29:57.559991: step 5470, loss = 2.25 (663.1 examples/sec; 0.193 sec/batch)
2017-05-18 07:29:59.494093: step 5480, loss = 2.28 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:30:01.431047: step 5490, loss = 2.27 (660.8 examples/sec; 0.194 sec/batch)
2017-05-18 07:30:03.612445: step 5500, loss = 2.31 (631.1 examples/sec; 0.203 sec/batch), precision = 18.70%
2017-05-18 07:30:06.616385: step 5510, loss = 2.30 (405.5 examples/sec; 0.316 sec/batch)
2017-05-18 07:30:08.500223: step 5520, loss = 2.30 (679.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:30:10.366625: step 5530, loss = 2.28 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:12.242032: step 5540, loss = 2.21 (682.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:30:14.112108: step 5550, loss = 2.27 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:15.986018: step 5560, loss = 2.31 (683.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:17.865159: step 5570, loss = 2.37 (681.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:30:19.736508: step 5580, loss = 2.27 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:21.599143: step 5590, loss = 2.27 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:30:23.692366: step 5600, loss = 2.25 (657.2 examples/sec; 0.195 sec/batch), precision = 16.40%
2017-05-18 07:30:26.685677: step 5610, loss = 2.29 (407.8 examples/sec; 0.314 sec/batch)
2017-05-18 07:30:28.560479: step 5620, loss = 2.25 (682.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:30.419582: step 5630, loss = 2.29 (688.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:30:32.285748: step 5640, loss = 2.24 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:34.145636: step 5650, loss = 2.28 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:30:36.023704: step 5660, loss = 2.21 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:30:37.927974: step 5670, loss = 2.27 (672.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:30:39.842755: step 5680, loss = 2.30 (668.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:30:41.775679: step 5690, loss = 2.30 (662.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:30:43.929150: step 5700, loss = 2.25 (637.9 examples/sec; 0.201 sec/batch), precision = 18.50%
2017-05-18 07:30:46.917542: step 5710, loss = 2.30 (408.3 examples/sec; 0.314 sec/batch)
2017-05-18 07:30:48.791183: step 5720, loss = 2.32 (683.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:50.644432: step 5730, loss = 2.29 (690.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:30:52.505644: step 5740, loss = 2.25 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:30:54.375005: step 5750, loss = 2.28 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:56.226514: step 5760, loss = 2.27 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:30:58.092789: step 5770, loss = 2.28 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:59.959487: step 5780, loss = 2.29 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:01.826826: step 5790, loss = 2.25 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:03.925877: step 5800, loss = 2.22 (654.6 examples/sec; 0.196 sec/batch), precision = 19.50%
2017-05-18 07:31:06.908222: step 5810, loss = 2.36 (409.5 examples/sec; 0.313 sec/batch)
2017-05-18 07:31:08.835053: step 5820, loss = 2.36 (664.3 examples/sec; 0.193 sec/batch)
2017-05-18 07:31:10.778749: step 5830, loss = 2.27 (658.5 examples/sec; 0.194 sec/batch)
2017-05-18 07:31:12.636463: step 5840, loss = 2.28 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:14.494942: step 5850, loss = 2.23 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:16.346177: step 5860, loss = 2.23 (691.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:31:18.204786: step 5870, loss = 2.30 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:20.055317: step 5880, loss = 2.28 (691.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:31:21.914866: step 5890, loss = 2.27 (688.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:24.002350: step 5900, loss = 2.27 (657.8 examples/sec; 0.195 sec/batch), precision = 17.70%
2017-05-18 07:31:28.103986: step 5910, loss = 2.27 (301.7 examples/sec; 0.424 sec/batch)
2017-05-18 07:31:29.267553: step 5920, loss = 2.30 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-18 07:31:31.132637: step 5930, loss = 2.24 (686.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:32.986035: step 5940, loss = 2.27 (690.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:31:34.846687: step 5950, loss = 2.24 (687.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:36.714154: step 5960, loss = 2.30 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:38.692647: step 5970, loss = 2.30 (647.0 examples/sec; 0.198 sec/batch)
2017-05-18 07:31:40.551042: step 5980, loss = 2.28 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:42.425131: step 5990, loss = 2.26 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:44.510105: step 6000, loss = 2.34 (658.9 examples/sec; 0.194 sec/batch), precision = 18.70%
2017-05-18 07:31:47.509927: step 6010, loss = 2.28 (407.4 examples/sec; 0.314 sec/batch)
2017-05-18 07:31:49.406162: step 6020, loss = 2.27 (675.0 examples/sec; 0.190 sec/batch)
2017-05-18 07:31:51.265036: step 6030, loss = 2.27 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:53.119147: step 6040, loss = 2.29 (690.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:31:54.993291: step 6050, loss = 2.25 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:56.865117: step 6060, loss = 2.24 (683.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:58.738535: step 6070, loss = 2.32 (683.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:00.605195: step 6080, loss = 2.28 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:02.472236: step 6090, loss = 2.24 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:04.558362: step 6100, loss = 2.29 (657.8 examples/sec; 0.195 sec/batch), precision = 18.30%
2017-05-18 07:32:07.544628: step 6110, loss = 2.27 (409.4 examples/sec; 0.313 sec/batch)
2017-05-18 07:32:09.436459: step 6120, loss = 2.23 (676.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:32:11.311217: step 6130, loss = 2.28 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:13.179870: step 6140, loss = 2.27 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:15.080322: step 6150, loss = 2.28 (673.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:32:17.365588: step 6160, loss = 2.20 (560.1 examples/sec; 0.229 sec/batch)
2017-05-18 07:32:20.143855: step 6170, loss = 2.24 (460.7 examples/sec; 0.278 sec/batch)
2017-05-18 07:32:22.542050: step 6180, loss = 2.27 (533.7 examples/sec; 0.240 sec/batch)
2017-05-18 07:32:24.716229: step 6190, loss = 2.23 (588.7 examples/sec; 0.217 sec/batch)
2017-05-18 07:32:27.735571: step 6200, loss = 2.32 (447.0 examples/sec; 0.286 sec/batch), precision = 19.70%
2017-05-18 07:32:32.032120: step 6210, loss = 2.25 (287.5 examples/sec; 0.445 sec/batch)
2017-05-18 07:32:34.595452: step 6220, loss = 2.24 (499.3 examples/sec; 0.256 sec/batch)
2017-05-18 07:32:36.488598: step 6230, loss = 2.25 (676.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:32:38.366082: step 6240, loss = 2.31 (681.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:32:40.244010: step 6250, loss = 2.29 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:32:42.113904: step 6260, loss = 2.26 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:43.986297: step 6270, loss = 2.24 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:45.853210: step 6280, loss = 2.26 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:47.728011: step 6290, loss = 2.26 (682.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:49.830331: step 6300, loss = 2.20 (654.3 examples/sec; 0.196 sec/batch), precision = 19.20%
2017-05-18 07:32:52.844942: step 6310, loss = 2.28 (405.0 examples/sec; 0.316 sec/batch)
2017-05-18 07:32:54.730303: step 6320, loss = 2.31 (678.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:32:56.590711: step 6330, loss = 2.23 (688.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:32:58.470214: step 6340, loss = 2.26 (681.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:00.374709: step 6350, loss = 2.30 (672.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:33:02.301206: step 6360, loss = 2.22 (664.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:33:04.245788: step 6370, loss = 2.28 (658.2 examples/sec; 0.194 sec/batch)
2017-05-18 07:33:06.187359: step 6380, loss = 2.25 (659.3 examples/sec; 0.194 sec/batch)
2017-05-18 07:33:08.122841: step 6390, loss = 2.19 (661.3 examples/sec; 0.194 sec/batch)
2017-05-18 07:33:10.296527: step 6400, loss = 2.29 (631.5 examples/sec; 0.203 sec/batch), precision = 19.40%
2017-05-18 07:33:13.434618: step 6410, loss = 2.26 (389.7 examples/sec; 0.328 sec/batch)
2017-05-18 07:33:15.327819: step 6420, loss = 2.28 (676.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:17.215101: step 6430, loss = 2.22 (678.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:19.090180: step 6440, loss = 2.27 (682.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:20.969443: step 6450, loss = 2.28 (681.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:22.852096: step 6460, loss = 2.21 (679.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:24.724909: step 6470, loss = 2.33 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:33:26.620019: step 6480, loss = 2.27 (675.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:33:28.494213: step 6490, loss = 2.28 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:33:30.613941: step 6500, loss = 2.27 (647.6 examples/sec; 0.198 sec/batch), precision = 18.80%
2017-05-18 07:33:33.612523: step 6510, loss = 2.27 (407.4 examples/sec; 0.314 sec/batch)
2017-05-18 07:33:35.510698: step 6520, loss = 2.31 (674.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:33:37.396492: step 6530, loss = 2.34 (678.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:39.283978: step 6540, loss = 2.26 (678.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:41.168116: step 6550, loss = 2.20 (679.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:43.052915: step 6560, loss = 2.24 (679.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:44.936980: step 6570, loss = 2.29 (679.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:46.811274: step 6580, loss = 2.27 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:33:48.700365: step 6590, loss = 2.28 (677.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:50.839832: step 6600, loss = 2.26 (642.6 examples/sec; 0.199 sec/batch), precision = 18.50%
2017-05-18 07:33:53.876957: step 6610, loss = 2.21 (401.9 examples/sec; 0.318 sec/batch)
2017-05-18 07:33:55.770774: step 6620, loss = 2.21 (675.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:57.657078: step 6630, loss = 2.24 (678.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:59.531733: step 6640, loss = 2.29 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:34:01.428692: step 6650, loss = 2.22 (674.8 examples/sec; 0.190 sec/batch)
2017-05-18 07:34:03.302991: step 6660, loss = 2.26 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:34:05.194245: step 6670, loss = 2.28 (676.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:34:07.072145: step 6680, loss = 2.23 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:08.949133: step 6690, loss = 2.22 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:11.064146: step 6700, loss = 2.30 (649.5 examples/sec; 0.197 sec/batch), precision = 19.60%
2017-05-18 07:34:14.072834: step 6710, loss = 2.30 (406.0 examples/sec; 0.315 sec/batch)
2017-05-18 07:34:15.981869: step 6720, loss = 2.21 (670.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:34:17.858952: step 6730, loss = 2.29 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:19.735776: step 6740, loss = 2.28 (682.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:21.616485: step 6750, loss = 2.26 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:23.501113: step 6760, loss = 2.28 (679.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:25.378299: step 6770, loss = 2.21 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:27.249379: step 6780, loss = 2.26 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:34:29.124762: step 6790, loss = 2.18 (682.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:31.230692: step 6800, loss = 2.26 (652.6 examples/sec; 0.196 sec/batch), precision = 19.20%
2017-05-18 07:34:34.241680: step 6810, loss = 2.23 (405.7 examples/sec; 0.316 sec/batch)
2017-05-18 07:34:36.154837: step 6820, loss = 2.25 (669.0 examples/sec; 0.191 sec/batch)
2017-05-18 07:34:38.076173: step 6830, loss = 2.22 (666.2 examples/sec; 0.192 sec/batch)
2017-05-18 07:34:40.013696: step 6840, loss = 2.23 (660.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:34:41.956478: step 6850, loss = 2.25 (658.8 examples/sec; 0.194 sec/batch)
2017-05-18 07:34:43.906781: step 6860, loss = 2.25 (656.3 examples/sec; 0.195 sec/batch)
2017-05-18 07:34:45.846274: step 6870, loss = 2.30 (660.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:34:47.813724: step 6880, loss = 2.26 (650.6 examples/sec; 0.197 sec/batch)
2017-05-18 07:34:49.771808: step 6890, loss = 2.21 (653.7 examples/sec; 0.196 sec/batch)
2017-05-18 07:34:51.938436: step 6900, loss = 2.26 (634.0 examples/sec; 0.202 sec/batch), precision = 18.90%
2017-05-18 07:34:54.940819: step 6910, loss = 2.27 (406.3 examples/sec; 0.315 sec/batch)
2017-05-18 07:34:56.825216: step 6920, loss = 2.23 (679.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:58.687377: step 6930, loss = 2.26 (687.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:00.557708: step 6940, loss = 2.24 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:35:02.427800: step 6950, loss = 2.27 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:35:04.293935: step 6960, loss = 2.24 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:35:06.157368: step 6970, loss = 2.24 (686.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:08.019028: step 6980, loss = 2.22 (687.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:09.904091: step 6990, loss = 2.21 (679.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:35:12.017132: step 7000, loss = 2.28 (650.7 examples/sec; 0.197 sec/batch), precision = 18.70%
2017-05-18 07:35:15.088191: step 7010, loss = 2.27 (397.9 examples/sec; 0.322 sec/batch)
2017-05-18 07:35:17.026007: step 7020, loss = 2.23 (660.5 examples/sec; 0.194 sec/batch)
2017-05-18 07:35:18.896355: step 7030, loss = 2.23 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:35:20.787611: step 7040, loss = 2.18 (676.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:35:22.660925: step 7050, loss = 2.21 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:35:24.546904: step 7060, loss = 2.23 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:35:26.450723: step 7070, loss = 2.23 (672.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:35:28.348384: step 7080, loss = 2.25 (674.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:35:30.231759: step 7090, loss = 2.26 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:35:32.383489: step 7100, loss = 2.21 (638.6 examples/sec; 0.200 sec/batch), precision = 20.40%
2017-05-18 07:35:35.448681: step 7110, loss = 2.29 (398.4 examples/sec; 0.321 sec/batch)
2017-05-18 07:35:37.309398: step 7120, loss = 2.22 (687.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:39.164805: step 7130, loss = 2.26 (689.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:41.025022: step 7140, loss = 2.31 (688.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:42.881351: step 7150, loss = 2.23 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:44.733053: step 7160, loss = 2.25 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:35:46.583430: step 7170, loss = 2.27 (691.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:35:48.434305: step 7180, loss = 2.20 (691.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:35:50.297072: step 7190, loss = 2.25 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:52.390287: step 7200, loss = 2.26 (656.3 examples/sec; 0.195 sec/batch), precision = 18.80%
2017-05-18 07:35:55.335045: step 7210, loss = 2.26 (414.6 examples/sec; 0.309 sec/batch)
2017-05-18 07:35:57.218230: step 7220, loss = 2.25 (679.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:35:59.077664: step 7230, loss = 2.25 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:00.952909: step 7240, loss = 2.20 (682.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:36:02.822988: step 7250, loss = 2.25 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:36:04.680525: step 7260, loss = 2.27 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:06.547807: step 7270, loss = 2.25 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:36:08.411205: step 7280, loss = 2.21 (686.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:10.275813: step 7290, loss = 2.25 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:12.410861: step 7300, loss = 2.29 (644.0 examples/sec; 0.199 sec/batch), precision = 20.00%
2017-05-18 07:36:15.410575: step 7310, loss = 2.26 (406.7 examples/sec; 0.315 sec/batch)
2017-05-18 07:36:17.300647: step 7320, loss = 2.19 (677.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:36:19.161972: step 7330, loss = 2.24 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:21.033101: step 7340, loss = 2.27 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:36:22.888098: step 7350, loss = 2.25 (690.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:24.742600: step 7360, loss = 2.27 (690.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:26.590642: step 7370, loss = 2.25 (692.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:28.449853: step 7380, loss = 2.28 (688.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:30.303047: step 7390, loss = 2.23 (690.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:32.397110: step 7400, loss = 2.26 (656.4 examples/sec; 0.195 sec/batch), precision = 18.40%
2017-05-18 07:36:35.379525: step 7410, loss = 2.25 (409.4 examples/sec; 0.313 sec/batch)
2017-05-18 07:36:37.265473: step 7420, loss = 2.23 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:36:39.121526: step 7430, loss = 2.28 (689.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:40.995583: step 7440, loss = 2.24 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:36:42.860945: step 7450, loss = 2.26 (686.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:36:44.711222: step 7460, loss = 2.22 (691.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:46.564806: step 7470, loss = 2.23 (690.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:48.419479: step 7480, loss = 2.26 (690.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:50.284068: step 7490, loss = 2.24 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:52.371066: step 7500, loss = 2.22 (659.4 examples/sec; 0.194 sec/batch), precision = 20.50%
2017-05-18 07:36:55.339660: step 7510, loss = 2.24 (411.0 examples/sec; 0.311 sec/batch)
2017-05-18 07:36:57.247441: step 7520, loss = 2.25 (670.9 examples/sec; 0.191 sec/batch)
2017-05-18 07:36:59.159552: step 7530, loss = 2.24 (669.4 examples/sec; 0.191 sec/batch)
2017-05-18 07:37:01.091533: step 7540, loss = 2.24 (662.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:37:03.018587: step 7550, loss = 2.24 (664.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:37:04.939195: step 7560, loss = 2.27 (666.5 examples/sec; 0.192 sec/batch)
2017-05-18 07:37:06.872263: step 7570, loss = 2.25 (662.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:37:08.814550: step 7580, loss = 2.26 (659.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:37:10.735167: step 7590, loss = 2.26 (666.5 examples/sec; 0.192 sec/batch)
2017-05-18 07:37:12.910196: step 7600, loss = 2.25 (634.1 examples/sec; 0.202 sec/batch), precision = 20.00%
2017-05-18 07:37:16.009012: step 7610, loss = 2.22 (393.2 examples/sec; 0.326 sec/batch)
2017-05-18 07:37:17.900762: step 7620, loss = 2.24 (676.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:37:19.773393: step 7630, loss = 2.20 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:37:21.640777: step 7640, loss = 2.28 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:37:23.519324: step 7650, loss = 2.27 (681.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:25.399328: step 7660, loss = 2.23 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:27.263000: step 7670, loss = 2.23 (686.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:37:29.144736: step 7680, loss = 2.26 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:31.027000: step 7690, loss = 2.21 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:33.140433: step 7700, loss = 2.26 (650.0 examples/sec; 0.197 sec/batch), precision = 18.40%
2017-05-18 07:37:36.237157: step 7710, loss = 2.21 (395.0 examples/sec; 0.324 sec/batch)
2017-05-18 07:37:38.289282: step 7720, loss = 2.28 (623.7 examples/sec; 0.205 sec/batch)
2017-05-18 07:37:40.170802: step 7730, loss = 2.29 (680.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:42.048009: step 7740, loss = 2.20 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:43.919156: step 7750, loss = 2.24 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:37:45.801604: step 7760, loss = 2.22 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:47.705268: step 7770, loss = 2.24 (672.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:37:49.645953: step 7780, loss = 2.21 (659.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:37:51.573824: step 7790, loss = 2.20 (663.9 examples/sec; 0.193 sec/batch)
2017-05-18 07:37:53.726069: step 7800, loss = 2.25 (640.0 examples/sec; 0.200 sec/batch), precision = 18.60%
2017-05-18 07:37:56.767499: step 7810, loss = 2.32 (400.8 examples/sec; 0.319 sec/batch)
2017-05-18 07:37:58.673833: step 7820, loss = 2.26 (671.4 examples/sec; 0.191 sec/batch)
2017-05-18 07:38:00.567738: step 7830, loss = 2.22 (675.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:02.463804: step 7840, loss = 2.26 (675.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:38:04.334678: step 7850, loss = 2.23 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:38:06.223143: step 7860, loss = 2.26 (677.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:08.110735: step 7870, loss = 2.23 (678.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:09.996604: step 7880, loss = 2.20 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:11.873522: step 7890, loss = 2.23 (682.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:38:13.989295: step 7900, loss = 2.29 (648.6 examples/sec; 0.197 sec/batch), precision = 18.10%
2017-05-18 07:38:16.989080: step 7910, loss = 2.23 (407.4 examples/sec; 0.314 sec/batch)
2017-05-18 07:38:18.880900: step 7920, loss = 2.22 (676.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:20.743175: step 7930, loss = 2.26 (687.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:38:22.610779: step 7940, loss = 2.26 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:38:24.490794: step 7950, loss = 2.27 (680.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:38:26.356331: step 7960, loss = 2.24 (686.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:38:28.230781: step 7970, loss = 2.19 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:38:30.116019: step 7980, loss = 2.27 (679.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:32.004865: step 7990, loss = 2.24 (677.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:34.096113: step 8000, loss = 2.26 (656.6 examples/sec; 0.195 sec/batch), precision = 18.70%
2017-05-18 07:38:37.076847: step 8010, loss = 2.24 (409.9 examples/sec; 0.312 sec/batch)
2017-05-18 07:38:38.983707: step 8020, loss = 2.24 (671.3 examples/sec; 0.191 sec/batch)
2017-05-18 07:38:40.892396: step 8030, loss = 2.29 (670.6 examples/sec; 0.191 sec/batch)
2017-05-18 07:38:42.820231: step 8040, loss = 2.26 (664.0 examples/sec; 0.193 sec/batch)
2017-05-18 07:38:44.760451: step 8050, loss = 2.24 (659.7 examples/sec; 0.194 sec/batch)
2017-05-18 07:38:46.702227: step 8060, loss = 2.29 (659.2 examples/sec; 0.194 sec/batch)
2017-05-18 07:38:48.636635: step 8070, loss = 2.24 (661.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:38:50.584354: step 8080, loss = 2.24 (657.2 examples/sec; 0.195 sec/batch)
2017-05-18 07:38:52.514895: step 8090, loss = 2.23 (663.0 examples/sec; 0.193 sec/batch)
2017-05-18 07:38:54.676893: step 8100, loss = 2.28 (635.9 examples/sec; 0.201 sec/batch), precision = 19.10%
2017-05-18 07:38:57.662375: step 8110, loss = 2.22 (408.4 examples/sec; 0.313 sec/batch)
2017-05-18 07:38:59.560730: step 8120, loss = 2.21 (674.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:39:01.450479: step 8130, loss = 2.25 (677.3 examples/sec; 0.189 sec/batch)
2017-05-18 07:39:03.328958: step 8140, loss = 2.25 (681.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:39:05.207527: step 8150, loss = 2.27 (681.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:39:07.096766: step 8160, loss = 2.28 (677.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:39:09.105622: step 8170, loss = 2.25 (637.2 examples/sec; 0.201 sec/batch)
2017-05-18 07:39:10.982904: step 8180, loss = 2.23 (681.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:39:12.858920: step 8190, loss = 2.27 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:39:14.947762: step 8200, loss = 2.28 (657.8 examples/sec; 0.195 sec/batch), precision = 18.40%
2017-05-18 07:39:18.063305: step 8210, loss = 2.22 (392.8 examples/sec; 0.326 sec/batch)
2017-05-18 07:39:19.952291: step 8220, loss = 2.23 (677.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:39:21.847975: step 8230, loss = 2.28 (675.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:39:23.736051: step 8240, loss = 2.21 (677.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:39:25.614104: step 8250, loss = 2.24 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:39:27.501276: step 8260, loss = 2.24 (678.3 examples/sec; 0.189 sec/batch)
2017-05-18 07:39:29.418243: step 8270, loss = 2.26 (667.7 examples/sec; 0.192 sec/batch)
2017-05-18 07:39:31.343424: step 8280, loss = 2.30 (664.9 examples/sec; 0.193 sec/batch)
2017-05-18 07:39:33.279891: step 8290, loss = 2.26 (661.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:39:35.482246: step 8300, loss = 2.24 (626.0 examples/sec; 0.204 sec/batch), precision = 18.50%
2017-05-18 07:39:38.661631: step 8310, loss = 2.25 (383.6 examples/sec; 0.334 sec/batch)
2017-05-18 07:39:41.394524: step 8320, loss = 2.27 (468.4 examples/sec; 0.273 sec/batch)
2017-05-18 07:39:43.718710: step 8330, loss = 2.27 (550.7 examples/sec; 0.232 sec/batch)
2017-05-18 07:39:45.922728: step 8340, loss = 2.25 (580.8 examples/sec; 0.220 sec/batch)
2017-05-18 07:39:48.074378: step 8350, loss = 2.23 (594.9 examples/sec; 0.215 sec/batch)
2017-05-18 07:39:50.951588: step 8360, loss = 2.31 (444.9 examples/sec; 0.288 sec/batch)
2017-05-18 07:39:53.607977: step 8370, loss = 2.22 (481.9 examples/sec; 0.266 sec/batch)
2017-05-18 07:39:55.947432: step 8380, loss = 2.22 (547.1 examples/sec; 0.234 sec/batch)
2017-05-18 07:39:58.071002: step 8390, loss = 2.28 (602.8 examples/sec; 0.212 sec/batch)
2017-05-18 07:40:00.183703: step 8400, loss = 2.21 (650.2 examples/sec; 0.197 sec/batch), precision = 18.60%
2017-05-18 07:40:03.225425: step 8410, loss = 2.25 (401.8 examples/sec; 0.319 sec/batch)
2017-05-18 07:40:05.102107: step 8420, loss = 2.29 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:40:06.968657: step 8430, loss = 2.24 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:40:08.845956: step 8440, loss = 2.28 (681.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:40:10.714493: step 8450, loss = 2.17 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:40:12.620797: step 8460, loss = 2.25 (671.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:40:14.491450: step 8470, loss = 2.26 (684.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:40:16.408785: step 8480, loss = 2.24 (667.6 examples/sec; 0.192 sec/batch)
2017-05-18 07:40:18.329165: step 8490, loss = 2.27 (666.5 examples/sec; 0.192 sec/batch)
2017-05-18 07:40:20.538317: step 8500, loss = 2.21 (621.9 examples/sec; 0.206 sec/batch), precision = 20.90%
2017-05-18 07:40:23.743505: step 8510, loss = 2.21 (381.4 examples/sec; 0.336 sec/batch)
2017-05-18 07:40:25.877228: step 8520, loss = 2.27 (599.9 examples/sec; 0.213 sec/batch)
2017-05-18 07:40:27.947663: step 8530, loss = 2.21 (618.2 examples/sec; 0.207 sec/batch)
2017-05-18 07:40:30.001652: step 8540, loss = 2.23 (623.2 examples/sec; 0.205 sec/batch)
2017-05-18 07:40:32.079660: step 8550, loss = 2.20 (616.0 examples/sec; 0.208 sec/batch)
2017-05-18 07:40:34.192366: step 8560, loss = 2.26 (605.9 examples/sec; 0.211 sec/batch)
2017-05-18 07:40:36.291833: step 8570, loss = 2.23 (609.7 examples/sec; 0.210 sec/batch)
2017-05-18 07:40:38.376608: step 8580, loss = 2.28 (614.0 examples/sec; 0.208 sec/batch)
2017-05-18 07:40:40.251536: step 8590, loss = 2.24 (682.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:40:42.347762: step 8600, loss = 2.24 (655.9 examples/sec; 0.195 sec/batch), precision = 20.20%
2017-05-18 07:40:45.332657: step 8610, loss = 2.27 (409.0 examples/sec; 0.313 sec/batch)
2017-05-18 07:40:47.215523: step 8620, loss = 2.23 (679.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:40:49.085640: step 8630, loss = 2.23 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:40:50.954383: step 8640, loss = 2.23 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:40:52.820923: step 8650, loss = 2.23 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:40:54.694092: step 8660, loss = 2.26 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:40:56.564254: step 8670, loss = 2.23 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:40:58.432110: step 8680, loss = 2.19 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:41:00.330789: step 8690, loss = 2.28 (674.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:41:02.428798: step 8700, loss = 2.26 (654.0 examples/sec; 0.196 sec/batch), precision = 19.90%
2017-05-18 07:41:05.394554: step 8710, loss = 2.24 (412.0 examples/sec; 0.311 sec/batch)
2017-05-18 07:41:07.295037: step 8720, loss = 2.21 (673.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:41:09.206630: step 8730, loss = 2.25 (669.6 examples/sec; 0.191 sec/batch)
2017-05-18 07:41:11.136006: step 8740, loss = 2.27 (663.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:41:13.092554: step 8750, loss = 2.25 (654.2 examples/sec; 0.196 sec/batch)
2017-05-18 07:41:15.020425: step 8760, loss = 2.29 (663.9 examples/sec; 0.193 sec/batch)
2017-05-18 07:41:16.959735: step 8770, loss = 2.25 (660.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:41:18.877973: step 8780, loss = 2.22 (667.3 examples/sec; 0.192 sec/batch)
2017-05-18 07:41:20.804357: step 8790, loss = 2.23 (664.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:41:22.969434: step 8800, loss = 2.29 (637.0 examples/sec; 0.201 sec/batch), precision = 18.10%
2017-05-18 07:41:26.026502: step 8810, loss = 2.29 (398.4 examples/sec; 0.321 sec/batch)
2017-05-18 07:41:28.473666: step 8820, loss = 2.25 (523.1 examples/sec; 0.245 sec/batch)
2017-05-18 07:41:30.189921: step 8830, loss = 2.25 (745.8 examples/sec; 0.172 sec/batch)
2017-05-18 07:41:32.748981: step 8840, loss = 2.22 (500.2 examples/sec; 0.256 sec/batch)
2017-05-18 07:41:35.302483: step 8850, loss = 2.22 (501.3 examples/sec; 0.255 sec/batch)
2017-05-18 07:41:37.771468: step 8860, loss = 2.24 (518.4 examples/sec; 0.247 sec/batch)
2017-05-18 07:41:40.236971: step 8870, loss = 2.27 (519.2 examples/sec; 0.247 sec/batch)
2017-05-18 07:41:42.767781: step 8880, loss = 2.27 (505.8 examples/sec; 0.253 sec/batch)
2017-05-18 07:41:45.244414: step 8890, loss = 2.22 (516.8 examples/sec; 0.248 sec/batch)
2017-05-18 07:41:47.988274: step 8900, loss = 2.23 (494.8 examples/sec; 0.259 sec/batch), precision = 17.90%
2017-05-18 07:41:51.854792: step 8910, loss = 2.20 (318.1 examples/sec; 0.402 sec/batch)
2017-05-18 07:41:54.364622: step 8920, loss = 2.29 (510.0 examples/sec; 0.251 sec/batch)
2017-05-18 07:41:56.878402: step 8930, loss = 2.26 (509.2 examples/sec; 0.251 sec/batch)
2017-05-18 07:41:59.358224: step 8940, loss = 2.18 (516.2 examples/sec; 0.248 sec/batch)
2017-05-18 07:42:02.049823: step 8950, loss = 2.23 (475.6 examples/sec; 0.269 sec/batch)
2017-05-18 07:42:04.412393: step 8960, loss = 2.27 (541.8 examples/sec; 0.236 sec/batch)
2017-05-18 07:42:06.743901: step 8970, loss = 2.19 (549.0 examples/sec; 0.233 sec/batch)
2017-05-18 07:42:08.866294: step 8980, loss = 2.26 (603.1 examples/sec; 0.212 sec/batch)
2017-05-18 07:42:11.352812: step 8990, loss = 2.20 (514.8 examples/sec; 0.249 sec/batch)
2017-05-18 07:42:14.175331: step 9000, loss = 2.27 (480.6 examples/sec; 0.266 sec/batch), precision = 19.10%
2017-05-18 07:42:18.364430: step 9010, loss = 2.25 (294.4 examples/sec; 0.435 sec/batch)
2017-05-18 07:42:20.615009: step 9020, loss = 2.27 (568.7 examples/sec; 0.225 sec/batch)
2017-05-18 07:42:23.314490: step 9030, loss = 2.22 (474.2 examples/sec; 0.270 sec/batch)
2017-05-18 07:42:25.167450: step 9040, loss = 2.25 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:42:27.016094: step 9050, loss = 2.24 (692.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:42:28.874104: step 9060, loss = 2.25 (688.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:42:30.731970: step 9070, loss = 2.25 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:42:32.603595: step 9080, loss = 2.20 (683.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:42:34.457204: step 9090, loss = 2.26 (690.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:42:36.545324: step 9100, loss = 2.28 (658.5 examples/sec; 0.194 sec/batch), precision = 18.70%
2017-05-18 07:42:39.502116: step 9110, loss = 2.19 (412.7 examples/sec; 0.310 sec/batch)
2017-05-18 07:42:41.363663: step 9120, loss = 2.23 (687.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:42:43.219041: step 9130, loss = 2.27 (689.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:42:45.078052: step 9140, loss = 2.19 (688.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:42:46.930582: step 9150, loss = 2.26 (690.9 examples/sec; 0.185 sec/batch)
2017-05-18 07:42:48.787954: step 9160, loss = 2.19 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:42:50.640855: step 9170, loss = 2.29 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:42:52.489223: step 9180, loss = 2.19 (692.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:42:54.344779: step 9190, loss = 2.22 (689.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:42:56.461936: step 9200, loss = 2.25 (650.9 examples/sec; 0.197 sec/batch), precision = 17.60%
2017-05-18 07:42:59.416690: step 9210, loss = 2.25 (412.2 examples/sec; 0.311 sec/batch)
2017-05-18 07:43:01.302908: step 9220, loss = 2.26 (678.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:43:03.163927: step 9230, loss = 2.31 (687.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:43:05.006302: step 9240, loss = 2.27 (694.8 examples/sec; 0.184 sec/batch)
2017-05-18 07:43:06.865366: step 9250, loss = 2.22 (688.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:43:08.727054: step 9260, loss = 2.22 (687.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:43:10.598319: step 9270, loss = 2.28 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:43:12.455512: step 9280, loss = 2.24 (689.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:43:14.310037: step 9290, loss = 2.26 (690.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:43:16.398631: step 9300, loss = 2.23 (657.7 examples/sec; 0.195 sec/batch), precision = 19.50%
2017-05-18 07:43:19.393684: step 9310, loss = 2.22 (408.0 examples/sec; 0.314 sec/batch)
2017-05-18 07:43:21.261154: step 9320, loss = 2.21 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:43:23.110289: step 9330, loss = 2.24 (692.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:43:24.957506: step 9340, loss = 2.26 (692.9 examples/sec; 0.185 sec/batch)
2017-05-18 07:43:26.814635: step 9350, loss = 2.22 (689.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:43:28.666193: step 9360, loss = 2.25 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:43:30.499793: step 9370, loss = 2.27 (698.1 examples/sec; 0.183 sec/batch)
2017-05-18 07:43:32.349335: step 9380, loss = 2.27 (692.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:43:34.198859: step 9390, loss = 2.26 (692.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:43:36.283663: step 9400, loss = 2.22 (660.1 examples/sec; 0.194 sec/batch), precision = 18.90%
2017-05-18 07:43:39.286897: step 9410, loss = 2.23 (406.5 examples/sec; 0.315 sec/batch)
2017-05-18 07:43:41.179364: step 9420, loss = 2.19 (676.4 examples/sec; 0.189 sec/batch)
2017-05-18 07:43:43.059938: step 9430, loss = 2.24 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:43:44.961422: step 9440, loss = 2.19 (673.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:43:46.867540: step 9450, loss = 2.20 (671.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:43:48.774462: step 9460, loss = 2.29 (671.2 examples/sec; 0.191 sec/batch)
2017-05-18 07:43:50.685416: step 9470, loss = 2.25 (669.8 examples/sec; 0.191 sec/batch)
2017-05-18 07:43:52.589707: step 9480, loss = 2.26 (672.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:43:54.501705: step 9490, loss = 2.28 (669.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:43:56.669952: step 9500, loss = 2.29 (635.4 examples/sec; 0.201 sec/batch), precision = 17.50%
2017-05-18 07:43:59.686544: step 9510, loss = 2.24 (403.8 examples/sec; 0.317 sec/batch)
2017-05-18 07:44:01.549075: step 9520, loss = 2.26 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:44:03.406946: step 9530, loss = 2.21 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:44:05.261825: step 9540, loss = 2.27 (690.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:44:07.107030: step 9550, loss = 2.22 (693.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:44:08.967504: step 9560, loss = 2.20 (688.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:44:10.831175: step 9570, loss = 2.20 (686.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:44:12.682807: step 9580, loss = 2.25 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:44:14.534941: step 9590, loss = 2.23 (691.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:44:16.628245: step 9600, loss = 2.24 (656.4 examples/sec; 0.195 sec/batch), precision = 19.40%
2017-05-18 07:44:19.665590: step 9610, loss = 2.25 (402.5 examples/sec; 0.318 sec/batch)
2017-05-18 07:44:21.514298: step 9620, loss = 2.24 (692.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:44:23.378545: step 9630, loss = 2.20 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:44:25.248813: step 9640, loss = 2.20 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:44:27.106277: step 9650, loss = 2.25 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:44:28.973311: step 9660, loss = 2.19 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:44:30.862057: step 9670, loss = 2.29 (677.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:44:32.775430: step 9680, loss = 2.23 (669.0 examples/sec; 0.191 sec/batch)
2017-05-18 07:44:34.691912: step 9690, loss = 2.23 (667.9 examples/sec; 0.192 sec/batch)
2017-05-18 07:44:36.850520: step 9700, loss = 2.26 (636.6 examples/sec; 0.201 sec/batch), precision = 17.70%
2017-05-18 07:44:39.862587: step 9710, loss = 2.24 (405.1 examples/sec; 0.316 sec/batch)
2017-05-18 07:44:41.739650: step 9720, loss = 2.25 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:44:43.595948: step 9730, loss = 2.20 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:44:45.443236: step 9740, loss = 2.22 (692.9 examples/sec; 0.185 sec/batch)
2017-05-18 07:44:47.288518: step 9750, loss = 2.26 (693.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:44:49.148844: step 9760, loss = 2.22 (688.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:44:51.001834: step 9770, loss = 2.17 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:44:52.867952: step 9780, loss = 2.29 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:44:54.732762: step 9790, loss = 2.24 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:44:56.806373: step 9800, loss = 2.24 (662.8 examples/sec; 0.193 sec/batch), precision = 20.70%
2017-05-18 07:44:59.829952: step 9810, loss = 2.21 (404.3 examples/sec; 0.317 sec/batch)
2017-05-18 07:45:01.721522: step 9820, loss = 2.22 (676.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:45:03.580397: step 9830, loss = 2.27 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:45:05.446077: step 9840, loss = 2.25 (686.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:45:07.304208: step 9850, loss = 2.22 (688.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:45:09.154620: step 9860, loss = 2.23 (691.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:45:11.004786: step 9870, loss = 2.27 (691.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:45:12.857699: step 9880, loss = 2.27 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:45:14.708965: step 9890, loss = 2.25 (691.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:45:16.802150: step 9900, loss = 2.20 (661.4 examples/sec; 0.194 sec/batch), precision = 20.30%
2017-05-18 07:45:19.873654: step 9910, loss = 2.22 (396.4 examples/sec; 0.323 sec/batch)
2017-05-18 07:45:21.774092: step 9920, loss = 2.24 (673.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:45:23.661909: step 9930, loss = 2.26 (678.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:45:25.569169: step 9940, loss = 2.25 (671.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:45:27.494188: step 9950, loss = 2.25 (664.9 examples/sec; 0.193 sec/batch)
2017-05-18 07:45:29.410628: step 9960, loss = 2.21 (667.9 examples/sec; 0.192 sec/batch)
2017-05-18 07:45:31.342272: step 9970, loss = 2.24 (662.6 examples/sec; 0.193 sec/batch)
2017-05-18 07:45:33.253349: step 9980, loss = 2.20 (669.8 examples/sec; 0.191 sec/batch)
2017-05-18 07:45:35.175971: step 9990, loss = 2.25 (665.8 examples/sec; 0.192 sec/batch)
2017-05-18 07:45:37.328611: step 10000, loss = 2.22 (639.2 examples/sec; 0.200 sec/batch), precision = 18.40%
2017-05-18 07:45:40.342228: step 10010, loss = 2.26 (404.6 examples/sec; 0.316 sec/batch)
2017-05-18 07:45:42.218007: step 10020, loss = 2.26 (682.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:45:44.079360: step 10030, loss = 2.24 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:45:45.945932: step 10040, loss = 2.20 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:45:47.801456: step 10050, loss = 2.26 (689.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:45:49.656683: step 10060, loss = 2.27 (689.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:45:51.521143: step 10070, loss = 2.21 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:45:53.383539: step 10080, loss = 2.28 (687.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:45:55.251598: step 10090, loss = 2.23 (685.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:45:57.333781: step 10100, loss = 2.29 (662.5 examples/sec; 0.193 sec/batch), precision = 19.00%
2017-05-18 07:46:00.354395: step 10110, loss = 2.19 (403.7 examples/sec; 0.317 sec/batch)
2017-05-18 07:46:02.227927: step 10120, loss = 2.25 (683.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:46:04.074727: step 10130, loss = 2.28 (693.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:46:05.921825: step 10140, loss = 2.23 (693.0 examples/sec; 0.185 sec/batch)
2017-05-18 07:46:07.760663: step 10150, loss = 2.27 (696.1 examples/sec; 0.184 sec/batch)
2017-05-18 07:46:09.618802: step 10160, loss = 2.25 (688.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:46:11.516105: step 10170, loss = 2.26 (674.6 examples/sec; 0.190 sec/batch)
2017-05-18 07:46:13.404389: step 10180, loss = 2.28 (677.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:46:15.323223: step 10190, loss = 2.31 (667.1 examples/sec; 0.192 sec/batch)
2017-05-18 07:46:17.476754: step 10200, loss = 2.20 (640.2 examples/sec; 0.200 sec/batch), precision = 19.90%
2017-05-18 07:46:20.471659: step 10210, loss = 2.23 (406.5 examples/sec; 0.315 sec/batch)
2017-05-18 07:46:22.367763: step 10220, loss = 2.25 (675.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:46:24.216195: step 10230, loss = 2.26 (692.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:46:26.063235: step 10240, loss = 2.26 (693.0 examples/sec; 0.185 sec/batch)
2017-05-18 07:46:27.927476: step 10250, loss = 2.22 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:46:29.785792: step 10260, loss = 2.21 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:46:31.645968: step 10270, loss = 2.22 (688.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:46:33.505893: step 10280, loss = 2.26 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:46:35.378368: step 10290, loss = 2.23 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:46:37.462859: step 10300, loss = 2.24 (659.0 examples/sec; 0.194 sec/batch), precision = 18.00%
2017-05-18 07:46:40.438348: step 10310, loss = 2.20 (410.6 examples/sec; 0.312 sec/batch)
2017-05-18 07:46:42.312418: step 10320, loss = 2.22 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:46:44.163660: step 10330, loss = 2.29 (691.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:46:46.018314: step 10340, loss = 2.23 (690.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:46:47.876701: step 10350, loss = 2.23 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:46:49.728688: step 10360, loss = 2.21 (691.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:46:51.584246: step 10370, loss = 2.18 (689.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:46:53.442517: step 10380, loss = 2.15 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:46:55.309379: step 10390, loss = 2.27 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:46:57.396393: step 10400, loss = 2.24 (658.5 examples/sec; 0.194 sec/batch), precision = 20.30%
2017-05-18 07:47:00.360469: step 10410, loss = 2.27 (411.9 examples/sec; 0.311 sec/batch)
2017-05-18 07:47:02.256220: step 10420, loss = 2.24 (675.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:47:04.147224: step 10430, loss = 2.27 (676.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:47:06.055088: step 10440, loss = 2.26 (670.9 examples/sec; 0.191 sec/batch)
2017-05-18 07:47:07.970848: step 10450, loss = 2.22 (668.1 examples/sec; 0.192 sec/batch)
2017-05-18 07:47:09.880592: step 10460, loss = 2.26 (670.2 examples/sec; 0.191 sec/batch)
2017-05-18 07:47:11.790678: step 10470, loss = 2.25 (670.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:47:13.696746: step 10480, loss = 2.22 (671.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:47:15.601531: step 10490, loss = 2.28 (672.0 examples/sec; 0.190 sec/batch)
2017-05-18 07:47:17.758672: step 10500, loss = 2.17 (638.6 examples/sec; 0.200 sec/batch), precision = 19.20%
2017-05-18 07:47:20.865314: step 10510, loss = 2.22 (392.7 examples/sec; 0.326 sec/batch)
2017-05-18 07:47:22.734813: step 10520, loss = 2.25 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:47:24.570551: step 10530, loss = 2.23 (697.3 examples/sec; 0.184 sec/batch)
2017-05-18 07:47:26.410853: step 10540, loss = 2.24 (695.5 examples/sec; 0.184 sec/batch)
2017-05-18 07:47:28.262492: step 10550, loss = 2.32 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:47:30.105549: step 10560, loss = 2.21 (694.5 examples/sec; 0.184 sec/batch)
2017-05-18 07:47:31.946110: step 10570, loss = 2.23 (695.4 examples/sec; 0.184 sec/batch)
2017-05-18 07:47:33.795586: step 10580, loss = 2.21 (692.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:47:35.649700: step 10590, loss = 2.22 (690.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:47:37.720002: step 10600, loss = 2.26 (664.4 examples/sec; 0.193 sec/batch), precision = 21.90%
2017-05-18 07:47:40.700527: step 10610, loss = 2.26 (409.7 examples/sec; 0.312 sec/batch)
2017-05-18 07:47:42.576848: step 10620, loss = 2.25 (682.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:47:44.424936: step 10630, loss = 2.26 (692.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:47:46.269197: step 10640, loss = 2.25 (694.0 examples/sec; 0.184 sec/batch)
2017-05-18 07:47:48.132406: step 10650, loss = 2.22 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:47:49.991277: step 10660, loss = 2.28 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:47:51.877823: step 10670, loss = 2.24 (678.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:47:53.757536: step 10680, loss = 2.24 (681.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:47:55.640174: step 10690, loss = 2.19 (679.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:47:57.744869: step 10700, loss = 2.21 (654.0 examples/sec; 0.196 sec/batch), precision = 19.60%
2017-05-18 07:48:00.756835: step 10710, loss = 2.27 (405.1 examples/sec; 0.316 sec/batch)
2017-05-18 07:48:02.623167: step 10720, loss = 2.28 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:48:04.487498: step 10730, loss = 2.21 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:48:06.345566: step 10740, loss = 2.20 (688.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:48:08.200694: step 10750, loss = 2.23 (690.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:48:10.057679: step 10760, loss = 2.25 (689.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:48:11.911476: step 10770, loss = 2.23 (690.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:48:13.759681: step 10780, loss = 2.25 (692.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:48:15.612669: step 10790, loss = 2.24 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:48:17.689524: step 10800, loss = 2.24 (661.6 examples/sec; 0.193 sec/batch), precision = 19.50%
2017-05-18 07:48:20.671577: step 10810, loss = 2.25 (409.7 examples/sec; 0.312 sec/batch)
2017-05-18 07:48:22.526557: step 10820, loss = 2.22 (690.0 examples/sec; 0.185 sec/batch)
2017-05-18 07:48:24.367447: step 10830, loss = 2.28 (695.3 examples/sec; 0.184 sec/batch)
2017-05-18 07:48:26.211094: step 10840, loss = 2.24 (694.3 examples/sec; 0.184 sec/batch)
2017-05-18 07:48:28.057863: step 10850, loss = 2.26 (693.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:48:29.906859: step 10860, loss = 2.24 (692.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:48:31.761841: step 10870, loss = 2.24 (690.0 examples/sec; 0.185 sec/batch)
2017-05-18 07:48:33.628422: step 10880, loss = 2.28 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:48:35.483507: step 10890, loss = 2.26 (690.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:48:37.588095: step 10900, loss = 2.29 (654.2 examples/sec; 0.196 sec/batch), precision = 17.80%
2017-05-18 07:48:40.576809: step 10910, loss = 2.23 (408.1 examples/sec; 0.314 sec/batch)
2017-05-18 07:48:42.448372: step 10920, loss = 2.16 (683.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:48:44.303609: step 10930, loss = 2.27 (689.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:48:46.152875: step 10940, loss = 2.31 (692.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:48:48.000476: step 10950, loss = 2.24 (692.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:48:49.859694: step 10960, loss = 2.22 (688.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:48:51.718182: step 10970, loss = 2.18 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:48:53.560861: step 10980, loss = 2.25 (694.6 examples/sec; 0.184 sec/batch)
2017-05-18 07:48:55.409329: step 10990, loss = 2.31 (692.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:48:57.475849: step 11000, loss = 2.25 (664.8 examples/sec; 0.193 sec/batch), precision = 18.40%
2017-05-18 07:49:00.452879: step 11010, loss = 2.22 (410.5 examples/sec; 0.312 sec/batch)
2017-05-18 07:49:02.322430: step 11020, loss = 2.23 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:49:04.180083: step 11030, loss = 2.23 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:49:06.037685: step 11040, loss = 2.20 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:49:07.902220: step 11050, loss = 2.26 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:49:09.755138: step 11060, loss = 2.23 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:49:11.624115: step 11070, loss = 2.21 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:49:13.468303: step 11080, loss = 2.23 (694.1 examples/sec; 0.184 sec/batch)
2017-05-18 07:49:15.326950: step 11090, loss = 2.23 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:49:17.403631: step 11100, loss = 2.22 (661.7 examples/sec; 0.193 sec/batch), precision = 21.50%
2017-05-18 07:49:20.469882: step 11110, loss = 2.27 (398.9 examples/sec; 0.321 sec/batch)
2017-05-18 07:49:22.387436: step 11120, loss = 2.26 (667.7 examples/sec; 0.192 sec/batch)
2017-05-18 07:49:24.275662: step 11130, loss = 2.19 (677.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:49:26.184708: step 11140, loss = 2.26 (670.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:49:28.114620: step 11150, loss = 2.20 (663.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:49:30.048135: step 11160, loss = 2.29 (662.0 examples/sec; 0.193 sec/batch)
2017-05-18 07:49:31.968756: step 11170, loss = 2.26 (666.5 examples/sec; 0.192 sec/batch)
2017-05-18 07:49:33.905800: step 11180, loss = 2.18 (660.8 examples/sec; 0.194 sec/batch)
2017-05-18 07:49:35.837506: step 11190, loss = 2.25 (662.6 examples/sec; 0.193 sec/batch)
2017-05-18 07:49:38.013303: step 11200, loss = 2.22 (633.1 examples/sec; 0.202 sec/batch), precision = 18.60%
2017-05-18 07:49:41.002010: step 11210, loss = 2.22 (407.3 examples/sec; 0.314 sec/batch)
2017-05-18 07:49:42.895338: step 11220, loss = 2.23 (676.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:49:44.758401: step 11230, loss = 2.23 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:49:46.612512: step 11240, loss = 2.23 (690.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:49:48.483922: step 11250, loss = 2.20 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:49:50.346494: step 11260, loss = 2.18 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:49:52.209531: step 11270, loss = 2.24 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:49:54.078953: step 11280, loss = 2.18 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:49:55.942916: step 11290, loss = 2.23 (686.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:49:58.024441: step 11300, loss = 2.21 (659.7 examples/sec; 0.194 sec/batch), precision = 18.30%
2017-05-18 07:50:01.010151: step 11310, loss = 2.28 (409.4 examples/sec; 0.313 sec/batch)
2017-05-18 07:50:02.910386: step 11320, loss = 2.23 (673.6 examples/sec; 0.190 sec/batch)
2017-05-18 07:50:04.782072: step 11330, loss = 2.23 (683.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:50:06.645884: step 11340, loss = 2.21 (686.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:50:08.507097: step 11350, loss = 2.30 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:50:10.387152: step 11360, loss = 2.26 (680.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:50:12.297578: step 11370, loss = 2.27 (670.0 examples/sec; 0.191 sec/batch)
2017-05-18 07:50:14.219791: step 11380, loss = 2.26 (665.9 examples/sec; 0.192 sec/batch)
2017-05-18 07:50:16.140782: step 11390, loss = 2.25 (666.3 examples/sec; 0.192 sec/batch)
2017-05-18 07:50:18.306233: step 11400, loss = 2.23 (635.6 examples/sec; 0.201 sec/batch), precision = 18.70%
2017-05-18 07:50:21.305522: step 11410, loss = 2.20 (406.2 examples/sec; 0.315 sec/batch)
2017-05-18 07:50:23.181413: step 11420, loss = 2.22 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:50:25.048625: step 11430, loss = 2.19 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:50:26.914704: step 11440, loss = 2.31 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:50:28.772561: step 11450, loss = 2.21 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:50:30.641941: step 11460, loss = 2.26 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:50:32.508740: step 11470, loss = 2.25 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:50:34.369699: step 11480, loss = 2.20 (687.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:50:36.233483: step 11490, loss = 2.22 (686.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:50:38.324122: step 11500, loss = 2.23 (658.3 examples/sec; 0.194 sec/batch), precision = 18.00%
2017-05-18 07:50:41.284864: step 11510, loss = 2.16 (412.0 examples/sec; 0.311 sec/batch)
2017-05-18 07:50:43.171700: step 11520, loss = 2.28 (678.4 examples/sec; 0.189 sec/batch)
2017-05-18 07:50:45.038018: step 11530, loss = 2.21 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:50:46.903874: step 11540, loss = 2.26 (686.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:50:48.763592: step 11550, loss = 2.24 (688.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:50:50.643135: step 11560, loss = 2.27 (681.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:50:52.510596: step 11570, loss = 2.24 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:50:54.360621: step 11580, loss = 2.18 (691.9 examples/sec; 0.185 sec/batch)
2017-05-18 07:50:56.223481: step 11590, loss = 2.20 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:50:58.319613: step 11600, loss = 2.27 (655.9 examples/sec; 0.195 sec/batch), precision = 18.90%
2017-05-18 07:51:01.285777: step 11610, loss = 2.27 (411.5 examples/sec; 0.311 sec/batch)
2017-05-18 07:51:03.178223: step 11620, loss = 2.24 (676.4 examples/sec; 0.189 sec/batch)
2017-05-18 07:51:05.076220: step 11630, loss = 2.23 (674.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:51:06.971872: step 11640, loss = 2.24 (675.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:51:08.860887: step 11650, loss = 2.29 (677.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:51:10.760467: step 11660, loss = 2.25 (673.8 examples/sec; 0.190 sec/batch)
2017-05-18 07:51:12.679607: step 11670, loss = 2.29 (667.0 examples/sec; 0.192 sec/batch)
2017-05-18 07:51:14.594406: step 11680, loss = 2.24 (668.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:51:16.513739: step 11690, loss = 2.23 (666.9 examples/sec; 0.192 sec/batch)
2017-05-18 07:51:18.663767: step 11700, loss = 2.18 (640.5 examples/sec; 0.200 sec/batch), precision = 18.90%
2017-05-18 07:51:21.627050: step 11710, loss = 2.25 (410.9 examples/sec; 0.311 sec/batch)
2017-05-18 07:51:23.507730: step 11720, loss = 2.24 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:51:25.371431: step 11730, loss = 2.24 (686.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:51:28.391423: step 11740, loss = 2.23 (423.8 examples/sec; 0.302 sec/batch)
2017-05-18 07:51:29.460736: step 11750, loss = 2.23 (1197.0 examples/sec; 0.107 sec/batch)
2017-05-18 07:51:31.317465: step 11760, loss = 2.21 (689.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:51:33.173928: step 11770, loss = 2.29 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:51:35.023101: step 11780, loss = 2.24 (692.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:51:36.872633: step 11790, loss = 2.21 (692.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:51:38.974577: step 11800, loss = 2.26 (654.0 examples/sec; 0.196 sec/batch), precision = 18.40%
2017-05-18 07:51:41.999535: step 11810, loss = 2.22 (403.8 examples/sec; 0.317 sec/batch)
2017-05-18 07:51:43.857387: step 11820, loss = 2.21 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:51:45.709503: step 11830, loss = 2.24 (691.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:51:47.580590: step 11840, loss = 2.22 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:51:49.431630: step 11850, loss = 2.22 (691.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:51:51.289175: step 11860, loss = 2.23 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:51:53.144374: step 11870, loss = 2.28 (690.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:51:55.004204: step 11880, loss = 2.25 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:51:56.871853: step 11890, loss = 2.23 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:51:58.997014: step 11900, loss = 2.23 (646.1 examples/sec; 0.198 sec/batch), precision = 18.90%
2017-05-18 07:52:02.009596: step 11910, loss = 2.20 (405.5 examples/sec; 0.316 sec/batch)
2017-05-18 07:52:03.882746: step 11920, loss = 2.23 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:52:05.734109: step 11930, loss = 2.19 (691.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:52:07.595898: step 11940, loss = 2.23 (687.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:52:09.452024: step 11950, loss = 2.21 (689.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:52:11.313851: step 11960, loss = 2.25 (687.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:52:13.186864: step 11970, loss = 2.26 (683.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:52:15.045806: step 11980, loss = 2.23 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:52:16.902686: step 11990, loss = 2.21 (689.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:52:18.999327: step 12000, loss = 2.24 (656.3 examples/sec; 0.195 sec/batch), precision = 18.70%
2017-05-18 07:52:22.003528: step 12010, loss = 2.30 (406.3 examples/sec; 0.315 sec/batch)
2017-05-18 07:52:23.887667: step 12020, loss = 2.23 (679.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:52:25.743240: step 12030, loss = 2.20 (689.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:52:27.599526: step 12040, loss = 2.25 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:52:29.466288: step 12050, loss = 2.23 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:52:31.324207: step 12060, loss = 2.21 (688.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:52:33.194609: step 12070, loss = 2.31 (684.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:52:35.054080: step 12080, loss = 2.18 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:52:36.915912: step 12090, loss = 2.23 (687.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:52:39.003920: step 12100, loss = 2.23 (658.0 examples/sec; 0.195 sec/batch), precision = 19.70%
2017-05-18 07:52:41.983913: step 12110, loss = 2.21 (409.9 examples/sec; 0.312 sec/batch)
2017-05-18 07:52:43.897249: step 12120, loss = 2.23 (669.0 examples/sec; 0.191 sec/batch)
2017-05-18 07:52:45.800316: step 12130, loss = 2.23 (672.6 examples/sec; 0.190 sec/batch)
2017-05-18 07:52:47.701873: step 12140, loss = 2.21 (673.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:52:49.607359: step 12150, loss = 2.22 (671.8 examples/sec; 0.191 sec/batch)
2017-05-18 07:52:51.535020: step 12160, loss = 2.21 (664.0 examples/sec; 0.193 sec/batch)
2017-05-18 07:52:53.479107: step 12170, loss = 2.23 (658.4 examples/sec; 0.194 sec/batch)
2017-05-18 07:52:55.422189: step 12180, loss = 2.24 (658.7 examples/sec; 0.194 sec/batch)
2017-05-18 07:52:57.358442: step 12190, loss = 2.24 (661.1 examples/sec; 0.194 sec/batch)
2017-05-18 07:52:59.543467: step 12200, loss = 2.28 (630.9 examples/sec; 0.203 sec/batch), precision = 19.20%
2017-05-18 07:53:02.540755: step 12210, loss = 2.24 (405.9 examples/sec; 0.315 sec/batch)
2017-05-18 07:53:04.431848: step 12220, loss = 2.26 (676.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:53:06.302560: step 12230, loss = 2.22 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:53:08.169252: step 12240, loss = 2.26 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:53:10.030564: step 12250, loss = 2.19 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:53:11.906309: step 12260, loss = 2.24 (682.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:53:13.769151: step 12270, loss = 2.26 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:53:15.639495: step 12280, loss = 2.20 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:53:17.503870: step 12290, loss = 2.26 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:53:19.604379: step 12300, loss = 2.22 (655.2 examples/sec; 0.195 sec/batch), precision = 19.50%
2017-05-18 07:53:22.671332: step 12310, loss = 2.25 (398.3 examples/sec; 0.321 sec/batch)
2017-05-18 07:53:24.545148: step 12320, loss = 2.23 (683.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:53:26.424471: step 12330, loss = 2.22 (681.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:53:28.291627: step 12340, loss = 2.20 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:53:30.162297: step 12350, loss = 2.28 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:53:32.077915: step 12360, loss = 2.23 (668.2 examples/sec; 0.192 sec/batch)
2017-05-18 07:53:34.090975: step 12370, loss = 2.27 (635.8 examples/sec; 0.201 sec/batch)
2017-05-18 07:53:36.004388: step 12380, loss = 2.24 (669.0 examples/sec; 0.191 sec/batch)
2017-05-18 07:53:37.880468: step 12390, loss = 2.23 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:53:40.035723: step 12400, loss = 2.23 (640.9 examples/sec; 0.200 sec/batch), precision = 16.40%
2017-05-18 07:53:43.038940: step 12410, loss = 2.25 (404.9 examples/sec; 0.316 sec/batch)
2017-05-18 07:53:44.919809: step 12420, loss = 2.25 (680.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:53:46.776117: step 12430, loss = 2.22 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:53:48.646730: step 12440, loss = 2.26 (684.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:53:50.522941: step 12450, loss = 2.28 (682.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:53:52.409953: step 12460, loss = 2.22 (678.3 examples/sec; 0.189 sec/batch)
2017-05-18 07:53:54.276790: step 12470, loss = 2.24 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:53:56.169966: step 12480, loss = 2.29 (676.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:53:58.037925: step 12490, loss = 2.23 (685.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:54:00.297529: step 12500, loss = 2.21 (607.4 examples/sec; 0.211 sec/batch), precision = 20.20%
2017-05-18 07:54:03.632021: step 12510, loss = 2.23 (367.1 examples/sec; 0.349 sec/batch)
2017-05-18 07:54:05.729082: step 12520, loss = 2.23 (610.4 examples/sec; 0.210 sec/batch)
2017-05-18 07:54:07.665433: step 12530, loss = 2.27 (661.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:54:09.540914: step 12540, loss = 2.23 (682.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:54:11.400362: step 12550, loss = 2.21 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:54:13.269135: step 12560, loss = 2.25 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:54:15.129834: step 12570, loss = 2.24 (687.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:54:17.000873: step 12580, loss = 2.26 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:54:18.860359: step 12590, loss = 2.29 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:54:20.956130: step 12600, loss = 2.14 (656.3 examples/sec; 0.195 sec/batch), precision = 19.30%
2017-05-18 07:54:23.923070: step 12610, loss = 2.26 (411.3 examples/sec; 0.311 sec/batch)
2017-05-18 07:54:25.821801: step 12620, loss = 2.28 (674.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:54:27.730473: step 12630, loss = 2.21 (670.6 examples/sec; 0.191 sec/batch)
2017-05-18 07:54:29.652989: step 12640, loss = 2.28 (665.8 examples/sec; 0.192 sec/batch)
2017-05-18 07:54:31.587955: step 12650, loss = 2.23 (661.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:54:33.521279: step 12660, loss = 2.16 (662.1 examples/sec; 0.193 sec/batch)
2017-05-18 07:54:35.445841: step 12670, loss = 2.24 (665.1 examples/sec; 0.192 sec/batch)
2017-05-18 07:54:37.380009: step 12680, loss = 2.24 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:54:39.303669: step 12690, loss = 2.24 (665.4 examples/sec; 0.192 sec/batch)
2017-05-18 07:54:41.447128: step 12700, loss = 2.25 (641.5 examples/sec; 0.200 sec/batch), precision = 19.50%
2017-05-18 07:54:44.439364: step 12710, loss = 2.27 (407.6 examples/sec; 0.314 sec/batch)
2017-05-18 07:54:46.314864: step 12720, loss = 2.23 (682.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:54:48.182690: step 12730, loss = 2.24 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:54:50.037910: step 12740, loss = 2.25 (689.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:54:51.888662: step 12750, loss = 2.20 (691.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:54:53.741960: step 12760, loss = 2.27 (690.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:54:55.597081: step 12770, loss = 2.24 (690.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:54:57.450113: step 12780, loss = 2.19 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:54:59.308301: step 12790, loss = 2.21 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:55:01.388697: step 12800, loss = 2.21 (660.7 examples/sec; 0.194 sec/batch), precision = 20.40%
2017-05-18 07:55:04.375546: step 12810, loss = 2.23 (408.9 examples/sec; 0.313 sec/batch)
2017-05-18 07:55:06.257376: step 12820, loss = 2.29 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:55:08.103957: step 12830, loss = 2.29 (693.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:55:09.960290: step 12840, loss = 2.25 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:55:11.809426: step 12850, loss = 2.23 (692.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:55:13.665669: step 12860, loss = 2.22 (689.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:55:15.549134: step 12870, loss = 2.20 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:55:17.435455: step 12880, loss = 2.26 (678.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:55:19.315588: step 12890, loss = 2.22 (680.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:55:21.434140: step 12900, loss = 2.18 (649.6 examples/sec; 0.197 sec/batch), precision = 19.80%
2017-05-18 07:55:24.505107: step 12910, loss = 2.19 (397.6 examples/sec; 0.322 sec/batch)
2017-05-18 07:55:26.382215: step 12920, loss = 2.23 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:55:28.234553: step 12930, loss = 2.21 (691.0 examples/sec; 0.185 sec/batch)
2017-05-18 07:55:30.112180: step 12940, loss = 2.27 (681.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:55:31.971731: step 12950, loss = 2.25 (688.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:55:33.821030: step 12960, loss = 2.26 (692.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:55:35.685400: step 12970, loss = 2.26 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:55:37.540731: step 12980, loss = 2.18 (689.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:55:39.404521: step 12990, loss = 2.24 (686.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:55:41.480566: step 13000, loss = 2.26 (662.4 examples/sec; 0.193 sec/batch), precision = 19.00%
2017-05-18 07:55:44.477541: step 13010, loss = 2.36 (407.6 examples/sec; 0.314 sec/batch)
2017-05-18 07:55:46.376685: step 13020, loss = 2.20 (674.0 examples/sec; 0.190 sec/batch)
2017-05-18 07:55:48.229241: step 13030, loss = 2.23 (690.9 examples/sec; 0.185 sec/batch)
2017-05-18 07:55:50.083247: step 13040, loss = 2.22 (690.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:55:51.949346: step 13050, loss = 2.26 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:55:53.803037: step 13060, loss = 2.21 (690.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:55:55.654863: step 13070, loss = 2.22 (691.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:55:57.505813: step 13080, loss = 2.20 (691.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:55:59.376137: step 13090, loss = 2.24 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:56:01.505416: step 13100, loss = 2.20 (645.7 examples/sec; 0.198 sec/batch), precision = 20.70%
2017-05-18 07:56:04.506213: step 13110, loss = 2.22 (406.7 examples/sec; 0.315 sec/batch)
2017-05-18 07:56:06.382702: step 13120, loss = 2.28 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:56:08.242687: step 13130, loss = 2.24 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:56:10.119076: step 13140, loss = 2.22 (682.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:56:11.984463: step 13150, loss = 2.22 (686.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:56:13.845137: step 13160, loss = 2.28 (687.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:56:15.703142: step 13170, loss = 2.24 (688.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:56:17.563791: step 13180, loss = 2.20 (687.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:56:19.433609: step 13190, loss = 2.26 (684.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:56:21.519520: step 13200, loss = 2.25 (658.9 examples/sec; 0.194 sec/batch), precision = 20.30%
2017-05-18 07:56:24.536448: step 13210, loss = 2.23 (405.0 examples/sec; 0.316 sec/batch)
2017-05-18 07:56:26.403616: step 13220, loss = 2.24 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:56:28.269325: step 13230, loss = 2.21 (686.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:56:30.132143: step 13240, loss = 2.20 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:56:32.006305: step 13250, loss = 2.31 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:56:33.886326: step 13260, loss = 2.29 (680.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:56:35.751867: step 13270, loss = 2.25 (686.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:56:37.623918: step 13280, loss = 2.25 (683.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:56:39.496134: step 13290, loss = 2.19 (683.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:56:41.594193: step 13300, loss = 2.20 (654.7 examples/sec; 0.196 sec/batch), precision = 19.20%
2017-05-18 07:56:44.605157: step 13310, loss = 2.27 (405.9 examples/sec; 0.315 sec/batch)
2017-05-18 07:56:46.491353: step 13320, loss = 2.24 (678.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:56:48.414996: step 13330, loss = 2.21 (665.4 examples/sec; 0.192 sec/batch)
2017-05-18 07:56:50.356480: step 13340, loss = 2.26 (659.3 examples/sec; 0.194 sec/batch)
2017-05-18 07:56:52.283968: step 13350, loss = 2.24 (664.1 examples/sec; 0.193 sec/batch)
2017-05-18 07:56:54.217336: step 13360, loss = 2.24 (662.1 examples/sec; 0.193 sec/batch)
2017-05-18 07:56:56.150724: step 13370, loss = 2.23 (662.1 examples/sec; 0.193 sec/batch)
2017-05-18 07:56:58.092323: step 13380, loss = 2.22 (659.3 examples/sec; 0.194 sec/batch)
2017-05-18 07:57:00.032322: step 13390, loss = 2.16 (659.8 examples/sec; 0.194 sec/batch)
2017-05-18 07:57:02.220559: step 13400, loss = 2.26 (629.1 examples/sec; 0.203 sec/batch), precision = 20.10%
2017-05-18 07:57:05.209138: step 13410, loss = 2.29 (407.4 examples/sec; 0.314 sec/batch)
2017-05-18 07:57:07.101043: step 13420, loss = 2.24 (676.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:57:08.968463: step 13430, loss = 2.23 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:57:10.821261: step 13440, loss = 2.29 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:57:12.690754: step 13450, loss = 2.25 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:57:14.559992: step 13460, loss = 2.22 (684.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:57:16.418271: step 13470, loss = 2.23 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:57:18.282135: step 13480, loss = 2.26 (686.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:57:20.149003: step 13490, loss = 2.22 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:57:22.243945: step 13500, loss = 2.19 (659.3 examples/sec; 0.194 sec/batch), precision = 20.30%
2017-05-18 07:57:25.284605: step 13510, loss = 2.26 (400.7 examples/sec; 0.319 sec/batch)
2017-05-18 07:57:27.167305: step 13520, loss = 2.24 (679.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:57:29.013632: step 13530, loss = 2.20 (693.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:57:30.866301: step 13540, loss = 2.26 (690.9 examples/sec; 0.185 sec/batch)
2017-05-18 07:57:32.705143: step 13550, loss = 2.24 (696.1 examples/sec; 0.184 sec/batch)
2017-05-18 07:57:34.555786: step 13560, loss = 2.18 (691.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:57:36.445843: step 13570, loss = 2.26 (677.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:57:38.331314: step 13580, loss = 2.21 (678.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:57:40.227089: step 13590, loss = 2.24 (675.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:57:42.357632: step 13600, loss = 2.17 (646.8 examples/sec; 0.198 sec/batch), precision = 20.40%
2017-05-18 07:57:45.369649: step 13610, loss = 2.23 (404.6 examples/sec; 0.316 sec/batch)
2017-05-18 07:57:47.242006: step 13620, loss = 2.22 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:57:49.095404: step 13630, loss = 2.22 (690.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:57:50.948019: step 13640, loss = 2.30 (690.9 examples/sec; 0.185 sec/batch)
2017-05-18 07:57:52.802657: step 13650, loss = 2.24 (690.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:57:54.653937: step 13660, loss = 2.23 (691.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:57:56.510907: step 13670, loss = 2.23 (689.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:57:58.369346: step 13680, loss = 2.27 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:58:00.224347: step 13690, loss = 2.26 (690.0 examples/sec; 0.185 sec/batch)
2017-05-18 07:58:02.313883: step 13700, loss = 2.26 (657.5 examples/sec; 0.195 sec/batch), precision = 20.40%
2017-05-18 07:58:05.277476: step 13710, loss = 2.21 (412.1 examples/sec; 0.311 sec/batch)
2017-05-18 07:58:07.155218: step 13720, loss = 2.22 (681.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:58:09.007784: step 13730, loss = 2.26 (690.9 examples/sec; 0.185 sec/batch)
2017-05-18 07:58:10.860429: step 13740, loss = 2.18 (690.9 examples/sec; 0.185 sec/batch)
2017-05-18 07:58:12.721939: step 13750, loss = 2.24 (687.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:58:14.581549: step 13760, loss = 2.27 (688.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:58:16.447895: step 13770, loss = 2.19 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:58:18.310126: step 13780, loss = 2.22 (687.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:58:20.161575: step 13790, loss = 2.28 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:58:22.254966: step 13800, loss = 2.16 (659.9 examples/sec; 0.194 sec/batch), precision = 18.70%
2017-05-18 07:58:25.244235: step 13810, loss = 2.24 (407.3 examples/sec; 0.314 sec/batch)
2017-05-18 07:58:27.155584: step 13820, loss = 2.19 (669.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:58:29.051959: step 13830, loss = 2.18 (675.0 examples/sec; 0.190 sec/batch)
2017-05-18 07:58:30.955695: step 13840, loss = 2.26 (672.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:58:32.856298: step 13850, loss = 2.24 (673.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:58:34.778306: step 13860, loss = 2.26 (666.0 examples/sec; 0.192 sec/batch)
2017-05-18 07:58:36.701011: step 13870, loss = 2.18 (665.7 examples/sec; 0.192 sec/batch)
2017-05-18 07:58:38.630319: step 13880, loss = 2.24 (663.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:58:40.556004: step 13890, loss = 2.25 (664.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:58:42.713896: step 13900, loss = 2.21 (639.3 examples/sec; 0.200 sec/batch), precision = 16.50%
2017-05-18 07:58:45.683609: step 13910, loss = 2.29 (409.5 examples/sec; 0.313 sec/batch)
2017-05-18 07:58:47.565482: step 13920, loss = 2.23 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:58:49.423504: step 13930, loss = 2.26 (688.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:58:51.282181: step 13940, loss = 2.26 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:58:53.134099: step 13950, loss = 2.22 (691.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:58:54.985055: step 13960, loss = 2.29 (691.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:58:56.838134: step 13970, loss = 2.29 (690.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:58:58.689306: step 13980, loss = 2.26 (691.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:00.544221: step 13990, loss = 2.18 (690.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:02.650712: step 14000, loss = 2.24 (652.1 examples/sec; 0.196 sec/batch), precision = 19.40%
2017-05-18 07:59:05.611854: step 14010, loss = 2.26 (412.3 examples/sec; 0.310 sec/batch)
2017-05-18 07:59:07.491574: step 14020, loss = 2.25 (681.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:59:09.334263: step 14030, loss = 2.26 (694.6 examples/sec; 0.184 sec/batch)
2017-05-18 07:59:11.185958: step 14040, loss = 2.25 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:13.039948: step 14050, loss = 2.20 (690.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:14.891952: step 14060, loss = 2.20 (691.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:16.737496: step 14070, loss = 2.22 (693.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:18.573318: step 14080, loss = 2.24 (697.2 examples/sec; 0.184 sec/batch)
2017-05-18 07:59:20.419559: step 14090, loss = 2.19 (693.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:22.528205: step 14100, loss = 2.23 (653.7 examples/sec; 0.196 sec/batch), precision = 19.20%
2017-05-18 07:59:25.581415: step 14110, loss = 2.28 (399.5 examples/sec; 0.320 sec/batch)
2017-05-18 07:59:27.454108: step 14120, loss = 2.23 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:59:29.294895: step 14130, loss = 2.20 (695.4 examples/sec; 0.184 sec/batch)
2017-05-18 07:59:31.159458: step 14140, loss = 2.26 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:59:33.011403: step 14150, loss = 2.27 (691.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:34.860717: step 14160, loss = 2.21 (692.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:36.721195: step 14170, loss = 2.29 (688.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:59:38.579005: step 14180, loss = 2.25 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:59:40.430495: step 14190, loss = 2.25 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:42.529961: step 14200, loss = 2.24 (656.4 examples/sec; 0.195 sec/batch), precision = 22.20%
2017-05-18 07:59:45.496117: step 14210, loss = 2.19 (410.8 examples/sec; 0.312 sec/batch)
2017-05-18 07:59:47.369976: step 14220, loss = 2.22 (683.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:59:49.211518: step 14230, loss = 2.19 (695.1 examples/sec; 0.184 sec/batch)
2017-05-18 07:59:51.051330: step 14240, loss = 2.23 (695.7 examples/sec; 0.184 sec/batch)
2017-05-18 07:59:52.910414: step 14250, loss = 2.19 (688.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:59:54.758467: step 14260, loss = 2.24 (692.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:56.609340: step 14270, loss = 2.20 (691.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:59:58.468222: step 14280, loss = 2.22 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 08:00:00.323158: step 14290, loss = 2.22 (690.1 examples/sec; 0.185 sec/batch)
2017-05-18 08:00:02.415098: step 14300, loss = 2.25 (659.3 examples/sec; 0.194 sec/batch), precision = 20.00%
2017-05-18 08:00:05.404360: step 14310, loss = 2.27 (407.7 examples/sec; 0.314 sec/batch)
2017-05-18 08:00:07.293909: step 14320, loss = 2.26 (677.4 examples/sec; 0.189 sec/batch)
2017-05-18 08:00:09.178982: step 14330, loss = 2.21 (679.0 examples/sec; 0.189 sec/batch)
2017-05-18 08:00:11.097584: step 14340, loss = 2.24 (667.2 examples/sec; 0.192 sec/batch)
2017-05-18 08:00:13.018713: step 14350, loss = 2.24 (666.3 examples/sec; 0.192 sec/batch)
2017-05-18 08:00:14.931282: step 14360, loss = 2.23 (669.3 examples/sec; 0.191 sec/batch)
2017-05-18 08:00:16.833144: step 14370, loss = 2.24 (673.0 examples/sec; 0.190 sec/batch)
2017-05-18 08:00:18.740033: step 14380, loss = 2.21 (671.3 examples/sec; 0.191 sec/batch)
2017-05-18 08:00:20.664735: step 14390, loss = 2.27 (665.0 examples/sec; 0.192 sec/batch)
2017-05-18 08:00:22.818977: step 14400, loss = 2.22 (638.8 examples/sec; 0.200 sec/batch), precision = 16.90%
2017-05-18 08:00:25.756025: step 14410, loss = 2.24 (414.6 examples/sec; 0.309 sec/batch)
2017-05-18 08:00:27.644122: step 14420, loss = 2.22 (677.9 examples/sec; 0.189 sec/batch)
2017-05-18 08:00:29.516834: step 14430, loss = 2.19 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 08:00:31.389603: step 14440, loss = 2.25 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 08:00:33.255148: step 14450, loss = 2.25 (686.1 examples/sec; 0.187 sec/batch)
2017-05-18 08:00:35.120052: step 14460, loss = 2.26 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 08:00:36.981476: step 14470, loss = 2.19 (687.6 examples/sec; 0.186 sec/batch)
2017-05-18 08:00:38.837920: step 14480, loss = 2.16 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 08:00:40.705120: step 14490, loss = 2.19 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 08:00:42.800119: step 14500, loss = 2.23 (655.8 examples/sec; 0.195 sec/batch), precision = 17.90%
2017-05-18 08:00:45.780607: step 14510, loss = 2.24 (409.8 examples/sec; 0.312 sec/batch)
2017-05-18 08:00:47.661048: step 14520, loss = 2.22 (680.7 examples/sec; 0.188 sec/batch)
2017-05-18 08:00:49.520720: step 14530, loss = 2.25 (688.3 examples/sec; 0.186 sec/batch)
2017-05-18 08:00:51.380650: step 14540, loss = 2.18 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:00:53.243791: step 14550, loss = 2.28 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 08:00:55.117913: step 14560, loss = 2.22 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 08:00:57.014165: step 14570, loss = 2.23 (675.0 examples/sec; 0.190 sec/batch)
2017-05-18 08:00:58.914602: step 14580, loss = 2.29 (673.5 examples/sec; 0.190 sec/batch)
2017-05-18 08:01:00.844339: step 14590, loss = 2.22 (663.3 examples/sec; 0.193 sec/batch)
2017-05-18 08:01:03.017316: step 14600, loss = 2.25 (635.1 examples/sec; 0.202 sec/batch), precision = 20.00%
2017-05-18 08:01:06.043872: step 14610, loss = 2.26 (402.0 examples/sec; 0.318 sec/batch)
2017-05-18 08:01:07.935572: step 14620, loss = 2.17 (676.6 examples/sec; 0.189 sec/batch)
2017-05-18 08:01:09.798467: step 14630, loss = 2.24 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 08:01:11.656822: step 14640, loss = 2.27 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 08:01:13.517875: step 14650, loss = 2.22 (687.8 examples/sec; 0.186 sec/batch)
2017-05-18 08:01:15.377860: step 14660, loss = 2.23 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:01:17.237130: step 14670, loss = 2.25 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 08:01:19.100205: step 14680, loss = 2.20 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 08:01:20.974645: step 14690, loss = 2.22 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 08:01:23.086350: step 14700, loss = 2.25 (650.4 examples/sec; 0.197 sec/batch), precision = 20.10%
2017-05-18 08:01:26.156090: step 14710, loss = 2.24 (398.3 examples/sec; 0.321 sec/batch)
2017-05-18 08:01:28.583493: step 14720, loss = 2.19 (527.3 examples/sec; 0.243 sec/batch)
2017-05-18 08:01:30.242221: step 14730, loss = 2.23 (771.7 examples/sec; 0.166 sec/batch)
2017-05-18 08:01:32.109805: step 14740, loss = 2.29 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 08:01:33.970330: step 14750, loss = 2.24 (688.0 examples/sec; 0.186 sec/batch)
2017-05-18 08:01:35.845638: step 14760, loss = 2.19 (682.6 examples/sec; 0.188 sec/batch)
2017-05-18 08:01:37.700597: step 14770, loss = 2.23 (690.0 examples/sec; 0.185 sec/batch)
2017-05-18 08:01:39.560140: step 14780, loss = 2.28 (688.3 examples/sec; 0.186 sec/batch)
2017-05-18 08:01:41.419982: step 14790, loss = 2.23 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:01:43.506982: step 14800, loss = 2.25 (658.3 examples/sec; 0.194 sec/batch), precision = 18.40%
2017-05-18 08:01:46.511678: step 14810, loss = 2.27 (406.7 examples/sec; 0.315 sec/batch)
2017-05-18 08:01:48.427784: step 14820, loss = 2.24 (668.0 examples/sec; 0.192 sec/batch)
2017-05-18 08:01:50.319548: step 14830, loss = 2.23 (676.6 examples/sec; 0.189 sec/batch)
2017-05-18 08:01:52.224785: step 14840, loss = 2.25 (671.8 examples/sec; 0.191 sec/batch)
2017-05-18 08:01:54.115252: step 14850, loss = 2.22 (677.1 examples/sec; 0.189 sec/batch)
2017-05-18 08:01:56.014688: step 14860, loss = 2.21 (673.9 examples/sec; 0.190 sec/batch)
2017-05-18 08:01:57.900787: step 14870, loss = 2.28 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 08:01:59.834224: step 14880, loss = 2.23 (662.0 examples/sec; 0.193 sec/batch)
2017-05-18 08:02:01.778422: step 14890, loss = 2.22 (658.4 examples/sec; 0.194 sec/batch)
2017-05-18 08:02:03.949460: step 14900, loss = 2.21 (632.5 examples/sec; 0.202 sec/batch), precision = 19.10%
2017-05-18 08:02:06.911672: step 14910, loss = 2.24 (411.6 examples/sec; 0.311 sec/batch)
2017-05-18 08:02:08.789744: step 14920, loss = 2.25 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 08:02:10.632926: step 14930, loss = 2.19 (694.4 examples/sec; 0.184 sec/batch)
2017-05-18 08:02:12.482921: step 14940, loss = 2.22 (691.9 examples/sec; 0.185 sec/batch)
2017-05-18 08:02:14.339684: step 14950, loss = 2.24 (689.4 examples/sec; 0.186 sec/batch)
2017-05-18 08:02:16.188165: step 14960, loss = 2.25 (692.5 examples/sec; 0.185 sec/batch)
2017-05-18 08:02:18.037455: step 14970, loss = 2.24 (692.2 examples/sec; 0.185 sec/batch)
2017-05-18 08:02:19.890405: step 14980, loss = 2.21 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 08:02:21.736801: step 14990, loss = 2.21 (693.2 examples/sec; 0.185 sec/batch)
2017-05-18 08:02:23.825547: step 15000, loss = 2.26 (658.4 examples/sec; 0.194 sec/batch), precision = 21.90%
2017-05-18 08:02:26.780748: step 15010, loss = 2.24 (412.9 examples/sec; 0.310 sec/batch)
2017-05-18 08:02:28.648700: step 15020, loss = 2.26 (685.2 examples/sec; 0.187 sec/batch)
2017-05-18 08:02:30.493594: step 15030, loss = 2.28 (693.8 examples/sec; 0.184 sec/batch)
2017-05-18 08:02:32.355207: step 15040, loss = 2.23 (687.6 examples/sec; 0.186 sec/batch)
2017-05-18 08:02:34.219419: step 15050, loss = 2.24 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 08:02:36.074680: step 15060, loss = 2.24 (689.9 examples/sec; 0.186 sec/batch)
2017-05-18 08:02:37.923702: step 15070, loss = 2.17 (692.3 examples/sec; 0.185 sec/batch)
2017-05-18 08:02:39.764027: step 15080, loss = 2.23 (695.5 examples/sec; 0.184 sec/batch)
2017-05-18 08:02:41.625841: step 15090, loss = 2.24 (687.5 examples/sec; 0.186 sec/batch)
2017-05-18 08:02:43.695655: step 15100, loss = 2.20 (664.6 examples/sec; 0.193 sec/batch), precision = 22.60%
2017-05-18 08:02:46.657970: step 15110, loss = 2.26 (412.1 examples/sec; 0.311 sec/batch)
2017-05-18 08:02:48.569465: step 15120, loss = 2.27 (669.6 examples/sec; 0.191 sec/batch)
2017-05-18 08:02:50.451080: step 15130, loss = 2.22 (680.3 examples/sec; 0.188 sec/batch)
2017-05-18 08:02:52.330078: step 15140, loss = 2.25 (681.2 examples/sec; 0.188 sec/batch)
2017-05-18 08:02:54.217223: step 15150, loss = 2.19 (678.3 examples/sec; 0.189 sec/batch)
2017-05-18 08:02:56.093643: step 15160, loss = 2.27 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 08:02:57.971672: step 15170, loss = 2.24 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 08:02:59.854733: step 15180, loss = 2.17 (679.7 examples/sec; 0.188 sec/batch)
2017-05-18 08:03:01.757467: step 15190, loss = 2.21 (672.7 examples/sec; 0.190 sec/batch)
2017-05-18 08:03:03.898123: step 15200, loss = 2.22 (644.4 examples/sec; 0.199 sec/batch), precision = 18.40%
2017-05-18 08:03:06.986449: step 15210, loss = 2.22 (394.8 examples/sec; 0.324 sec/batch)
2017-05-18 08:03:08.862178: step 15220, loss = 2.21 (682.4 examples/sec; 0.188 sec/batch)
2017-05-18 08:03:10.709713: step 15230, loss = 2.19 (692.8 examples/sec; 0.185 sec/batch)
2017-05-18 08:03:12.551460: step 15240, loss = 2.26 (695.0 examples/sec; 0.184 sec/batch)
2017-05-18 08:03:14.399653: step 15250, loss = 2.30 (692.6 examples/sec; 0.185 sec/batch)
2017-05-18 08:03:16.249338: step 15260, loss = 2.28 (692.0 examples/sec; 0.185 sec/batch)
2017-05-18 08:03:18.089381: step 15270, loss = 2.21 (695.6 examples/sec; 0.184 sec/batch)
2017-05-18 08:03:19.940862: step 15280, loss = 2.25 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 08:03:21.796266: step 15290, loss = 2.21 (689.9 examples/sec; 0.186 sec/batch)
2017-05-18 08:03:23.863667: step 15300, loss = 2.27 (667.4 examples/sec; 0.192 sec/batch), precision = 16.00%
2017-05-18 08:03:26.844305: step 15310, loss = 2.21 (408.9 examples/sec; 0.313 sec/batch)
2017-05-18 08:03:28.748838: step 15320, loss = 2.18 (672.1 examples/sec; 0.190 sec/batch)
2017-05-18 08:03:30.611575: step 15330, loss = 2.22 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:03:32.469575: step 15340, loss = 2.27 (688.9 examples/sec; 0.186 sec/batch)
2017-05-18 08:03:34.328867: step 15350, loss = 2.24 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 08:03:36.183777: step 15360, loss = 2.22 (690.1 examples/sec; 0.185 sec/batch)
2017-05-18 08:03:38.052375: step 15370, loss = 2.26 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 08:03:39.905223: step 15380, loss = 2.26 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 08:03:41.772109: step 15390, loss = 2.27 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 08:03:43.859924: step 15400, loss = 2.21 (659.6 examples/sec; 0.194 sec/batch), precision = 20.70%
2017-05-18 08:03:46.884381: step 15410, loss = 2.21 (403.6 examples/sec; 0.317 sec/batch)
2017-05-18 08:03:48.774556: step 15420, loss = 2.28 (677.2 examples/sec; 0.189 sec/batch)
2017-05-18 08:03:50.668127: step 15430, loss = 2.21 (676.0 examples/sec; 0.189 sec/batch)
2017-05-18 08:03:52.581414: step 15440, loss = 2.23 (669.0 examples/sec; 0.191 sec/batch)
2017-05-18 08:03:54.499665: step 15450, loss = 2.23 (667.3 examples/sec; 0.192 sec/batch)
2017-05-18 08:03:56.419303: step 15460, loss = 2.23 (666.8 examples/sec; 0.192 sec/batch)
2017-05-18 08:03:58.340381: step 15470, loss = 2.30 (666.3 examples/sec; 0.192 sec/batch)
2017-05-18 08:04:00.276027: step 15480, loss = 2.25 (661.3 examples/sec; 0.194 sec/batch)
2017-05-18 08:04:02.207309: step 15490, loss = 2.24 (662.8 examples/sec; 0.193 sec/batch)
2017-05-18 08:04:04.373566: step 15500, loss = 2.28 (636.2 examples/sec; 0.201 sec/batch), precision = 20.80%
2017-05-18 08:04:07.357109: step 15510, loss = 2.23 (407.9 examples/sec; 0.314 sec/batch)
2017-05-18 08:04:09.253768: step 15520, loss = 2.22 (674.9 examples/sec; 0.190 sec/batch)
2017-05-18 08:04:11.121080: step 15530, loss = 2.26 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 08:04:12.984287: step 15540, loss = 2.28 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 08:04:14.849314: step 15550, loss = 2.24 (686.3 examples/sec; 0.187 sec/batch)
2017-05-18 08:04:16.707446: step 15560, loss = 2.21 (688.9 examples/sec; 0.186 sec/batch)
2017-05-18 08:04:18.585688: step 15570, loss = 2.25 (681.5 examples/sec; 0.188 sec/batch)
2017-05-18 08:04:20.462451: step 15580, loss = 2.22 (682.0 examples/sec; 0.188 sec/batch)
2017-05-18 08:04:22.322462: step 15590, loss = 2.27 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:04:24.402588: step 15600, loss = 2.29 (661.7 examples/sec; 0.193 sec/batch), precision = 18.90%
2017-05-18 08:04:27.434748: step 15610, loss = 2.26 (402.8 examples/sec; 0.318 sec/batch)
2017-05-18 08:04:29.295010: step 15620, loss = 2.24 (688.1 examples/sec; 0.186 sec/batch)
2017-05-18 08:04:31.161847: step 15630, loss = 2.26 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 08:04:33.039431: step 15640, loss = 2.24 (681.7 examples/sec; 0.188 sec/batch)
2017-05-18 08:04:34.902527: step 15650, loss = 2.21 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 08:04:36.764497: step 15660, loss = 2.23 (687.4 examples/sec; 0.186 sec/batch)
2017-05-18 08:04:38.654467: step 15670, loss = 2.28 (677.3 examples/sec; 0.189 sec/batch)
2017-05-18 08:04:40.563995: step 15680, loss = 2.23 (670.3 examples/sec; 0.191 sec/batch)
2017-05-18 08:04:42.483601: step 15690, loss = 2.22 (666.8 examples/sec; 0.192 sec/batch)
2017-05-18 08:04:44.644241: step 15700, loss = 2.23 (637.8 examples/sec; 0.201 sec/batch), precision = 19.60%
2017-05-18 08:04:47.625205: step 15710, loss = 2.25 (408.3 examples/sec; 0.313 sec/batch)
2017-05-18 08:04:49.508249: step 15720, loss = 2.30 (679.8 examples/sec; 0.188 sec/batch)
2017-05-18 08:04:51.362129: step 15730, loss = 2.30 (690.4 examples/sec; 0.185 sec/batch)
2017-05-18 08:04:53.226143: step 15740, loss = 2.22 (686.7 examples/sec; 0.186 sec/batch)
2017-05-18 08:04:55.095110: step 15750, loss = 2.22 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 08:04:56.969756: step 15760, loss = 2.27 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 08:04:58.828319: step 15770, loss = 2.24 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 08:05:00.708278: step 15780, loss = 2.24 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 08:05:02.586207: step 15790, loss = 2.20 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 08:05:04.689289: step 15800, loss = 2.32 (653.0 examples/sec; 0.196 sec/batch), precision = 18.50%
2017-05-18 08:05:07.828079: step 15810, loss = 2.24 (390.1 examples/sec; 0.328 sec/batch)
2017-05-18 08:05:09.702843: step 15820, loss = 2.22 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 08:05:11.552383: step 15830, loss = 2.27 (692.1 examples/sec; 0.185 sec/batch)
2017-05-18 08:05:13.412407: step 15840, loss = 2.20 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:05:15.277206: step 15850, loss = 2.26 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 08:05:17.135794: step 15860, loss = 2.26 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 08:05:18.992114: step 15870, loss = 2.23 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 08:05:20.852602: step 15880, loss = 2.29 (688.0 examples/sec; 0.186 sec/batch)
2017-05-18 08:05:22.705409: step 15890, loss = 2.16 (690.8 examples/sec; 0.185 sec/batch)
2017-05-18 08:05:24.782772: step 15900, loss = 2.24 (662.3 examples/sec; 0.193 sec/batch), precision = 16.70%
2017-05-18 08:05:27.759266: step 15910, loss = 2.24 (410.1 examples/sec; 0.312 sec/batch)
2017-05-18 08:05:29.675910: step 15920, loss = 2.24 (667.8 examples/sec; 0.192 sec/batch)
2017-05-18 08:05:31.581598: step 15930, loss = 2.27 (671.7 examples/sec; 0.191 sec/batch)
2017-05-18 08:05:33.512485: step 15940, loss = 2.22 (662.9 examples/sec; 0.193 sec/batch)
2017-05-18 08:05:35.431216: step 15950, loss = 2.18 (667.1 examples/sec; 0.192 sec/batch)
2017-05-18 08:05:37.365110: step 15960, loss = 2.25 (661.9 examples/sec; 0.193 sec/batch)
2017-05-18 08:05:39.292279: step 15970, loss = 2.25 (664.2 examples/sec; 0.193 sec/batch)
2017-05-18 08:05:41.222451: step 15980, loss = 2.24 (663.2 examples/sec; 0.193 sec/batch)
2017-05-18 08:05:43.147814: step 15990, loss = 2.23 (664.8 examples/sec; 0.193 sec/batch)
2017-05-18 08:05:45.311752: step 16000, loss = 2.23 (636.0 examples/sec; 0.201 sec/batch), precision = 19.80%
2017-05-18 08:05:48.309713: step 16010, loss = 2.21 (406.4 examples/sec; 0.315 sec/batch)
2017-05-18 08:05:50.206454: step 16020, loss = 2.25 (674.8 examples/sec; 0.190 sec/batch)
2017-05-18 08:05:52.089764: step 16030, loss = 2.23 (679.7 examples/sec; 0.188 sec/batch)
2017-05-18 08:05:53.976255: step 16040, loss = 2.22 (678.5 examples/sec; 0.189 sec/batch)
2017-05-18 08:05:55.837344: step 16050, loss = 2.27 (687.8 examples/sec; 0.186 sec/batch)
2017-05-18 08:05:57.704422: step 16060, loss = 2.23 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 08:05:59.576916: step 16070, loss = 2.24 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 08:06:01.462261: step 16080, loss = 2.25 (678.9 examples/sec; 0.189 sec/batch)
2017-05-18 08:06:03.348416: step 16090, loss = 2.22 (678.6 examples/sec; 0.189 sec/batch)
2017-05-18 08:06:05.457139: step 16100, loss = 2.25 (652.3 examples/sec; 0.196 sec/batch), precision = 17.00%
2017-05-18 08:06:08.473901: step 16110, loss = 2.23 (404.7 examples/sec; 0.316 sec/batch)
2017-05-18 08:06:10.357940: step 16120, loss = 2.19 (679.4 examples/sec; 0.188 sec/batch)
2017-05-18 08:06:12.216736: step 16130, loss = 2.28 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 08:06:14.076616: step 16140, loss = 2.24 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:06:15.964413: step 16150, loss = 2.25 (678.0 examples/sec; 0.189 sec/batch)
2017-05-18 08:06:17.840234: step 16160, loss = 2.20 (682.4 examples/sec; 0.188 sec/batch)
2017-05-18 08:06:19.739889: step 16170, loss = 2.26 (673.8 examples/sec; 0.190 sec/batch)
2017-05-18 08:06:21.673822: step 16180, loss = 2.26 (661.9 examples/sec; 0.193 sec/batch)
2017-05-18 08:06:23.597991: step 16190, loss = 2.21 (665.2 examples/sec; 0.192 sec/batch)
2017-05-18 08:06:25.763370: step 16200, loss = 2.31 (636.0 examples/sec; 0.201 sec/batch), precision = 19.70%
2017-05-18 08:06:28.794717: step 16210, loss = 2.24 (402.0 examples/sec; 0.318 sec/batch)
2017-05-18 08:06:30.674817: step 16220, loss = 2.22 (680.8 examples/sec; 0.188 sec/batch)
2017-05-18 08:06:32.549957: step 16230, loss = 2.24 (682.6 examples/sec; 0.188 sec/batch)
2017-05-18 08:06:34.416624: step 16240, loss = 2.27 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 08:06:36.285929: step 16250, loss = 2.31 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 08:06:38.147712: step 16260, loss = 2.26 (687.5 examples/sec; 0.186 sec/batch)
2017-05-18 08:06:40.009786: step 16270, loss = 2.26 (687.4 examples/sec; 0.186 sec/batch)
2017-05-18 08:06:41.863391: step 16280, loss = 2.23 (690.5 examples/sec; 0.185 sec/batch)
2017-05-18 08:06:43.731128: step 16290, loss = 2.21 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 08:06:45.831937: step 16300, loss = 2.26 (654.8 examples/sec; 0.195 sec/batch), precision = 17.50%
2017-05-18 08:06:48.805924: step 16310, loss = 2.27 (410.3 examples/sec; 0.312 sec/batch)
2017-05-18 08:06:50.698903: step 16320, loss = 2.23 (676.2 examples/sec; 0.189 sec/batch)
2017-05-18 08:06:52.561309: step 16330, loss = 2.23 (687.3 examples/sec; 0.186 sec/batch)
2017-05-18 08:06:54.434957: step 16340, loss = 2.22 (683.2 examples/sec; 0.187 sec/batch)
2017-05-18 08:06:56.292409: step 16350, loss = 2.20 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 08:06:58.151976: step 16360, loss = 2.27 (688.3 examples/sec; 0.186 sec/batch)
2017-05-18 08:07:00.019029: step 16370, loss = 2.25 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 08:07:01.905792: step 16380, loss = 2.21 (678.4 examples/sec; 0.189 sec/batch)
2017-05-18 08:07:03.773686: step 16390, loss = 2.24 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 08:07:05.865563: step 16400, loss = 2.27 (656.8 examples/sec; 0.195 sec/batch), precision = 16.90%
2017-05-18 08:07:08.963485: step 16410, loss = 2.19 (394.9 examples/sec; 0.324 sec/batch)
2017-05-18 08:07:10.879803: step 16420, loss = 2.24 (667.9 examples/sec; 0.192 sec/batch)
2017-05-18 08:07:12.778265: step 16430, loss = 2.24 (674.2 examples/sec; 0.190 sec/batch)
2017-05-18 08:07:14.697327: step 16440, loss = 2.27 (667.0 examples/sec; 0.192 sec/batch)
2017-05-18 08:07:16.603485: step 16450, loss = 2.24 (671.5 examples/sec; 0.191 sec/batch)
2017-05-18 08:07:18.526277: step 16460, loss = 2.24 (665.7 examples/sec; 0.192 sec/batch)
2017-05-18 08:07:20.468221: step 16470, loss = 2.23 (659.1 examples/sec; 0.194 sec/batch)
2017-05-18 08:07:22.412856: step 16480, loss = 2.26 (658.2 examples/sec; 0.194 sec/batch)
2017-05-18 08:07:24.349215: step 16490, loss = 2.23 (661.0 examples/sec; 0.194 sec/batch)
2017-05-18 08:07:26.508027: step 16500, loss = 2.20 (637.1 examples/sec; 0.201 sec/batch), precision = 17.80%
2017-05-18 08:07:29.506316: step 16510, loss = 2.25 (406.6 examples/sec; 0.315 sec/batch)
2017-05-18 08:07:31.370071: step 16520, loss = 2.17 (686.8 examples/sec; 0.186 sec/batch)
2017-05-18 08:07:33.234855: step 16530, loss = 2.25 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 08:07:35.091310: step 16540, loss = 2.28 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 08:07:36.968826: step 16550, loss = 2.21 (681.7 examples/sec; 0.188 sec/batch)
2017-05-18 08:07:38.828764: step 16560, loss = 2.23 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:07:40.688811: step 16570, loss = 2.26 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:07:42.557368: step 16580, loss = 2.18 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 08:07:44.427739: step 16590, loss = 2.25 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 08:07:46.527876: step 16600, loss = 2.21 (654.7 examples/sec; 0.196 sec/batch), precision = 17.40%
2017-05-18 08:07:49.541147: step 16610, loss = 2.25 (405.3 examples/sec; 0.316 sec/batch)
2017-05-18 08:07:51.408442: step 16620, loss = 2.19 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 08:07:53.263275: step 16630, loss = 2.22 (690.1 examples/sec; 0.185 sec/batch)
2017-05-18 08:07:55.120568: step 16640, loss = 2.23 (689.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:07:56.987433: step 16650, loss = 2.23 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 08:07:58.857295: step 16660, loss = 2.26 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:00.727054: step 16670, loss = 2.29 (684.6 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:02.592349: step 16680, loss = 2.27 (686.2 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:04.466855: step 16690, loss = 2.22 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:06.590062: step 16700, loss = 2.25 (647.8 examples/sec; 0.198 sec/batch), precision = 16.80%
2017-05-18 08:08:09.600217: step 16710, loss = 2.26 (405.4 examples/sec; 0.316 sec/batch)
2017-05-18 08:08:11.495091: step 16720, loss = 2.21 (675.5 examples/sec; 0.189 sec/batch)
2017-05-18 08:08:13.370889: step 16730, loss = 2.24 (682.4 examples/sec; 0.188 sec/batch)
2017-05-18 08:08:15.245449: step 16740, loss = 2.25 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:17.115651: step 16750, loss = 2.21 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:18.983440: step 16760, loss = 2.19 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:20.852819: step 16770, loss = 2.21 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:22.732069: step 16780, loss = 2.23 (681.1 examples/sec; 0.188 sec/batch)
2017-05-18 08:08:24.609691: step 16790, loss = 2.23 (681.7 examples/sec; 0.188 sec/batch)
2017-05-18 08:08:26.714146: step 16800, loss = 2.25 (652.1 examples/sec; 0.196 sec/batch), precision = 21.20%
2017-05-18 08:08:29.722954: step 16810, loss = 2.25 (406.3 examples/sec; 0.315 sec/batch)
2017-05-18 08:08:31.604668: step 16820, loss = 2.29 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 08:08:33.473638: step 16830, loss = 2.22 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:35.345801: step 16840, loss = 2.24 (683.7 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:37.209748: step 16850, loss = 2.23 (686.7 examples/sec; 0.186 sec/batch)
2017-05-18 08:08:39.080702: step 16860, loss = 2.20 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:40.948517: step 16870, loss = 2.24 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:42.821746: step 16880, loss = 2.19 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:44.687759: step 16890, loss = 2.23 (686.0 examples/sec; 0.187 sec/batch)
2017-05-18 08:08:46.771682: step 16900, loss = 2.30 (659.3 examples/sec; 0.194 sec/batch), precision = 19.20%
2017-05-18 08:08:49.727718: step 16910, loss = 2.25 (413.1 examples/sec; 0.310 sec/batch)
2017-05-18 08:08:51.631106: step 16920, loss = 2.26 (672.5 examples/sec; 0.190 sec/batch)
2017-05-18 08:08:53.541369: step 16930, loss = 2.27 (670.1 examples/sec; 0.191 sec/batch)
2017-05-18 08:08:55.460553: step 16940, loss = 2.26 (667.0 examples/sec; 0.192 sec/batch)
2017-05-18 08:08:57.388642: step 16950, loss = 2.22 (663.9 examples/sec; 0.193 sec/batch)
2017-05-18 08:08:59.321340: step 16960, loss = 2.27 (662.3 examples/sec; 0.193 sec/batch)
2017-05-18 08:09:01.247699: step 16970, loss = 2.21 (664.5 examples/sec; 0.193 sec/batch)
2017-05-18 08:09:03.176178: step 16980, loss = 2.28 (663.7 examples/sec; 0.193 sec/batch)
2017-05-18 08:09:05.107170: step 16990, loss = 2.26 (662.9 examples/sec; 0.193 sec/batch)
2017-05-18 08:09:07.269098: step 17000, loss = 2.21 (635.2 examples/sec; 0.202 sec/batch), precision = 19.70%
2017-05-18 08:09:10.384983: step 17010, loss = 2.20 (392.3 examples/sec; 0.326 sec/batch)
2017-05-18 08:09:12.275630: step 17020, loss = 2.24 (677.0 examples/sec; 0.189 sec/batch)
2017-05-18 08:09:14.133139: step 17030, loss = 2.26 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 08:09:15.994017: step 17040, loss = 2.23 (687.8 examples/sec; 0.186 sec/batch)
2017-05-18 08:09:17.866040: step 17050, loss = 2.22 (683.8 examples/sec; 0.187 sec/batch)
2017-05-18 08:09:19.735016: step 17060, loss = 2.23 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 08:09:21.593761: step 17070, loss = 2.20 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 08:09:23.459725: step 17080, loss = 2.19 (686.0 examples/sec; 0.187 sec/batch)
2017-05-18 08:09:25.326051: step 17090, loss = 2.23 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 08:09:27.422936: step 17100, loss = 2.17 (654.3 examples/sec; 0.196 sec/batch), precision = 20.30%
2017-05-18 08:09:30.431241: step 17110, loss = 2.30 (406.5 examples/sec; 0.315 sec/batch)
2017-05-18 08:09:32.311154: step 17120, loss = 2.24 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 08:09:34.170299: step 17130, loss = 2.23 (688.5 examples/sec; 0.186 sec/batch)
2017-05-18 08:09:36.038783: step 17140, loss = 2.27 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 08:09:37.901985: step 17150, loss = 2.23 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 08:09:39.776613: step 17160, loss = 2.25 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 08:09:41.674265: step 17170, loss = 2.20 (674.5 examples/sec; 0.190 sec/batch)
2017-05-18 08:09:43.582616: step 17180, loss = 2.26 (670.7 examples/sec; 0.191 sec/batch)
2017-05-18 08:09:45.500342: step 17190, loss = 2.24 (667.5 examples/sec; 0.192 sec/batch)
2017-05-18 08:09:47.653486: step 17200, loss = 2.21 (640.1 examples/sec; 0.200 sec/batch), precision = 18.20%
2017-05-18 08:09:50.680522: step 17210, loss = 2.19 (402.4 examples/sec; 0.318 sec/batch)
2017-05-18 08:09:52.551307: step 17220, loss = 2.28 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 08:09:54.416231: step 17230, loss = 2.26 (686.3 examples/sec; 0.186 sec/batch)
2017-05-18 08:09:56.266318: step 17240, loss = 2.22 (691.9 examples/sec; 0.185 sec/batch)
2017-05-18 08:09:58.132263: step 17250, loss = 2.22 (686.0 examples/sec; 0.187 sec/batch)
2017-05-18 08:09:59.987188: step 17260, loss = 2.22 (690.1 examples/sec; 0.185 sec/batch)
2017-05-18 08:10:01.858778: step 17270, loss = 2.29 (683.9 examples/sec; 0.187 sec/batch)
2017-05-18 08:10:03.737709: step 17280, loss = 2.22 (681.2 examples/sec; 0.188 sec/batch)
2017-05-18 08:10:05.587961: step 17290, loss = 2.23 (691.8 examples/sec; 0.185 sec/batch)
2017-05-18 08:10:07.680950: step 17300, loss = 2.24 (658.6 examples/sec; 0.194 sec/batch), precision = 20.40%
2017-05-18 08:10:10.663207: step 17310, loss = 2.27 (408.7 examples/sec; 0.313 sec/batch)
2017-05-18 08:10:12.557415: step 17320, loss = 2.29 (675.7 examples/sec; 0.189 sec/batch)
2017-05-18 08:10:14.413275: step 17330, loss = 2.24 (689.7 examples/sec; 0.186 sec/batch)
2017-05-18 08:10:16.269649: step 17340, loss = 2.20 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 08:10:18.130963: step 17350, loss = 2.20 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 08:10:19.999119: step 17360, loss = 2.24 (685.2 examples/sec; 0.187 sec/batch)
2017-05-18 08:10:21.855185: step 17370, loss = 2.29 (689.6 examples/sec; 0.186 sec/batch)
2017-05-18 08:10:23.704185: step 17380, loss = 2.26 (692.3 examples/sec; 0.185 sec/batch)
2017-05-18 08:10:25.566887: step 17390, loss = 2.22 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 08:10:27.645011: step 17400, loss = 2.23 (663.1 examples/sec; 0.193 sec/batch), precision = 20.60%
2017-05-18 08:10:30.617424: step 17410, loss = 2.27 (410.2 examples/sec; 0.312 sec/batch)
2017-05-18 08:10:32.529315: step 17420, loss = 2.20 (669.5 examples/sec; 0.191 sec/batch)
2017-05-18 08:10:34.430967: step 17430, loss = 2.26 (673.1 examples/sec; 0.190 sec/batch)
2017-05-18 08:10:36.359050: step 17440, loss = 2.19 (663.9 examples/sec; 0.193 sec/batch)
2017-05-18 08:10:38.290914: step 17450, loss = 2.28 (662.6