loading raw file: ../data/train_32x32.mat
filling input queue
loading raw file: ../data/test_32x32.mat
random validation set [22910:23910]
filling input queue
loading batch of samples: 128
loading batch of samples: 1000
TEST after_create_session
2017-05-18 07:11:28.552031: step 0, loss = 6.38 (55.7 examples/sec; 2.297 sec/batch), precision = 16.10%
2017-05-18 07:11:31.251403: step 10, loss = 6.29 (396.0 examples/sec; 0.323 sec/batch)
2017-05-18 07:11:33.405454: step 20, loss = 6.24 (594.2 examples/sec; 0.215 sec/batch)
2017-05-18 07:11:35.274223: step 30, loss = 6.19 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:11:37.143056: step 40, loss = 6.20 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:11:39.016555: step 50, loss = 6.16 (683.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:11:40.908675: step 60, loss = 6.13 (676.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:11:42.775437: step 70, loss = 6.12 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:11:44.637282: step 80, loss = 6.04 (687.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:11:46.515244: step 90, loss = 6.03 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:11:48.606358: step 100, loss = 6.01 (657.5 examples/sec; 0.195 sec/batch), precision = 19.90%
2017-05-18 07:11:51.607641: step 110, loss = 5.94 (406.9 examples/sec; 0.315 sec/batch)
2017-05-18 07:11:53.529270: step 120, loss = 5.97 (666.1 examples/sec; 0.192 sec/batch)
2017-05-18 07:11:55.412344: step 130, loss = 5.91 (679.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:11:57.284444: step 140, loss = 5.93 (683.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:11:59.174939: step 150, loss = 5.90 (677.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:12:01.065749: step 160, loss = 5.85 (677.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:12:02.948815: step 170, loss = 5.77 (679.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:04.852645: step 180, loss = 5.76 (672.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:12:06.771561: step 190, loss = 5.73 (667.0 examples/sec; 0.192 sec/batch)
2017-05-18 07:12:08.941517: step 200, loss = 5.77 (634.5 examples/sec; 0.202 sec/batch), precision = 19.60%
2017-05-18 07:12:11.987116: step 210, loss = 5.74 (400.2 examples/sec; 0.320 sec/batch)
2017-05-18 07:12:13.876861: step 220, loss = 5.64 (677.3 examples/sec; 0.189 sec/batch)
2017-05-18 07:12:15.742226: step 230, loss = 5.63 (686.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:12:17.599652: step 240, loss = 5.59 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:12:19.470462: step 250, loss = 5.52 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:12:21.335354: step 260, loss = 5.57 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:12:23.203767: step 270, loss = 5.47 (685.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:12:25.083698: step 280, loss = 5.55 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:26.937263: step 290, loss = 5.53 (690.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:12:29.028084: step 300, loss = 5.40 (657.6 examples/sec; 0.195 sec/batch), precision = 20.40%
2017-05-18 07:12:32.021604: step 310, loss = 5.41 (407.9 examples/sec; 0.314 sec/batch)
2017-05-18 07:12:33.903874: step 320, loss = 5.40 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:35.774986: step 330, loss = 5.34 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:12:37.649700: step 340, loss = 5.33 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:12:39.532017: step 350, loss = 5.30 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:41.390953: step 360, loss = 5.29 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:12:43.248414: step 370, loss = 5.33 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:12:45.130311: step 380, loss = 5.25 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:47.012743: step 390, loss = 5.21 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:12:49.114886: step 400, loss = 5.22 (653.3 examples/sec; 0.196 sec/batch), precision = 18.30%
2017-05-18 07:12:52.120095: step 410, loss = 5.16 (406.6 examples/sec; 0.315 sec/batch)
2017-05-18 07:12:54.030146: step 420, loss = 5.18 (670.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:12:55.936986: step 430, loss = 5.15 (671.3 examples/sec; 0.191 sec/batch)
2017-05-18 07:12:57.840906: step 440, loss = 5.08 (672.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:12:59.728476: step 450, loss = 5.09 (678.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:13:01.634075: step 460, loss = 5.11 (671.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:13:03.569984: step 470, loss = 5.02 (661.2 examples/sec; 0.194 sec/batch)
2017-05-18 07:13:05.504948: step 480, loss = 4.96 (661.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:13:07.427059: step 490, loss = 5.04 (665.9 examples/sec; 0.192 sec/batch)
2017-05-18 07:13:09.594298: step 500, loss = 5.02 (636.4 examples/sec; 0.201 sec/batch), precision = 18.70%
2017-05-18 07:13:12.559404: step 510, loss = 4.96 (410.1 examples/sec; 0.312 sec/batch)
2017-05-18 07:13:14.419207: step 520, loss = 4.95 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:16.285817: step 530, loss = 4.88 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:13:18.137549: step 540, loss = 4.90 (691.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:13:19.994694: step 550, loss = 4.85 (689.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:21.859604: step 560, loss = 4.84 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:23.717306: step 570, loss = 4.83 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:25.572858: step 580, loss = 4.82 (689.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:27.421457: step 590, loss = 4.84 (692.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:13:29.504079: step 600, loss = 4.80 (659.9 examples/sec; 0.194 sec/batch), precision = 19.50%
2017-05-18 07:13:32.578720: step 610, loss = 4.75 (397.8 examples/sec; 0.322 sec/batch)
2017-05-18 07:13:34.452707: step 620, loss = 4.74 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:13:36.310136: step 630, loss = 4.68 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:38.187119: step 640, loss = 4.67 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:13:40.050072: step 650, loss = 4.68 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:41.906433: step 660, loss = 4.67 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:43.761144: step 670, loss = 4.59 (690.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:13:45.625407: step 680, loss = 4.58 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:13:47.497786: step 690, loss = 4.59 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:13:49.627372: step 700, loss = 4.59 (646.1 examples/sec; 0.198 sec/batch), precision = 18.00%
2017-05-18 07:13:52.596716: step 710, loss = 4.57 (410.5 examples/sec; 0.312 sec/batch)
2017-05-18 07:13:54.480464: step 720, loss = 4.56 (679.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:13:56.326045: step 730, loss = 4.50 (693.5 examples/sec; 0.185 sec/batch)
2017-05-18 07:13:58.181127: step 740, loss = 4.48 (690.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:00.039327: step 750, loss = 4.50 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:01.893737: step 760, loss = 4.47 (690.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:14:03.740612: step 770, loss = 4.38 (693.1 examples/sec; 0.185 sec/batch)
2017-05-18 07:14:05.600026: step 780, loss = 4.40 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:07.451626: step 790, loss = 4.46 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:14:09.523236: step 800, loss = 4.36 (662.9 examples/sec; 0.193 sec/batch), precision = 18.10%
2017-05-18 07:14:12.501350: step 810, loss = 4.35 (410.4 examples/sec; 0.312 sec/batch)
2017-05-18 07:14:14.388946: step 820, loss = 4.37 (678.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:14:16.250480: step 830, loss = 4.34 (687.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:18.114816: step 840, loss = 4.39 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:19.980010: step 850, loss = 4.32 (686.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:14:21.843989: step 860, loss = 4.29 (686.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:23.703987: step 870, loss = 4.27 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:25.568387: step 880, loss = 4.30 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:27.429292: step 890, loss = 4.23 (687.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:14:29.517667: step 900, loss = 4.22 (657.3 examples/sec; 0.195 sec/batch), precision = 19.30%
2017-05-18 07:14:32.546055: step 910, loss = 4.16 (403.9 examples/sec; 0.317 sec/batch)
2017-05-18 07:14:34.452283: step 920, loss = 4.27 (671.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:14:36.356225: step 930, loss = 4.17 (672.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:14:38.268712: step 940, loss = 4.15 (669.3 examples/sec; 0.191 sec/batch)
2017-05-18 07:14:40.189328: step 950, loss = 4.16 (666.5 examples/sec; 0.192 sec/batch)
2017-05-18 07:14:42.111178: step 960, loss = 4.07 (666.0 examples/sec; 0.192 sec/batch)
2017-05-18 07:14:44.043053: step 970, loss = 4.13 (662.6 examples/sec; 0.193 sec/batch)
2017-05-18 07:14:45.970431: step 980, loss = 4.15 (664.1 examples/sec; 0.193 sec/batch)
2017-05-18 07:14:47.910696: step 990, loss = 4.06 (659.7 examples/sec; 0.194 sec/batch)
2017-05-18 07:14:50.098071: step 1000, loss = 4.09 (629.1 examples/sec; 0.203 sec/batch), precision = 16.10%
2017-05-18 07:14:53.114262: step 1010, loss = 3.96 (403.9 examples/sec; 0.317 sec/batch)
2017-05-18 07:14:54.990376: step 1020, loss = 4.03 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:14:56.869468: step 1030, loss = 4.05 (681.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:14:58.742667: step 1040, loss = 4.02 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:00.638714: step 1050, loss = 4.02 (675.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:15:02.518558: step 1060, loss = 3.96 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:04.389799: step 1070, loss = 3.93 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:06.265851: step 1080, loss = 3.97 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:08.130240: step 1090, loss = 3.88 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:15:10.230284: step 1100, loss = 3.94 (654.1 examples/sec; 0.196 sec/batch), precision = 20.00%
2017-05-18 07:15:13.187833: step 1110, loss = 3.85 (412.8 examples/sec; 0.310 sec/batch)
2017-05-18 07:15:15.086441: step 1120, loss = 3.92 (674.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:15:16.955786: step 1130, loss = 3.91 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:18.829895: step 1140, loss = 3.89 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:20.710271: step 1150, loss = 3.87 (680.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:22.595344: step 1160, loss = 3.86 (679.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:15:24.462054: step 1170, loss = 3.88 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:26.355029: step 1180, loss = 3.82 (676.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:15:28.257345: step 1190, loss = 3.80 (672.9 examples/sec; 0.190 sec/batch)
2017-05-18 07:15:30.424039: step 1200, loss = 3.81 (635.5 examples/sec; 0.201 sec/batch), precision = 17.10%
2017-05-18 07:15:33.532170: step 1210, loss = 3.82 (392.5 examples/sec; 0.326 sec/batch)
2017-05-18 07:15:35.432610: step 1220, loss = 3.79 (673.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:15:37.299164: step 1230, loss = 3.77 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:39.182722: step 1240, loss = 3.68 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:41.063604: step 1250, loss = 3.75 (680.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:42.950177: step 1260, loss = 3.76 (678.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:15:44.812367: step 1270, loss = 3.70 (687.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:15:46.692350: step 1280, loss = 3.71 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:48.560102: step 1290, loss = 3.69 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:15:50.651460: step 1300, loss = 3.69 (657.7 examples/sec; 0.195 sec/batch), precision = 20.40%
2017-05-18 07:15:53.635880: step 1310, loss = 3.70 (409.0 examples/sec; 0.313 sec/batch)
2017-05-18 07:15:55.544293: step 1320, loss = 3.69 (670.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:15:57.422542: step 1330, loss = 3.65 (681.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:15:59.296959: step 1340, loss = 3.59 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:16:01.182999: step 1350, loss = 3.60 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:16:03.056221: step 1360, loss = 3.65 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:16:04.937669: step 1370, loss = 3.63 (680.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:16:06.799164: step 1380, loss = 3.59 (687.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:16:08.666622: step 1390, loss = 3.59 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:16:10.781953: step 1400, loss = 3.52 (650.4 examples/sec; 0.197 sec/batch), precision = 17.10%
2017-05-18 07:16:13.779417: step 1410, loss = 3.54 (407.0 examples/sec; 0.314 sec/batch)
2017-05-18 07:16:15.685821: step 1420, loss = 3.54 (671.4 examples/sec; 0.191 sec/batch)
2017-05-18 07:16:17.588161: step 1430, loss = 3.56 (672.9 examples/sec; 0.190 sec/batch)
2017-05-18 07:16:19.519839: step 1440, loss = 3.54 (662.6 examples/sec; 0.193 sec/batch)
2017-05-18 07:16:21.474238: step 1450, loss = 3.48 (654.9 examples/sec; 0.195 sec/batch)
2017-05-18 07:16:23.399794: step 1460, loss = 3.50 (664.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:16:25.336162: step 1470, loss = 3.49 (661.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:16:27.281347: step 1480, loss = 3.52 (658.0 examples/sec; 0.195 sec/batch)
2017-05-18 07:16:29.249534: step 1490, loss = 3.49 (650.3 examples/sec; 0.197 sec/batch)
2017-05-18 07:16:31.437949: step 1500, loss = 3.45 (628.9 examples/sec; 0.204 sec/batch), precision = 19.80%
2017-05-18 07:16:34.485059: step 1510, loss = 3.44 (400.0 examples/sec; 0.320 sec/batch)
2017-05-18 07:16:36.388561: step 1520, loss = 3.47 (672.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:16:38.276924: step 1530, loss = 3.39 (677.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:16:40.167092: step 1540, loss = 3.44 (677.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:16:42.057999: step 1550, loss = 3.36 (676.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:16:43.934704: step 1560, loss = 3.45 (682.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:16:45.810498: step 1570, loss = 3.41 (682.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:16:47.692958: step 1580, loss = 3.40 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:16:49.560472: step 1590, loss = 3.36 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:16:51.654834: step 1600, loss = 3.37 (656.0 examples/sec; 0.195 sec/batch), precision = 18.60%
2017-05-18 07:16:54.681289: step 1610, loss = 3.36 (403.8 examples/sec; 0.317 sec/batch)
2017-05-18 07:16:56.551446: step 1620, loss = 3.37 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:16:58.418158: step 1630, loss = 3.36 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:00.292513: step 1640, loss = 3.31 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:02.167964: step 1650, loss = 3.35 (682.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:17:04.036608: step 1660, loss = 3.35 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:05.945120: step 1670, loss = 3.27 (670.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:17:07.873189: step 1680, loss = 3.28 (663.9 examples/sec; 0.193 sec/batch)
2017-05-18 07:17:09.799865: step 1690, loss = 3.30 (664.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:17:11.971729: step 1700, loss = 3.21 (632.9 examples/sec; 0.202 sec/batch), precision = 19.80%
2017-05-18 07:17:14.987658: step 1710, loss = 3.31 (404.4 examples/sec; 0.317 sec/batch)
2017-05-18 07:17:16.870610: step 1720, loss = 3.23 (679.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:17:18.740228: step 1730, loss = 3.30 (684.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:20.609967: step 1740, loss = 3.24 (684.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:22.475239: step 1750, loss = 3.20 (686.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:24.346946: step 1760, loss = 3.22 (683.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:26.214251: step 1770, loss = 3.24 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:28.088136: step 1780, loss = 3.25 (683.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:29.994310: step 1790, loss = 3.30 (671.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:17:32.092068: step 1800, loss = 3.22 (654.7 examples/sec; 0.195 sec/batch), precision = 19.60%
2017-05-18 07:17:35.171800: step 1810, loss = 3.19 (397.2 examples/sec; 0.322 sec/batch)
2017-05-18 07:17:37.078062: step 1820, loss = 3.15 (671.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:17:38.936532: step 1830, loss = 3.22 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:17:40.818960: step 1840, loss = 3.17 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:17:42.702131: step 1850, loss = 3.18 (679.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:17:44.572312: step 1860, loss = 3.14 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:17:46.455101: step 1870, loss = 3.15 (679.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:17:48.318213: step 1880, loss = 3.14 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:17:50.207305: step 1890, loss = 3.14 (677.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:17:52.302464: step 1900, loss = 3.16 (655.6 examples/sec; 0.195 sec/batch), precision = 17.00%
2017-05-18 07:17:55.330979: step 1910, loss = 3.12 (403.6 examples/sec; 0.317 sec/batch)
2017-05-18 07:17:57.261104: step 1920, loss = 3.12 (663.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:17:59.170581: step 1930, loss = 3.14 (670.3 examples/sec; 0.191 sec/batch)
2017-05-18 07:18:01.112474: step 1940, loss = 3.13 (659.2 examples/sec; 0.194 sec/batch)
2017-05-18 07:18:03.049230: step 1950, loss = 3.14 (660.9 examples/sec; 0.194 sec/batch)
2017-05-18 07:18:04.994603: step 1960, loss = 3.07 (658.0 examples/sec; 0.195 sec/batch)
2017-05-18 07:18:06.929627: step 1970, loss = 3.06 (661.5 examples/sec; 0.194 sec/batch)
2017-05-18 07:18:08.874501: step 1980, loss = 3.04 (658.1 examples/sec; 0.194 sec/batch)
2017-05-18 07:18:10.810645: step 1990, loss = 3.10 (661.1 examples/sec; 0.194 sec/batch)
2017-05-18 07:18:13.001123: step 2000, loss = 3.03 (629.2 examples/sec; 0.203 sec/batch), precision = 18.60%
2017-05-18 07:18:16.047761: step 2010, loss = 3.02 (399.7 examples/sec; 0.320 sec/batch)
2017-05-18 07:18:17.980009: step 2020, loss = 3.08 (662.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:18:19.866214: step 2030, loss = 3.03 (678.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:18:21.747868: step 2040, loss = 3.09 (680.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:18:23.622711: step 2050, loss = 3.05 (682.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:18:25.522973: step 2060, loss = 3.07 (673.6 examples/sec; 0.190 sec/batch)
2017-05-18 07:18:27.408823: step 2070, loss = 3.01 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:18:29.289646: step 2080, loss = 3.03 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:18:31.171147: step 2090, loss = 2.98 (680.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:18:33.263025: step 2100, loss = 3.03 (657.1 examples/sec; 0.195 sec/batch), precision = 17.90%
2017-05-18 07:18:36.245807: step 2110, loss = 2.99 (409.4 examples/sec; 0.313 sec/batch)
2017-05-18 07:18:38.123584: step 2120, loss = 2.94 (681.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:18:39.975400: step 2130, loss = 2.92 (691.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:18:41.834156: step 2140, loss = 2.98 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:18:43.693639: step 2150, loss = 2.95 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:18:45.559343: step 2160, loss = 2.93 (686.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:18:47.452933: step 2170, loss = 2.96 (676.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:18:49.369937: step 2180, loss = 2.92 (667.7 examples/sec; 0.192 sec/batch)
2017-05-18 07:18:51.295251: step 2190, loss = 2.93 (664.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:18:53.456835: step 2200, loss = 2.90 (636.5 examples/sec; 0.201 sec/batch), precision = 19.20%
2017-05-18 07:18:56.465412: step 2210, loss = 2.91 (405.2 examples/sec; 0.316 sec/batch)
2017-05-18 07:18:58.350533: step 2220, loss = 2.87 (679.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:19:00.218685: step 2230, loss = 2.98 (685.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:19:02.083627: step 2240, loss = 2.92 (686.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:19:03.956997: step 2250, loss = 2.91 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:19:05.816323: step 2260, loss = 2.93 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:19:07.678940: step 2270, loss = 2.92 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:19:09.539655: step 2280, loss = 2.92 (687.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:19:11.409147: step 2290, loss = 2.86 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:19:13.497205: step 2300, loss = 2.84 (657.9 examples/sec; 0.195 sec/batch), precision = 18.40%
2017-05-18 07:19:16.520411: step 2310, loss = 2.90 (404.3 examples/sec; 0.317 sec/batch)
2017-05-18 07:19:18.406844: step 2320, loss = 2.85 (678.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:19:20.285854: step 2330, loss = 2.82 (681.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:19:22.168153: step 2340, loss = 2.94 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:19:24.039367: step 2350, loss = 2.87 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:19:25.915252: step 2360, loss = 2.84 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:19:27.778313: step 2370, loss = 2.82 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:19:29.646645: step 2380, loss = 2.82 (685.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:19:31.525003: step 2390, loss = 2.84 (681.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:19:33.629279: step 2400, loss = 2.86 (653.8 examples/sec; 0.196 sec/batch), precision = 19.50%
2017-05-18 07:19:36.735654: step 2410, loss = 2.82 (393.5 examples/sec; 0.325 sec/batch)
2017-05-18 07:19:38.645890: step 2420, loss = 2.83 (670.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:19:40.553119: step 2430, loss = 2.84 (671.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:19:42.485388: step 2440, loss = 2.81 (662.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:19:44.426645: step 2450, loss = 2.82 (659.4 examples/sec; 0.194 sec/batch)
2017-05-18 07:19:46.362362: step 2460, loss = 2.78 (661.3 examples/sec; 0.194 sec/batch)
2017-05-18 07:19:48.290985: step 2470, loss = 2.81 (663.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:19:50.227270: step 2480, loss = 2.82 (661.1 examples/sec; 0.194 sec/batch)
2017-05-18 07:19:52.169530: step 2490, loss = 2.79 (659.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:19:54.382455: step 2500, loss = 2.79 (621.2 examples/sec; 0.206 sec/batch), precision = 19.70%
2017-05-18 07:19:57.396909: step 2510, loss = 2.83 (404.2 examples/sec; 0.317 sec/batch)
2017-05-18 07:19:59.290826: step 2520, loss = 2.74 (675.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:20:01.199822: step 2530, loss = 2.77 (670.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:20:03.083194: step 2540, loss = 2.77 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:04.960303: step 2550, loss = 2.78 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:06.858295: step 2560, loss = 2.76 (674.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:20:08.746483: step 2570, loss = 2.74 (677.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:20:10.632177: step 2580, loss = 2.77 (678.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:20:12.518310: step 2590, loss = 2.76 (678.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:20:14.624982: step 2600, loss = 2.72 (652.1 examples/sec; 0.196 sec/batch), precision = 19.70%
2017-05-18 07:20:17.631416: step 2610, loss = 2.75 (406.3 examples/sec; 0.315 sec/batch)
2017-05-18 07:20:19.529682: step 2620, loss = 2.79 (674.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:20:21.400474: step 2630, loss = 2.72 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:20:23.266883: step 2640, loss = 2.80 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:20:25.141921: step 2650, loss = 2.73 (682.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:27.014306: step 2660, loss = 2.74 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:20:28.898655: step 2670, loss = 2.68 (679.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:30.822609: step 2680, loss = 2.71 (665.3 examples/sec; 0.192 sec/batch)
2017-05-18 07:20:32.756638: step 2690, loss = 2.75 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:20:34.912287: step 2700, loss = 2.69 (640.0 examples/sec; 0.200 sec/batch), precision = 18.60%
2017-05-18 07:20:37.908145: step 2710, loss = 2.73 (406.1 examples/sec; 0.315 sec/batch)
2017-05-18 07:20:39.784711: step 2720, loss = 2.72 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:41.642345: step 2730, loss = 2.68 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:20:43.512040: step 2740, loss = 2.70 (684.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:20:45.388467: step 2750, loss = 2.76 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:20:47.261260: step 2760, loss = 2.64 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:20:49.113736: step 2770, loss = 2.67 (691.0 examples/sec; 0.185 sec/batch)
2017-05-18 07:20:50.973152: step 2780, loss = 2.66 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:20:52.835847: step 2790, loss = 2.67 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:20:54.929986: step 2800, loss = 2.68 (655.8 examples/sec; 0.195 sec/batch), precision = 19.00%
2017-05-18 07:20:57.939937: step 2810, loss = 2.68 (406.1 examples/sec; 0.315 sec/batch)
2017-05-18 07:20:59.851099: step 2820, loss = 2.62 (669.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:21:01.724782: step 2830, loss = 2.65 (683.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:03.608278: step 2840, loss = 2.72 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:05.487122: step 2850, loss = 2.66 (681.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:07.375078: step 2860, loss = 2.66 (678.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:21:09.245701: step 2870, loss = 2.60 (684.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:11.122041: step 2880, loss = 2.61 (682.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:13.011041: step 2890, loss = 2.68 (677.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:21:15.115709: step 2900, loss = 2.59 (651.9 examples/sec; 0.196 sec/batch), precision = 17.10%
2017-05-18 07:21:18.084675: step 2910, loss = 2.66 (411.5 examples/sec; 0.311 sec/batch)
2017-05-18 07:21:20.008710: step 2920, loss = 2.62 (665.3 examples/sec; 0.192 sec/batch)
2017-05-18 07:21:21.913313: step 2930, loss = 2.59 (672.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:21:23.847332: step 2940, loss = 2.55 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:21:25.770461: step 2950, loss = 2.64 (665.6 examples/sec; 0.192 sec/batch)
2017-05-18 07:21:28.322176: step 2960, loss = 2.63 (501.6 examples/sec; 0.255 sec/batch)
2017-05-18 07:21:29.994231: step 2970, loss = 2.65 (765.5 examples/sec; 0.167 sec/batch)
2017-05-18 07:21:31.870878: step 2980, loss = 2.62 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:33.739807: step 2990, loss = 2.64 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:35.833214: step 3000, loss = 2.61 (656.2 examples/sec; 0.195 sec/batch), precision = 19.10%
2017-05-18 07:21:38.860578: step 3010, loss = 2.59 (403.7 examples/sec; 0.317 sec/batch)
2017-05-18 07:21:40.768552: step 3020, loss = 2.60 (670.9 examples/sec; 0.191 sec/batch)
2017-05-18 07:21:42.650508: step 3030, loss = 2.62 (680.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:44.522362: step 3040, loss = 2.59 (683.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:46.400402: step 3050, loss = 2.61 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:48.269296: step 3060, loss = 2.55 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:50.148198: step 3070, loss = 2.59 (681.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:21:52.022756: step 3080, loss = 2.60 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:53.895742: step 3090, loss = 2.58 (683.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:21:56.009247: step 3100, loss = 2.60 (651.0 examples/sec; 0.197 sec/batch), precision = 19.70%
2017-05-18 07:21:59.018444: step 3110, loss = 2.63 (405.5 examples/sec; 0.316 sec/batch)
2017-05-18 07:22:00.959101: step 3120, loss = 2.54 (659.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:22:02.855584: step 3130, loss = 2.55 (674.9 examples/sec; 0.190 sec/batch)
2017-05-18 07:22:04.741993: step 3140, loss = 2.56 (678.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:22:06.634196: step 3150, loss = 2.58 (676.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:22:08.522896: step 3160, loss = 2.60 (677.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:22:10.457328: step 3170, loss = 2.58 (661.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:22:12.376059: step 3180, loss = 2.52 (667.1 examples/sec; 0.192 sec/batch)
2017-05-18 07:22:14.313144: step 3190, loss = 2.58 (660.8 examples/sec; 0.194 sec/batch)
2017-05-18 07:22:16.492971: step 3200, loss = 2.59 (631.7 examples/sec; 0.203 sec/batch), precision = 19.30%
2017-05-18 07:22:19.509996: step 3210, loss = 2.54 (403.7 examples/sec; 0.317 sec/batch)
2017-05-18 07:22:21.410585: step 3220, loss = 2.58 (673.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:22:23.290641: step 3230, loss = 2.54 (680.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:25.163165: step 3240, loss = 2.54 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:22:27.056585: step 3250, loss = 2.50 (676.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:22:28.953564: step 3260, loss = 2.50 (674.8 examples/sec; 0.190 sec/batch)
2017-05-18 07:22:30.829634: step 3270, loss = 2.56 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:32.698993: step 3280, loss = 2.57 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:22:34.580259: step 3290, loss = 2.47 (680.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:36.696278: step 3300, loss = 2.51 (650.9 examples/sec; 0.197 sec/batch), precision = 18.70%
2017-05-18 07:22:39.663888: step 3310, loss = 2.52 (410.7 examples/sec; 0.312 sec/batch)
2017-05-18 07:22:41.574875: step 3320, loss = 2.52 (669.8 examples/sec; 0.191 sec/batch)
2017-05-18 07:22:43.456045: step 3330, loss = 2.49 (680.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:45.330430: step 3340, loss = 2.51 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:22:47.210085: step 3350, loss = 2.51 (681.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:49.072868: step 3360, loss = 2.50 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:22:50.940602: step 3370, loss = 2.49 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:22:52.809159: step 3380, loss = 2.48 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:22:54.686963: step 3390, loss = 2.55 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:22:56.788713: step 3400, loss = 2.52 (654.1 examples/sec; 0.196 sec/batch), precision = 17.30%
2017-05-18 07:22:59.801930: step 3410, loss = 2.52 (405.3 examples/sec; 0.316 sec/batch)
2017-05-18 07:23:01.770304: step 3420, loss = 2.53 (650.3 examples/sec; 0.197 sec/batch)
2017-05-18 07:23:03.706831: step 3430, loss = 2.50 (661.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:23:05.648451: step 3440, loss = 2.52 (659.2 examples/sec; 0.194 sec/batch)
2017-05-18 07:23:07.588729: step 3450, loss = 2.52 (659.7 examples/sec; 0.194 sec/batch)
2017-05-18 07:23:09.532582: step 3460, loss = 2.57 (658.5 examples/sec; 0.194 sec/batch)
2017-05-18 07:23:11.478935: step 3470, loss = 2.50 (657.6 examples/sec; 0.195 sec/batch)
2017-05-18 07:23:13.422348: step 3480, loss = 2.42 (658.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:23:15.378574: step 3490, loss = 2.51 (654.3 examples/sec; 0.196 sec/batch)
2017-05-18 07:23:17.557815: step 3500, loss = 2.55 (632.1 examples/sec; 0.202 sec/batch), precision = 19.00%
2017-05-18 07:23:20.700568: step 3510, loss = 2.44 (388.2 examples/sec; 0.330 sec/batch)
2017-05-18 07:23:22.632576: step 3520, loss = 2.50 (662.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:23:24.512457: step 3530, loss = 2.43 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:23:26.402840: step 3540, loss = 2.49 (677.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:23:28.283563: step 3550, loss = 2.54 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:23:30.173324: step 3560, loss = 2.46 (677.3 examples/sec; 0.189 sec/batch)
2017-05-18 07:23:32.060191: step 3570, loss = 2.45 (678.4 examples/sec; 0.189 sec/batch)
2017-05-18 07:23:33.942336: step 3580, loss = 2.44 (680.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:23:35.837579: step 3590, loss = 2.45 (675.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:23:37.951469: step 3600, loss = 2.44 (650.3 examples/sec; 0.197 sec/batch), precision = 18.10%
2017-05-18 07:23:40.999829: step 3610, loss = 2.49 (400.7 examples/sec; 0.319 sec/batch)
2017-05-18 07:23:42.900855: step 3620, loss = 2.50 (673.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:23:44.781821: step 3630, loss = 2.43 (680.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:23:46.677966: step 3640, loss = 2.45 (675.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:23:48.559734: step 3650, loss = 2.45 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:23:50.458374: step 3660, loss = 2.41 (674.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:23:52.376760: step 3670, loss = 2.49 (667.2 examples/sec; 0.192 sec/batch)
2017-05-18 07:23:54.303254: step 3680, loss = 2.48 (664.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:23:56.253296: step 3690, loss = 2.42 (656.4 examples/sec; 0.195 sec/batch)
2017-05-18 07:23:58.439052: step 3700, loss = 2.47 (629.4 examples/sec; 0.203 sec/batch), precision = 16.50%
2017-05-18 07:24:01.464559: step 3710, loss = 2.43 (402.8 examples/sec; 0.318 sec/batch)
2017-05-18 07:24:03.360338: step 3720, loss = 2.50 (675.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:24:05.249037: step 3730, loss = 2.46 (677.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:24:07.129763: step 3740, loss = 2.42 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:09.013919: step 3750, loss = 2.45 (679.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:10.909114: step 3760, loss = 2.41 (675.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:24:12.781026: step 3770, loss = 2.41 (683.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:24:14.657629: step 3780, loss = 2.47 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:16.533774: step 3790, loss = 2.45 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:18.643205: step 3800, loss = 2.41 (650.7 examples/sec; 0.197 sec/batch), precision = 18.10%
2017-05-18 07:24:21.629209: step 3810, loss = 2.39 (409.2 examples/sec; 0.313 sec/batch)
2017-05-18 07:24:23.525114: step 3820, loss = 2.40 (675.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:24:25.398283: step 3830, loss = 2.40 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:24:27.277979: step 3840, loss = 2.40 (681.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:29.142859: step 3850, loss = 2.43 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:24:31.033236: step 3860, loss = 2.40 (677.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:24:32.931524: step 3870, loss = 2.43 (674.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:24:34.809858: step 3880, loss = 2.42 (681.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:36.693695: step 3890, loss = 2.39 (679.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:24:38.801458: step 3900, loss = 2.41 (651.6 examples/sec; 0.196 sec/batch), precision = 19.10%
2017-05-18 07:24:41.828657: step 3910, loss = 2.50 (403.7 examples/sec; 0.317 sec/batch)
2017-05-18 07:24:43.736020: step 3920, loss = 2.42 (671.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:24:45.648057: step 3930, loss = 2.47 (669.4 examples/sec; 0.191 sec/batch)
2017-05-18 07:24:47.590389: step 3940, loss = 2.41 (659.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:24:49.533774: step 3950, loss = 2.42 (658.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:24:51.526877: step 3960, loss = 2.42 (642.2 examples/sec; 0.199 sec/batch)
2017-05-18 07:24:53.458715: step 3970, loss = 2.36 (662.6 examples/sec; 0.193 sec/batch)
2017-05-18 07:24:55.406246: step 3980, loss = 2.38 (657.2 examples/sec; 0.195 sec/batch)
2017-05-18 07:24:57.354697: step 3990, loss = 2.37 (656.9 examples/sec; 0.195 sec/batch)
2017-05-18 07:24:59.533857: step 4000, loss = 2.44 (631.8 examples/sec; 0.203 sec/batch), precision = 19.70%
2017-05-18 07:25:02.567103: step 4010, loss = 2.37 (401.7 examples/sec; 0.319 sec/batch)
2017-05-18 07:25:04.444129: step 4020, loss = 2.43 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:25:06.315530: step 4030, loss = 2.39 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:08.256198: step 4040, loss = 2.32 (659.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:25:10.190728: step 4050, loss = 2.38 (661.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:25:12.085111: step 4060, loss = 2.39 (675.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:25:14.070426: step 4070, loss = 2.32 (644.7 examples/sec; 0.199 sec/batch)
2017-05-18 07:25:15.947266: step 4080, loss = 2.45 (682.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:25:17.815447: step 4090, loss = 2.36 (685.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:19.922036: step 4100, loss = 2.39 (651.8 examples/sec; 0.196 sec/batch), precision = 19.60%
2017-05-18 07:25:23.003919: step 4110, loss = 2.37 (396.9 examples/sec; 0.322 sec/batch)
2017-05-18 07:25:24.911813: step 4120, loss = 2.44 (670.9 examples/sec; 0.191 sec/batch)
2017-05-18 07:25:26.776233: step 4130, loss = 2.42 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:25:28.637546: step 4140, loss = 2.33 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:25:30.507973: step 4150, loss = 2.35 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:32.382288: step 4160, loss = 2.41 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:34.378177: step 4170, loss = 2.40 (641.3 examples/sec; 0.200 sec/batch)
2017-05-18 07:25:36.323673: step 4180, loss = 2.39 (657.9 examples/sec; 0.195 sec/batch)
2017-05-18 07:25:38.244370: step 4190, loss = 2.36 (666.4 examples/sec; 0.192 sec/batch)
2017-05-18 07:25:40.413777: step 4200, loss = 2.37 (634.5 examples/sec; 0.202 sec/batch), precision = 17.30%
2017-05-18 07:25:43.428748: step 4210, loss = 2.42 (404.2 examples/sec; 0.317 sec/batch)
2017-05-18 07:25:45.322266: step 4220, loss = 2.38 (676.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:25:47.302881: step 4230, loss = 2.43 (646.3 examples/sec; 0.198 sec/batch)
2017-05-18 07:25:49.166055: step 4240, loss = 2.38 (687.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:25:51.035180: step 4250, loss = 2.38 (684.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:52.915476: step 4260, loss = 2.32 (680.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:25:54.776799: step 4270, loss = 2.39 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:25:56.648159: step 4280, loss = 2.37 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:25:58.522280: step 4290, loss = 2.37 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:26:00.613885: step 4300, loss = 2.38 (657.2 examples/sec; 0.195 sec/batch), precision = 18.10%
2017-05-18 07:26:03.571604: step 4310, loss = 2.33 (412.7 examples/sec; 0.310 sec/batch)
2017-05-18 07:26:05.463767: step 4320, loss = 2.35 (676.5 examples/sec; 0.189 sec/batch)
2017-05-18 07:26:07.327264: step 4330, loss = 2.39 (686.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:09.196299: step 4340, loss = 2.34 (684.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:26:11.062902: step 4350, loss = 2.38 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:26:12.947445: step 4360, loss = 2.35 (679.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:26:14.816428: step 4370, loss = 2.34 (684.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:26:16.669717: step 4380, loss = 2.36 (690.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:26:18.535633: step 4390, loss = 2.35 (686.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:26:20.631258: step 4400, loss = 2.29 (655.9 examples/sec; 0.195 sec/batch), precision = 20.50%
2017-05-18 07:26:23.642090: step 4410, loss = 2.36 (405.7 examples/sec; 0.315 sec/batch)
2017-05-18 07:26:25.568375: step 4420, loss = 2.37 (664.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:26:27.471855: step 4430, loss = 2.39 (672.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:26:29.384319: step 4440, loss = 2.36 (669.3 examples/sec; 0.191 sec/batch)
2017-05-18 07:26:31.311810: step 4450, loss = 2.35 (664.1 examples/sec; 0.193 sec/batch)
2017-05-18 07:26:33.246415: step 4460, loss = 2.31 (661.6 examples/sec; 0.193 sec/batch)
2017-05-18 07:26:35.180748: step 4470, loss = 2.34 (661.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:26:37.118851: step 4480, loss = 2.35 (660.4 examples/sec; 0.194 sec/batch)
2017-05-18 07:26:39.051671: step 4490, loss = 2.30 (662.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:26:41.222365: step 4500, loss = 2.35 (633.8 examples/sec; 0.202 sec/batch), precision = 20.20%
2017-05-18 07:26:44.211628: step 4510, loss = 2.33 (407.6 examples/sec; 0.314 sec/batch)
2017-05-18 07:26:46.093576: step 4520, loss = 2.32 (680.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:26:47.950243: step 4530, loss = 2.35 (689.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:49.806715: step 4540, loss = 2.34 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:51.666376: step 4550, loss = 2.30 (688.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:53.525186: step 4560, loss = 2.32 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:55.369709: step 4570, loss = 2.38 (693.9 examples/sec; 0.184 sec/batch)
2017-05-18 07:26:57.231634: step 4580, loss = 2.33 (687.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:26:59.092002: step 4590, loss = 2.33 (688.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:01.193797: step 4600, loss = 2.35 (653.4 examples/sec; 0.196 sec/batch), precision = 16.30%
2017-05-18 07:27:04.152708: step 4610, loss = 2.35 (412.7 examples/sec; 0.310 sec/batch)
2017-05-18 07:27:06.058274: step 4620, loss = 2.35 (671.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:27:07.934495: step 4630, loss = 2.38 (682.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:27:09.805742: step 4640, loss = 2.32 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:11.670431: step 4650, loss = 2.30 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:13.537583: step 4660, loss = 2.34 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:15.437368: step 4670, loss = 2.31 (673.8 examples/sec; 0.190 sec/batch)
2017-05-18 07:27:17.345872: step 4680, loss = 2.32 (670.7 examples/sec; 0.191 sec/batch)
2017-05-18 07:27:19.279887: step 4690, loss = 2.34 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:27:21.462487: step 4700, loss = 2.30 (629.7 examples/sec; 0.203 sec/batch), precision = 19.50%
2017-05-18 07:27:24.563299: step 4710, loss = 2.31 (393.8 examples/sec; 0.325 sec/batch)
2017-05-18 07:27:26.441676: step 4720, loss = 2.29 (681.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:27:28.301168: step 4730, loss = 2.34 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:30.159456: step 4740, loss = 2.32 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:32.018846: step 4750, loss = 2.30 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:33.893710: step 4760, loss = 2.36 (682.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:35.757645: step 4770, loss = 2.37 (686.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:37.622551: step 4780, loss = 2.33 (686.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:39.494573: step 4790, loss = 2.31 (683.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:41.586937: step 4800, loss = 2.33 (657.3 examples/sec; 0.195 sec/batch), precision = 20.70%
2017-05-18 07:27:44.590849: step 4810, loss = 2.30 (406.5 examples/sec; 0.315 sec/batch)
2017-05-18 07:27:46.471458: step 4820, loss = 2.32 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:27:48.335109: step 4830, loss = 2.33 (686.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:50.197942: step 4840, loss = 2.30 (687.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:52.062143: step 4850, loss = 2.40 (686.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:27:53.934924: step 4860, loss = 2.29 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:55.807578: step 4870, loss = 2.35 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:57.681852: step 4880, loss = 2.31 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:27:59.546298: step 4890, loss = 2.29 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:28:01.656421: step 4900, loss = 2.30 (651.7 examples/sec; 0.196 sec/batch), precision = 21.60%
2017-05-18 07:28:04.660388: step 4910, loss = 2.34 (406.4 examples/sec; 0.315 sec/batch)
2017-05-18 07:28:06.555953: step 4920, loss = 2.30 (675.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:28:08.452554: step 4930, loss = 2.31 (674.9 examples/sec; 0.190 sec/batch)
2017-05-18 07:28:10.364014: step 4940, loss = 2.31 (669.6 examples/sec; 0.191 sec/batch)
2017-05-18 07:28:12.292533: step 4950, loss = 2.31 (663.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:28:14.228195: step 4960, loss = 2.31 (661.3 examples/sec; 0.194 sec/batch)
2017-05-18 07:28:16.168298: step 4970, loss = 2.34 (659.8 examples/sec; 0.194 sec/batch)
2017-05-18 07:28:18.093585: step 4980, loss = 2.32 (664.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:28:20.027673: step 4990, loss = 2.29 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:28:22.206828: step 5000, loss = 2.33 (632.4 examples/sec; 0.202 sec/batch), precision = 18.00%
2017-05-18 07:28:25.215094: step 5010, loss = 2.36 (404.6 examples/sec; 0.316 sec/batch)
2017-05-18 07:28:27.122553: step 5020, loss = 2.29 (671.1 examples/sec; 0.191 sec/batch)
2017-05-18 07:28:29.009987: step 5030, loss = 2.31 (678.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:28:30.878509: step 5040, loss = 2.36 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:28:32.756927: step 5050, loss = 2.31 (681.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:34.635180: step 5060, loss = 2.28 (681.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:36.504752: step 5070, loss = 2.30 (684.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:28:38.388338: step 5080, loss = 2.33 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:40.271060: step 5090, loss = 2.33 (679.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:42.380297: step 5100, loss = 2.33 (651.4 examples/sec; 0.197 sec/batch), precision = 18.80%
2017-05-18 07:28:45.369614: step 5110, loss = 2.30 (408.5 examples/sec; 0.313 sec/batch)
2017-05-18 07:28:47.271107: step 5120, loss = 2.32 (673.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:28:49.139219: step 5130, loss = 2.29 (685.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:28:51.015909: step 5140, loss = 2.36 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:52.882112: step 5150, loss = 2.34 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:28:54.749918: step 5160, loss = 2.23 (685.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:28:56.633888: step 5170, loss = 2.33 (679.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:28:58.537450: step 5180, loss = 2.33 (672.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:29:00.474206: step 5190, loss = 2.28 (660.9 examples/sec; 0.194 sec/batch)
2017-05-18 07:29:02.655023: step 5200, loss = 2.33 (631.7 examples/sec; 0.203 sec/batch), precision = 17.80%
2017-05-18 07:29:05.673290: step 5210, loss = 2.27 (403.4 examples/sec; 0.317 sec/batch)
2017-05-18 07:29:07.571788: step 5220, loss = 2.23 (674.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:29:09.442508: step 5230, loss = 2.33 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:29:11.317064: step 5240, loss = 2.31 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:29:13.197620: step 5250, loss = 2.30 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:15.075651: step 5260, loss = 2.30 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:16.953863: step 5270, loss = 2.32 (681.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:18.820438: step 5280, loss = 2.29 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:29:20.696345: step 5290, loss = 2.28 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:22.790011: step 5300, loss = 2.29 (656.2 examples/sec; 0.195 sec/batch), precision = 18.40%
2017-05-18 07:29:25.909631: step 5310, loss = 2.24 (392.3 examples/sec; 0.326 sec/batch)
2017-05-18 07:29:27.797248: step 5320, loss = 2.32 (678.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:29:29.673870: step 5330, loss = 2.31 (682.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:31.533285: step 5340, loss = 2.23 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:29:33.405544: step 5350, loss = 2.27 (683.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:29:35.287265: step 5360, loss = 2.28 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:37.163257: step 5370, loss = 2.28 (682.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:29:39.037876: step 5380, loss = 2.31 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:29:40.923515: step 5390, loss = 2.34 (678.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:29:43.018722: step 5400, loss = 2.31 (655.6 examples/sec; 0.195 sec/batch), precision = 18.20%
2017-05-18 07:29:46.011439: step 5410, loss = 2.27 (408.2 examples/sec; 0.314 sec/batch)
2017-05-18 07:29:47.927041: step 5420, loss = 2.29 (668.2 examples/sec; 0.192 sec/batch)
2017-05-18 07:29:49.836447: step 5430, loss = 2.27 (670.4 examples/sec; 0.191 sec/batch)
2017-05-18 07:29:51.754137: step 5440, loss = 2.32 (667.5 examples/sec; 0.192 sec/batch)
2017-05-18 07:29:53.690525: step 5450, loss = 2.26 (661.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:29:55.629587: step 5460, loss = 2.24 (660.1 examples/sec; 0.194 sec/batch)
2017-05-18 07:29:57.559991: step 5470, loss = 2.25 (663.1 examples/sec; 0.193 sec/batch)
2017-05-18 07:29:59.494093: step 5480, loss = 2.28 (661.8 examples/sec; 0.193 sec/batch)
2017-05-18 07:30:01.431047: step 5490, loss = 2.27 (660.8 examples/sec; 0.194 sec/batch)
2017-05-18 07:30:03.612445: step 5500, loss = 2.31 (631.1 examples/sec; 0.203 sec/batch), precision = 18.70%
2017-05-18 07:30:06.616385: step 5510, loss = 2.30 (405.5 examples/sec; 0.316 sec/batch)
2017-05-18 07:30:08.500223: step 5520, loss = 2.30 (679.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:30:10.366625: step 5530, loss = 2.28 (685.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:12.242032: step 5540, loss = 2.21 (682.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:30:14.112108: step 5550, loss = 2.27 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:15.986018: step 5560, loss = 2.31 (683.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:17.865159: step 5570, loss = 2.37 (681.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:30:19.736508: step 5580, loss = 2.27 (684.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:21.599143: step 5590, loss = 2.27 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:30:23.692366: step 5600, loss = 2.25 (657.2 examples/sec; 0.195 sec/batch), precision = 16.40%
2017-05-18 07:30:26.685677: step 5610, loss = 2.29 (407.8 examples/sec; 0.314 sec/batch)
2017-05-18 07:30:28.560479: step 5620, loss = 2.25 (682.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:30.419582: step 5630, loss = 2.29 (688.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:30:32.285748: step 5640, loss = 2.24 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:34.145636: step 5650, loss = 2.28 (688.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:30:36.023704: step 5660, loss = 2.21 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:30:37.927974: step 5670, loss = 2.27 (672.2 examples/sec; 0.190 sec/batch)
2017-05-18 07:30:39.842755: step 5680, loss = 2.30 (668.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:30:41.775679: step 5690, loss = 2.30 (662.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:30:43.929150: step 5700, loss = 2.25 (637.9 examples/sec; 0.201 sec/batch), precision = 18.50%
2017-05-18 07:30:46.917542: step 5710, loss = 2.30 (408.3 examples/sec; 0.314 sec/batch)
2017-05-18 07:30:48.791183: step 5720, loss = 2.32 (683.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:50.644432: step 5730, loss = 2.29 (690.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:30:52.505644: step 5740, loss = 2.25 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:30:54.375005: step 5750, loss = 2.28 (684.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:56.226514: step 5760, loss = 2.27 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:30:58.092789: step 5770, loss = 2.28 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:30:59.959487: step 5780, loss = 2.29 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:01.826826: step 5790, loss = 2.25 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:03.925877: step 5800, loss = 2.22 (654.6 examples/sec; 0.196 sec/batch), precision = 19.50%
2017-05-18 07:31:06.908222: step 5810, loss = 2.36 (409.5 examples/sec; 0.313 sec/batch)
2017-05-18 07:31:08.835053: step 5820, loss = 2.36 (664.3 examples/sec; 0.193 sec/batch)
2017-05-18 07:31:10.778749: step 5830, loss = 2.27 (658.5 examples/sec; 0.194 sec/batch)
2017-05-18 07:31:12.636463: step 5840, loss = 2.28 (689.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:14.494942: step 5850, loss = 2.23 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:16.346177: step 5860, loss = 2.23 (691.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:31:18.204786: step 5870, loss = 2.30 (688.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:20.055317: step 5880, loss = 2.28 (691.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:31:21.914866: step 5890, loss = 2.27 (688.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:24.002350: step 5900, loss = 2.27 (657.8 examples/sec; 0.195 sec/batch), precision = 17.70%
2017-05-18 07:31:28.103986: step 5910, loss = 2.27 (301.7 examples/sec; 0.424 sec/batch)
2017-05-18 07:31:29.267553: step 5920, loss = 2.30 (1100.1 examples/sec; 0.116 sec/batch)
2017-05-18 07:31:31.132637: step 5930, loss = 2.24 (686.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:32.986035: step 5940, loss = 2.27 (690.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:31:34.846687: step 5950, loss = 2.24 (687.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:36.714154: step 5960, loss = 2.30 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:38.692647: step 5970, loss = 2.30 (647.0 examples/sec; 0.198 sec/batch)
2017-05-18 07:31:40.551042: step 5980, loss = 2.28 (688.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:42.425131: step 5990, loss = 2.26 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:44.510105: step 6000, loss = 2.34 (658.9 examples/sec; 0.194 sec/batch), precision = 18.70%
2017-05-18 07:31:47.509927: step 6010, loss = 2.28 (407.4 examples/sec; 0.314 sec/batch)
2017-05-18 07:31:49.406162: step 6020, loss = 2.27 (675.0 examples/sec; 0.190 sec/batch)
2017-05-18 07:31:51.265036: step 6030, loss = 2.27 (688.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:31:53.119147: step 6040, loss = 2.29 (690.4 examples/sec; 0.185 sec/batch)
2017-05-18 07:31:54.993291: step 6050, loss = 2.25 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:56.865117: step 6060, loss = 2.24 (683.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:31:58.738535: step 6070, loss = 2.32 (683.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:00.605195: step 6080, loss = 2.28 (685.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:02.472236: step 6090, loss = 2.24 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:04.558362: step 6100, loss = 2.29 (657.8 examples/sec; 0.195 sec/batch), precision = 18.30%
2017-05-18 07:32:07.544628: step 6110, loss = 2.27 (409.4 examples/sec; 0.313 sec/batch)
2017-05-18 07:32:09.436459: step 6120, loss = 2.23 (676.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:32:11.311217: step 6130, loss = 2.28 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:13.179870: step 6140, loss = 2.27 (685.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:15.080322: step 6150, loss = 2.28 (673.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:32:17.365588: step 6160, loss = 2.20 (560.1 examples/sec; 0.229 sec/batch)
2017-05-18 07:32:20.143855: step 6170, loss = 2.24 (460.7 examples/sec; 0.278 sec/batch)
2017-05-18 07:32:22.542050: step 6180, loss = 2.27 (533.7 examples/sec; 0.240 sec/batch)
2017-05-18 07:32:24.716229: step 6190, loss = 2.23 (588.7 examples/sec; 0.217 sec/batch)
2017-05-18 07:32:27.735571: step 6200, loss = 2.32 (447.0 examples/sec; 0.286 sec/batch), precision = 19.70%
2017-05-18 07:32:32.032120: step 6210, loss = 2.25 (287.5 examples/sec; 0.445 sec/batch)
2017-05-18 07:32:34.595452: step 6220, loss = 2.24 (499.3 examples/sec; 0.256 sec/batch)
2017-05-18 07:32:36.488598: step 6230, loss = 2.25 (676.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:32:38.366082: step 6240, loss = 2.31 (681.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:32:40.244010: step 6250, loss = 2.29 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:32:42.113904: step 6260, loss = 2.26 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:43.986297: step 6270, loss = 2.24 (683.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:45.853210: step 6280, loss = 2.26 (685.6 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:47.728011: step 6290, loss = 2.26 (682.7 examples/sec; 0.187 sec/batch)
2017-05-18 07:32:49.830331: step 6300, loss = 2.20 (654.3 examples/sec; 0.196 sec/batch), precision = 19.20%
2017-05-18 07:32:52.844942: step 6310, loss = 2.28 (405.0 examples/sec; 0.316 sec/batch)
2017-05-18 07:32:54.730303: step 6320, loss = 2.31 (678.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:32:56.590711: step 6330, loss = 2.23 (688.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:32:58.470214: step 6340, loss = 2.26 (681.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:00.374709: step 6350, loss = 2.30 (672.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:33:02.301206: step 6360, loss = 2.22 (664.4 examples/sec; 0.193 sec/batch)
2017-05-18 07:33:04.245788: step 6370, loss = 2.28 (658.2 examples/sec; 0.194 sec/batch)
2017-05-18 07:33:06.187359: step 6380, loss = 2.25 (659.3 examples/sec; 0.194 sec/batch)
2017-05-18 07:33:08.122841: step 6390, loss = 2.19 (661.3 examples/sec; 0.194 sec/batch)
2017-05-18 07:33:10.296527: step 6400, loss = 2.29 (631.5 examples/sec; 0.203 sec/batch), precision = 19.40%
2017-05-18 07:33:13.434618: step 6410, loss = 2.26 (389.7 examples/sec; 0.328 sec/batch)
2017-05-18 07:33:15.327819: step 6420, loss = 2.28 (676.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:17.215101: step 6430, loss = 2.22 (678.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:19.090180: step 6440, loss = 2.27 (682.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:20.969443: step 6450, loss = 2.28 (681.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:22.852096: step 6460, loss = 2.21 (679.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:24.724909: step 6470, loss = 2.33 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:33:26.620019: step 6480, loss = 2.27 (675.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:33:28.494213: step 6490, loss = 2.28 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:33:30.613941: step 6500, loss = 2.27 (647.6 examples/sec; 0.198 sec/batch), precision = 18.80%
2017-05-18 07:33:33.612523: step 6510, loss = 2.27 (407.4 examples/sec; 0.314 sec/batch)
2017-05-18 07:33:35.510698: step 6520, loss = 2.31 (674.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:33:37.396492: step 6530, loss = 2.34 (678.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:39.283978: step 6540, loss = 2.26 (678.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:41.168116: step 6550, loss = 2.20 (679.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:43.052915: step 6560, loss = 2.24 (679.1 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:44.936980: step 6570, loss = 2.29 (679.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:33:46.811274: step 6580, loss = 2.27 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:33:48.700365: step 6590, loss = 2.28 (677.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:50.839832: step 6600, loss = 2.26 (642.6 examples/sec; 0.199 sec/batch), precision = 18.50%
2017-05-18 07:33:53.876957: step 6610, loss = 2.21 (401.9 examples/sec; 0.318 sec/batch)
2017-05-18 07:33:55.770774: step 6620, loss = 2.21 (675.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:57.657078: step 6630, loss = 2.24 (678.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:33:59.531733: step 6640, loss = 2.29 (682.8 examples/sec; 0.187 sec/batch)
2017-05-18 07:34:01.428692: step 6650, loss = 2.22 (674.8 examples/sec; 0.190 sec/batch)
2017-05-18 07:34:03.302991: step 6660, loss = 2.26 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:34:05.194245: step 6670, loss = 2.28 (676.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:34:07.072145: step 6680, loss = 2.23 (681.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:08.949133: step 6690, loss = 2.22 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:11.064146: step 6700, loss = 2.30 (649.5 examples/sec; 0.197 sec/batch), precision = 19.60%
2017-05-18 07:34:14.072834: step 6710, loss = 2.30 (406.0 examples/sec; 0.315 sec/batch)
2017-05-18 07:34:15.981869: step 6720, loss = 2.21 (670.5 examples/sec; 0.191 sec/batch)
2017-05-18 07:34:17.858952: step 6730, loss = 2.29 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:19.735776: step 6740, loss = 2.28 (682.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:21.616485: step 6750, loss = 2.26 (680.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:23.501113: step 6760, loss = 2.28 (679.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:25.378299: step 6770, loss = 2.21 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:27.249379: step 6780, loss = 2.26 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:34:29.124762: step 6790, loss = 2.18 (682.5 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:31.230692: step 6800, loss = 2.26 (652.6 examples/sec; 0.196 sec/batch), precision = 19.20%
2017-05-18 07:34:34.241680: step 6810, loss = 2.23 (405.7 examples/sec; 0.316 sec/batch)
2017-05-18 07:34:36.154837: step 6820, loss = 2.25 (669.0 examples/sec; 0.191 sec/batch)
2017-05-18 07:34:38.076173: step 6830, loss = 2.22 (666.2 examples/sec; 0.192 sec/batch)
2017-05-18 07:34:40.013696: step 6840, loss = 2.23 (660.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:34:41.956478: step 6850, loss = 2.25 (658.8 examples/sec; 0.194 sec/batch)
2017-05-18 07:34:43.906781: step 6860, loss = 2.25 (656.3 examples/sec; 0.195 sec/batch)
2017-05-18 07:34:45.846274: step 6870, loss = 2.30 (660.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:34:47.813724: step 6880, loss = 2.26 (650.6 examples/sec; 0.197 sec/batch)
2017-05-18 07:34:49.771808: step 6890, loss = 2.21 (653.7 examples/sec; 0.196 sec/batch)
2017-05-18 07:34:51.938436: step 6900, loss = 2.26 (634.0 examples/sec; 0.202 sec/batch), precision = 18.90%
2017-05-18 07:34:54.940819: step 6910, loss = 2.27 (406.3 examples/sec; 0.315 sec/batch)
2017-05-18 07:34:56.825216: step 6920, loss = 2.23 (679.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:34:58.687377: step 6930, loss = 2.26 (687.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:00.557708: step 6940, loss = 2.24 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:35:02.427800: step 6950, loss = 2.27 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:35:04.293935: step 6960, loss = 2.24 (685.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:35:06.157368: step 6970, loss = 2.24 (686.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:08.019028: step 6980, loss = 2.22 (687.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:09.904091: step 6990, loss = 2.21 (679.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:35:12.017132: step 7000, loss = 2.28 (650.7 examples/sec; 0.197 sec/batch), precision = 18.70%
2017-05-18 07:35:15.088191: step 7010, loss = 2.27 (397.9 examples/sec; 0.322 sec/batch)
2017-05-18 07:35:17.026007: step 7020, loss = 2.23 (660.5 examples/sec; 0.194 sec/batch)
2017-05-18 07:35:18.896355: step 7030, loss = 2.23 (684.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:35:20.787611: step 7040, loss = 2.18 (676.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:35:22.660925: step 7050, loss = 2.21 (683.3 examples/sec; 0.187 sec/batch)
2017-05-18 07:35:24.546904: step 7060, loss = 2.23 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:35:26.450723: step 7070, loss = 2.23 (672.3 examples/sec; 0.190 sec/batch)
2017-05-18 07:35:28.348384: step 7080, loss = 2.25 (674.5 examples/sec; 0.190 sec/batch)
2017-05-18 07:35:30.231759: step 7090, loss = 2.26 (679.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:35:32.383489: step 7100, loss = 2.21 (638.6 examples/sec; 0.200 sec/batch), precision = 20.40%
2017-05-18 07:35:35.448681: step 7110, loss = 2.29 (398.4 examples/sec; 0.321 sec/batch)
2017-05-18 07:35:37.309398: step 7120, loss = 2.22 (687.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:39.164805: step 7130, loss = 2.26 (689.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:41.025022: step 7140, loss = 2.31 (688.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:42.881351: step 7150, loss = 2.23 (689.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:44.733053: step 7160, loss = 2.25 (691.3 examples/sec; 0.185 sec/batch)
2017-05-18 07:35:46.583430: step 7170, loss = 2.27 (691.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:35:48.434305: step 7180, loss = 2.20 (691.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:35:50.297072: step 7190, loss = 2.25 (687.2 examples/sec; 0.186 sec/batch)
2017-05-18 07:35:52.390287: step 7200, loss = 2.26 (656.3 examples/sec; 0.195 sec/batch), precision = 18.80%
2017-05-18 07:35:55.335045: step 7210, loss = 2.26 (414.6 examples/sec; 0.309 sec/batch)
2017-05-18 07:35:57.218230: step 7220, loss = 2.25 (679.7 examples/sec; 0.188 sec/batch)
2017-05-18 07:35:59.077664: step 7230, loss = 2.25 (688.4 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:00.952909: step 7240, loss = 2.20 (682.6 examples/sec; 0.188 sec/batch)
2017-05-18 07:36:02.822988: step 7250, loss = 2.25 (684.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:36:04.680525: step 7260, loss = 2.27 (689.1 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:06.547807: step 7270, loss = 2.25 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:36:08.411205: step 7280, loss = 2.21 (686.9 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:10.275813: step 7290, loss = 2.25 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:12.410861: step 7300, loss = 2.29 (644.0 examples/sec; 0.199 sec/batch), precision = 20.00%
2017-05-18 07:36:15.410575: step 7310, loss = 2.26 (406.7 examples/sec; 0.315 sec/batch)
2017-05-18 07:36:17.300647: step 7320, loss = 2.19 (677.2 examples/sec; 0.189 sec/batch)
2017-05-18 07:36:19.161972: step 7330, loss = 2.24 (687.7 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:21.033101: step 7340, loss = 2.27 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:36:22.888098: step 7350, loss = 2.25 (690.0 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:24.742600: step 7360, loss = 2.27 (690.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:26.590642: step 7370, loss = 2.25 (692.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:28.449853: step 7380, loss = 2.28 (688.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:30.303047: step 7390, loss = 2.23 (690.7 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:32.397110: step 7400, loss = 2.26 (656.4 examples/sec; 0.195 sec/batch), precision = 18.40%
2017-05-18 07:36:35.379525: step 7410, loss = 2.25 (409.4 examples/sec; 0.313 sec/batch)
2017-05-18 07:36:37.265473: step 7420, loss = 2.23 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:36:39.121526: step 7430, loss = 2.28 (689.6 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:40.995583: step 7440, loss = 2.24 (683.0 examples/sec; 0.187 sec/batch)
2017-05-18 07:36:42.860945: step 7450, loss = 2.26 (686.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:36:44.711222: step 7460, loss = 2.22 (691.8 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:46.564806: step 7470, loss = 2.23 (690.6 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:48.419479: step 7480, loss = 2.26 (690.2 examples/sec; 0.185 sec/batch)
2017-05-18 07:36:50.284068: step 7490, loss = 2.24 (686.5 examples/sec; 0.186 sec/batch)
2017-05-18 07:36:52.371066: step 7500, loss = 2.22 (659.4 examples/sec; 0.194 sec/batch), precision = 20.50%
2017-05-18 07:36:55.339660: step 7510, loss = 2.24 (411.0 examples/sec; 0.311 sec/batch)
2017-05-18 07:36:57.247441: step 7520, loss = 2.25 (670.9 examples/sec; 0.191 sec/batch)
2017-05-18 07:36:59.159552: step 7530, loss = 2.24 (669.4 examples/sec; 0.191 sec/batch)
2017-05-18 07:37:01.091533: step 7540, loss = 2.24 (662.5 examples/sec; 0.193 sec/batch)
2017-05-18 07:37:03.018587: step 7550, loss = 2.24 (664.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:37:04.939195: step 7560, loss = 2.27 (666.5 examples/sec; 0.192 sec/batch)
2017-05-18 07:37:06.872263: step 7570, loss = 2.25 (662.2 examples/sec; 0.193 sec/batch)
2017-05-18 07:37:08.814550: step 7580, loss = 2.26 (659.0 examples/sec; 0.194 sec/batch)
2017-05-18 07:37:10.735167: step 7590, loss = 2.26 (666.5 examples/sec; 0.192 sec/batch)
2017-05-18 07:37:12.910196: step 7600, loss = 2.25 (634.1 examples/sec; 0.202 sec/batch), precision = 20.00%
2017-05-18 07:37:16.009012: step 7610, loss = 2.22 (393.2 examples/sec; 0.326 sec/batch)
2017-05-18 07:37:17.900762: step 7620, loss = 2.24 (676.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:37:19.773393: step 7630, loss = 2.20 (683.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:37:21.640777: step 7640, loss = 2.28 (685.5 examples/sec; 0.187 sec/batch)
2017-05-18 07:37:23.519324: step 7650, loss = 2.27 (681.4 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:25.399328: step 7660, loss = 2.23 (680.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:27.263000: step 7670, loss = 2.23 (686.8 examples/sec; 0.186 sec/batch)
2017-05-18 07:37:29.144736: step 7680, loss = 2.26 (680.2 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:31.027000: step 7690, loss = 2.21 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:33.140433: step 7700, loss = 2.26 (650.0 examples/sec; 0.197 sec/batch), precision = 18.40%
2017-05-18 07:37:36.237157: step 7710, loss = 2.21 (395.0 examples/sec; 0.324 sec/batch)
2017-05-18 07:37:38.289282: step 7720, loss = 2.28 (623.7 examples/sec; 0.205 sec/batch)
2017-05-18 07:37:40.170802: step 7730, loss = 2.29 (680.3 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:42.048009: step 7740, loss = 2.20 (681.9 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:43.919156: step 7750, loss = 2.24 (684.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:37:45.801604: step 7760, loss = 2.22 (680.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:37:47.705268: step 7770, loss = 2.24 (672.4 examples/sec; 0.190 sec/batch)
2017-05-18 07:37:49.645953: step 7780, loss = 2.21 (659.6 examples/sec; 0.194 sec/batch)
2017-05-18 07:37:51.573824: step 7790, loss = 2.20 (663.9 examples/sec; 0.193 sec/batch)
2017-05-18 07:37:53.726069: step 7800, loss = 2.25 (640.0 examples/sec; 0.200 sec/batch), precision = 18.60%
2017-05-18 07:37:56.767499: step 7810, loss = 2.32 (400.8 examples/sec; 0.319 sec/batch)
2017-05-18 07:37:58.673833: step 7820, loss = 2.26 (671.4 examples/sec; 0.191 sec/batch)
2017-05-18 07:38:00.567738: step 7830, loss = 2.22 (675.9 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:02.463804: step 7840, loss = 2.26 (675.1 examples/sec; 0.190 sec/batch)
2017-05-18 07:38:04.334678: step 7850, loss = 2.23 (684.2 examples/sec; 0.187 sec/batch)
2017-05-18 07:38:06.223143: step 7860, loss = 2.26 (677.8 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:08.110735: step 7870, loss = 2.23 (678.1 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:09.996604: step 7880, loss = 2.20 (678.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:11.873522: step 7890, loss = 2.23 (682.0 examples/sec; 0.188 sec/batch)
2017-05-18 07:38:13.989295: step 7900, loss = 2.29 (648.6 examples/sec; 0.197 sec/batch), precision = 18.10%
2017-05-18 07:38:16.989080: step 7910, loss = 2.23 (407.4 examples/sec; 0.314 sec/batch)
2017-05-18 07:38:18.880900: step 7920, loss = 2.22 (676.6 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:20.743175: step 7930, loss = 2.26 (687.3 examples/sec; 0.186 sec/batch)
2017-05-18 07:38:22.610779: step 7940, loss = 2.26 (685.4 examples/sec; 0.187 sec/batch)
2017-05-18 07:38:24.490794: step 7950, loss = 2.27 (680.8 examples/sec; 0.188 sec/batch)
2017-05-18 07:38:26.356331: step 7960, loss = 2.24 (686.1 examples/sec; 0.187 sec/batch)
2017-05-18 07:38:28.230781: step 7970, loss = 2.19 (682.9 examples/sec; 0.187 sec/batch)
2017-05-18 07:38:30.116019: step 7980, loss = 2.27 (679.0 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:32.004865: step 7990, loss = 2.24 (677.7 examples/sec; 0.189 sec/batch)
2017-05-18 07:38:34.096113: step 8000, loss = 2.26 (656.6 examples/sec; 0.195 sec/batch), precision = 18.70%
2017-05-18 07:38:37.076847: step 8010, loss = 2.24 (409.9 examples/sec; 0.312 sec/batch)
2017-05-18 07:38:38.983707: step 8020, loss = 2.24 (671.3 examples/sec; 0.191 sec/batch)
2017-05-18 07:38:40.892396: step 8030, loss = 2.29 (670.6 examples/sec; 0.191 sec/batch)
2017-05-18 07:38:42.820231: step 8040, loss = 2.26 (664.0 examples/sec; 0.193 sec/batch)
2017-05-18 07:38:44.760451: step 8050, loss = 2.24 (659.7 examples/sec; 0.194 sec/batch)
2017-05-18 07:38:46.702227: step 8060, loss = 2.29 (659.2 examples/sec; 0.194 sec/batch)
2017-05-18 07:38:48.636635: step 8070, loss = 2.24 (661.7 examples/sec; 0.193 sec/batch)
2017-05-18 07:38:50.584354: step 8080, loss = 2.24 (657.2 exa